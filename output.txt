=== hospital-data-chatbot-ml/Dockerfile (text) ===
FROM python:3.9-slim
WORKDIR /app
RUN pip install uv
ENV PYTHONUNBUFFERED=1
ENV PORT=8080
COPY . .
RUN uv pip install -e .
RUN mkdir -p data/models_data/features
EXPOSE 8080
CMD ["uvicorn", "app.ml.mcp.api:app", "--host", "0.0.0.0", "--port", "8080"]

=== hospital-data-chatbot-ml/.uvproject (text) ===
version = 1
[tool]
python-version = "3.9"
[env]
requirements-dev = ".[dev]"

=== hospital-data-chatbot-ml/pyproject.toml (toml) ===
[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"
[project]
name = "hospital-data-chatbot"
version = "0.1.0"
description = "AI-powered hospital data chatbot using AWS Bedrock and SageMaker"
readme = "README.md"
requires-python = ">=3.9"
license = {text = "MIT"}
dependencies = [
"polars>=0.18.0",          # Fast DataFrame library
"numpy>=1.24.0",           # Numerical computing
"fastapi>=0.100.0",        # API framework
"uvicorn>=0.22.0",         # ASGI server
"boto3>=1.28.0",           # AWS SDK
"openpyxl>=3.1.2",         # Excel file handling
"fastexcel>=0.7.0",        # Faster Excel support
"psycopg2-binary>=2.9.5",  # PostgreSQL adapter
"python-dotenv>=1.0.0",    # Environment variable management
"pydantic>=2.0.0",         # Data validation
"rich>=13.0.0",            # Better console output
]
[project.optional-dependencies]
dev = [
"pytest>=7.0.0",
"black>=23.0.0",
"isort>=5.0.0",
"flake8>=6.0.0",
"pytest-cov>=4.0.0",       # Coverage reporting
"mypy>=1.0.0",             # Type checking
]
prod = [
"gunicorn>=21.0.0",        # WSGI HTTP Server
]
database = [
"alembic>=1.10.0",         # Database migrations
"sqlalchemy>=2.0.0",       # SQL toolkit and ORM
]
[tool.black]
line-length = 88
target-version = ["py39"]
[tool.isort]
profile = "black"
[tool.mypy]
python_version = "3.9"
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
strict_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_return_any = true
warn_unreachable = true
[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = "test_*.py"
python_functions = "test_*"

=== hospital-data-chatbot-ml/ReadMe.md (markdown) ===
This guide outlines how to enhance your Hospital Data Chatbot with machine learning capabilities, leveraging your PostgreSQL database and AWS services.
The following diagram illustrates the machine learning architecture that we're implementing:
```mermaid
flowchart TD
subgraph DB["PostgreSQL Database"]
PatientData[Patient Details]
DiagnosisData[Diagnosis Details]
MLViews[ML Materialized Views]
end
subgraph FeatureLayer["Feature Engineering Layer"]
FE[Feature Engineering]
FS[Feature Store]
PatientRiskFeatures[Patient Risk Features]
ReadmissionFeatures[Readmission Features]
DiagnosisFeatures[Diagnosis Clustering Features]
end
subgraph ModelLayer["ML Model Layer"]
SageMaker[AWS SageMaker Integration]
Models[Hospital ML Models]
ModelRegistry[Model Registry]
RiskModel[Risk Stratification]
ReadmissionModel[Readmission Prediction]
ClusteringModel[Diagnosis Clustering]
end
subgraph APILayer["API Layer"]
MLEndpoints[ML API Endpoints]
SQLEndpoints[SQL Chat Endpoints]
ChatEndpoints[LLM Chat Endpoints]
end
subgraph ClientLayer["Client Applications"]
Dashboard[Risk Dashboard]
ChatInterface[Chat Interface]
Alerts[Clinical Alerts]
end
%% Data Flow Connections
PatientData --> FE
DiagnosisData --> FE
MLViews --> FE
FE --> PatientRiskFeatures
FE --> ReadmissionFeatures
FE --> DiagnosisFeatures
PatientRiskFeatures --> FS
ReadmissionFeatures --> FS
DiagnosisFeatures --> FS
FS --> Models
FS --> SageMaker
SageMaker --> ModelRegistry
ModelRegistry --> Models
Models --> RiskModel
Models --> ReadmissionModel
Models --> ClusteringModel
RiskModel --> MLEndpoints
ReadmissionModel --> MLEndpoints
ClusteringModel --> MLEndpoints
MLEndpoints --> APILayer
SQLEndpoints --> APILayer
ChatEndpoints --> APILayer
APILayer --> Dashboard
APILayer --> ChatInterface
APILayer --> Alerts
```
1. **Data Extraction Layer**
- Extract features from PostgreSQL database
- Transform raw data into ML-ready features
- Store features for reuse
2. **Model Training & Hosting Layer**
- Train models with AWS SageMaker
- Deploy models as endpoints
- Manage model versions
3. **API Integration Layer**
- Expose ML predictions via API
- Integrate with chatbot interface
- Provide analytics endpoints
First, create an IAM role for SageMaker with the necessary permissions:
```bash
aws iam create-role --role-name HospitalSageMakerRole \
aws iam attach-role-policy --role-name HospitalSageMakerRole \
aws iam attach-role-policy --role-name HospitalSageMakerRole \
```
Add the role ARN to your configuration:
```python
SAGEMAKER_ROLE_ARN = "arn:aws:iam::YOUR_ACCOUNT_ID:role/HospitalSageMakerRole"
```
Add the required dependencies to your `pyproject.toml` file:
```toml
[project.dependencies]
pandas = "^2.0.0"
scikit-learn = "^1.2.0"
sagemaker = "^2.132.0"
```
Install the dependencies:
```bash
uv pip install -e ".[ml]"
```
Implement the feature engineering module as shown in the code examples. This involves:
1. **Feature Extraction**: Pull relevant data from PostgreSQL
2. **Feature Transformation**: Convert raw data into ML-ready features
3. **Feature Storage**: Cache features for efficiency
The code provided in `app/ml/feature_engineering.py` implements these capabilities.
The `app/ml/sagemaker_integration.py` module provides:
1. **Model Training**: Train ML models with SageMaker
2. **Model Deployment**: Deploy models to endpoints
3. **Inference**: Get predictions from deployed models
In `app/ml/hospital_ml_models.py`, we've implemented:
1. **Readmission Risk Prediction**: Predict 30-day readmission risk
2. **Patient Risk Stratification**: Classify patients by risk level
3. **Diagnosis Clustering**: Group similar diagnoses
The `app/api/ml_routes.py` module exposes these ML capabilities through REST endpoints:
- `/api/ml/patient-risk` - Get risk stratification for patients
- `/api/ml/readmission-risk/{patient_id}` - Get readmission risk for a patient
- `/api/ml/diagnosis-clusters` - Get clusters of similar diagnoses
- `/api/ml/train-model` - Train a new ML model (dev/staging only)
- `/api/ml/deploy-model` - Deploy a trained model (dev/staging only)
To optimize your PostgreSQL database for machine learning:
```sql
CREATE MATERIALIZED VIEW ml_patient_features AS
SELECT
p.registry_id,
p.age,
p.gender,
p.stay_duration,
COUNT(d.id) AS diagnosis_count,
MAX(CASE WHEN d.diagnosis ILIKE '%diabetes%' THEN 1 ELSE 0 END) AS has_diabetes,
MAX(CASE WHEN d.diagnosis ILIKE '%hypertension%' THEN 1 ELSE 0 END) AS has_hypertension,
MAX(CASE WHEN d.diagnosis ILIKE '%heart%' THEN 1 ELSE 0 END) AS has_heart_condition,
MAX(CASE WHEN d.diagnosis ILIKE '%kidney%' THEN 1 ELSE 0 END) AS has_kidney_condition,
MAX(CASE WHEN d.diagnosis ILIKE '%liver%' THEN 1 ELSE 0 END) AS has_liver_condition
FROM
patient_details p
LEFT JOIN
diagnosis_details d ON p.registry_id = d.registry_id
GROUP BY
p.registry_id, p.age, p.gender, p.stay_duration;
CREATE INDEX ON ml_patient_features (registry_id);
```
Create a function to refresh the materialized views:
```sql
CREATE OR REPLACE FUNCTION refresh_ml_views() RETURNS void AS $$
BEGIN
REFRESH MATERIALIZED VIEW CONCURRENTLY ml_patient_features;
END;
$$ LANGUAGE plpgsql;
```
Schedule this to run regularly:
```bash
0 2 * * * psql -c "SELECT refresh_ml_views();" -d hospital_data_test
```
Add indexes on commonly queried columns:
```sql
CREATE INDEX ON patient_details (age);
CREATE INDEX ON patient_details (gender);
CREATE INDEX ON diagnosis_details (diagnosis);
CREATE INDEX ON diagnosis_details (registry_id);
```
Consider partitioning large tables:
```sql
CREATE TABLE patient_details (
registry_id TEXT PRIMARY KEY,
age INTEGER,
gender TEXT,
) PARTITION BY RANGE (age);
CREATE TABLE patient_details_young PARTITION OF patient_details
FOR VALUES FROM (0) TO (40);
CREATE TABLE patient_details_middle PARTITION OF patient_details
FOR VALUES FROM (40) TO (65);
CREATE TABLE patient_details_elderly PARTITION OF patient_details
FOR VALUES FROM (65) TO (MAXVALUE);
```
Here are some effective ML models for hospital data analysis:
- **Model Type**: XGBoost or Logistic Regression
- **Features**: Demographics, diagnoses, comorbidities, length of stay
- **Target**: Binary (readmitted within 30 days or not)
- **Evaluation**: AUC-ROC, precision, recall
- **Model Type**: Random Forest or Gradient Boosting Regressor
- **Features**: Demographics, admission type, diagnoses
- **Target**: Continuous (number of days)
- **Evaluation**: RMSE, MAE
- **Model Type**: K-Means or Hierarchical Clustering
- **Features**: Co-occurring conditions, patient demographics, outcomes
- **Approach**: Unsupervised learning
- **Evaluation**: Silhouette score, domain expert validation
- **Model Type**: Gradient Boosting Classifier
- **Features**: Age, vital signs, lab values, comorbidities
- **Target**: Binary (mortality outcome)
- **Evaluation**: AUC-ROC, calibration plots
To combine ML capabilities with your text-to-SQL feature:
Update your SQL query engine to recognize ML-related queries:
```python
def _generate_sql_prompt(self, user_query: str) -> str:
ml_capabilities = """
This database also has machine learning capabilities, including:
1. Patient risk stratification: ml_patient_features table
2. Readmission prediction: ml_readmission_prediction table
3. Diagnosis clustering: ml_diagnosis_clusters table
If the user is asking for predictions, risk assessments, or pattern analysis,
consider including these ML-derived tables in your query.
"""
prompt += ml_capabilities
return prompt
```
```python
def process_ml_enhanced_query(self, user_query: str) -> Dict[str, Any]:
"""Process a query that might need both SQL and ML capabilities."""
ml_keywords = ["predict", "risk", "likelihood", "similar", "pattern",
"cluster", "group", "readmission", "mortality"]
is_ml_query = any(keyword in user_query.lower() for keyword in ml_keywords)
sql_response = self.process_query(user_query)
if is_ml_query:
if not hasattr(self, "ml_models"):
from app.ml.hospital_ml_models import HospitalMLModels
self.ml_models = HospitalMLModels()
patient_id_match = re.search(r"patient\s+(\w+)", user_query, re.IGNORECASE)
patient_id = patient_id_match.group(1) if patient_id_match else None
ml_data = {}
if patient_id and "readmission" in user_query.lower():
ml_data["readmission_risk"] = self.ml_models.get_readmission_risk(patient_id)
elif "risk" in user_query.lower():
ml_data["patient_risk"] = self.ml_models.get_patient_risk_stratification(patient_id)
elif any(word in user_query.lower() for word in ["similar", "pattern", "cluster"]):
ml_data["diagnosis_clusters"] = self.ml_models.get_diagnosis_clusters()
combined_response = self._format_hybrid_response(user_query, sql_response, ml_data)
return combined_response
return sql_response
```
Use a feature store to avoid recomputing features:
```python
def get_feature(feature_name, entity_id, force_refresh=False):
cache_key = f"{feature_name}_{entity_id}"
if not force_refresh and cache_exists(cache_key) and not cache_expired(cache_key):
return get_from_cache(cache_key)
feature_value = compute_feature(feature_name, entity_id)
store_in_cache(cache_key, feature_value)
return feature_value
```
Implement batch prediction for efficiency:
```python
def batch_predict_readmission_risk(patient_ids):
"""Run predictions for multiple patients in one batch."""
features_list = []
for patient_id in patient_ids:
features = get_patient_features(patient_id)
features_list.append(features)
batch = create_prediction_batch(features_list)
batch_predictions = invoke_endpoint_with_batch(batch)
results = {}
for i, patient_id in enumerate(patient_ids):
results[patient_id] = batch_predictions[i]
return results
```
Implement a multi-level caching strategy:
```python
def get_ml_prediction(patient_id, prediction_type):
"""Get ML prediction with caching."""
cache_key = f"{prediction_type}_{patient_id}"
if cache_key in memory_cache:
return memory_cache[cache_key]
if os.path.exists(f"cache/{cache_key}.json"):
with open(f"cache/{cache_key}.json", "r") as f:
prediction = json.load(f)
memory_cache[cache_key] = prediction
return prediction
prediction = compute_prediction(patient_id, prediction_type)
memory_cache[cache_key] = prediction
with open(f"cache/{cache_key}.json", "w") as f:
json.dump(prediction, f)
return prediction
```
Create a model performance tracking system:
```python
def track_model_performance(model_id, prediction, actual_outcome):
"""Track model prediction performance over time."""
db.execute("""
INSERT INTO model_performance_tracking
(model_id, prediction_time, prediction, actual_outcome)
VALUES (%s, %s, %s, %s)
""", (model_id, datetime.now(), prediction, actual_outcome))
update_model_metrics(model_id)
```
Implement model drift detection:
```python
def check_model_drift(model_id, window_days=30):
"""Check if model performance is drifting."""
recent_performance = db.query("""
SELECT AVG(CASE WHEN prediction = actual_outcome THEN 1 ELSE 0 END) as accuracy
FROM model_performance_tracking
WHERE model_id = %s AND prediction_time > NOW() - INTERVAL '%s days'
""", (model_id, window_days))
baseline_performance = get_model_baseline(model_id)
drift_amount = baseline_performance - recent_performance
if drift_amount > 0.05:  # 5% threshold
send_alert(f"Model {model_id} has drifted by {drift_amount:.2%}")
return drift_amount
```
Use the ML endpoints to create a dashboard:
```javascript
async function loadPatientRiskDashboard() {
const riskData = await fetch('/api/ml/patient-risk').then(r => r.json());
createChart('risk-distribution', riskData.risk_distribution);
const highRiskPatients = await fetch('/api/db/sql-chat', {
method: 'POST',
body: JSON.stringify({
query: "Which patients are at high risk of readmission?",
include_sql: true
})
}).then(r => r.json());
populateTable('high-risk-table', highRiskPatients.data);
}
```
```python
def readmission_prevention_workflow():
"""Run daily to identify patients for intervention."""
recent_patients = db.query("""
SELECT registry_id FROM patient_details
WHERE discharge_date BETWEEN NOW() - INTERVAL '7 days' AND NOW()
""")
high_risk_patients = []
for patient in recent_patients:
risk = ml_models.get_readmission_risk(patient.registry_id)
if risk.get('risk_level') == 'High':
high_risk_patients.append({
'patient_id': patient.registry_id,
'risk_score': risk.get('risk_score'),
'key_factors': risk.get('key_factors')
})
if high_risk_patients:
generate_intervention_report(high_risk_patients)
return high_risk_patients
```
After implementing the base ML functionality:
1. **Model Refinement**: Tune hyperparameters and add more features
2. **A/B Testing**: Compare different model versions
3. **Scheduled Retraining**: Set up regular model retraining
4. **Feedback Loop**: Capture outcomes to improve future predictions
5. **Advanced Features**: Add time-series prediction and patient similarity analysis
This implementation guide provides a comprehensive foundation for adding machine learning capabilities to your hospital data chatbot. By following these steps, you'll enable data-driven insights and predictions from your PostgreSQL database.

=== hospital-data-chatbot-ml/setup.py (python) ===
from setuptools import setup, find_packages
setup(
 name="hospital-data-chatbot",
 version="0.1.0",
 packages=find_packages(),
 include_package_data=True,
 python_requires=">=3.9",
)

=== hospital-data-chatbot-ml/setup_uv.sh (bash) ===
if ! command -v uv &> /dev/null; then
echo "Installing uv package manager..."
curl -LsSf https://astral.sh/uv/install.sh | sh
fi
uv venv
source .venv/bin/activate
uv pip install -e .
uv pip install -e ".[dev]"
echo "Setup complete! Activate the environment with: source .venv/bin/activate"

=== hospital-data-chatbot-ml/ml_guide.mermaid (text) ===
flowchart TD
subgraph DataSources["Data Sources"]
RDS[("AWS RDS<br>PostgreSQL")]
S3Raw[("AWS S3<br>Raw Data")]
end
subgraph DataProcessing["Data Processing"]
Lambda["AWS Lambda<br>Feature Engineering"]
S3Features[("AWS S3<br>Feature Store")]
end
subgraph MLPipeline["ML Pipeline"]
SageTrain["Amazon SageMaker<br>Training Jobs"]
SageModel["Amazon SageMaker<br>Model Registry"]
SageEndpoint["Amazon SageMaker<br>Endpoints"]
end
subgraph APILayer["API Layer"]
API["Amazon API Gateway"]
EC2["EC2 Instance<br>FastAPI Application"]
ELB["Elastic Load Balancer"]
end
subgraph Monitoring["Monitoring & Management"]
CloudWatch["Amazon CloudWatch"]
CloudTrail["AWS CloudTrail"]
SNS["Amazon SNS<br>Alerts"]
end
%% Connections
RDS --> Lambda
S3Raw --> Lambda
Lambda --> S3Features
S3Features --> SageTrain
SageTrain --> SageModel
SageModel --> SageEndpoint
Lambda --> EC2
SageEndpoint --> EC2
EC2 --> ELB
ELB --> API
SageEndpoint --> CloudWatch
EC2 --> CloudWatch
CloudWatch --> SNS
API --> CloudTrail
%% Style definitions
classDef aws fill:#FF9900,stroke:#232F3E,color:#232F3E,stroke-width:2px
classDef db fill:#3B48CC,stroke:#232F3E,color:white,stroke-width:2px
classDef storage fill:#3B48CC,stroke:#232F3E,color:white,stroke-width:2px
classDef api fill:#CC2264,stroke:#232F3E,color:white,stroke-width:2px
classDef compute fill:#EC7211,stroke:#232F3E,color:white,stroke-width:2px
classDef monitoring fill:#CC2264,stroke:#232F3E,color:white,stroke-width:2px
%% Apply styles
class RDS,S3Raw,S3Features db
class Lambda compute
class SageTrain,SageModel,SageEndpoint aws
class API,ELB api
class EC2 compute
class CloudWatch,CloudTrail,SNS monitoring

=== hospital-data-chatbot-ml/app/config/__init__.py (python) ===


=== hospital-data-chatbot-ml/app/config/settings.py (python) ===
import os
from typing import Dict, Any
class AppConfig:
    """Application configuration settings."""

    # Environment selection
    ENV = os.getenv('APP_ENV', 'dev_local')  # Options: dev_local, dev-cloud, stage, prod

    # Basic settings without defaults that would override env-specific configs
    DEBUG = os.getenv('DEBUG') == 'True' if os.getenv('DEBUG') else None
    PORT = os.getenv('PORT')
    DATA_DIR = os.getenv('DATA_DIR')

    # Environment-specific configurations
    _env_configs: Dict[str, Dict[str, Any]] = {
        'dev_local': {
            'DEBUG': True,
            'PORT': '8080',
            'DATA_DIR': 'data',
            'DB_HOST': 'localhost',
            'DB_PORT': 5432,
            'DB_NAME': 'hospital_data_test',
            'DB_USER': 'postgres',
            'DB_PASSWORD': 'postgres',
            'USE_S3': False,
            'AWS_REGION': 'ap-south-1',
            'LOG_LEVEL': 'DEBUG',
            'API_KEY_REQUIRED': False,
        },
        'dev-cloud': {
            'DEBUG': True,
            'PORT': '8080',
            'DATA_DIR': 'data',
            'DB_HOST': 'dev-aurora-cluster.cluster-xyz.ap-south-1.rds.amazonaws.com',
            'DB_PORT': 5432,
            'DB_NAME': 'hospital_data_dev',
            'DB_USER': 'dev_user',
            'DB_PASSWORD': os.getenv('DEV_DB_PASSWORD', 'dev_password'),
            'USE_S3': True,
            'S3_BUCKET': 'hospital-data-chatbot-dev',
            'AWS_REGION': 'ap-south-1',
            'LOG_LEVEL': 'DEBUG',
            'API_KEY_REQUIRED': True,
        },
        'stage': {
            'DEBUG': False,
            'PORT': '8080',
            'DATA_DIR': 'data',
            'DB_HOST': 'stage-aurora-cluster.cluster-xyz.ap-south-1.rds.amazonaws.com',
            'DB_PORT': 5432,
            'DB_NAME': 'hospital_data_stage',
            'DB_USER': 'stage_user',
            'DB_PASSWORD': os.getenv('STAGE_DB_PASSWORD', 'stage_password'),
            'USE_S3': True,
            'S3_BUCKET': 'hospital-data-chatbot-stage',
            'AWS_REGION': 'ap-south-1',
            'LOG_LEVEL': 'INFO',
            'API_KEY_REQUIRED': True,
        },
        'prod': {
            'DEBUG': False,
            'PORT': '8080',
            'DATA_DIR': 'data',
            'DB_HOST': 'prod-aurora-cluster.cluster-xyz.ap-south-1.rds.amazonaws.com',
            'DB_PORT': 5432,
            'DB_NAME': 'hospital_data_prod',
            'DB_USER': 'prod_user',
            'DB_PASSWORD': os.getenv('PROD_DB_PASSWORD', 'change_this_in_prod'),
            'USE_S3': True,
            'S3_BUCKET': 'hospital-data-chatbot-prod',
            'AWS_REGION': 'ap-south-1',
            'LOG_LEVEL': 'WARNING',
            'API_KEY_REQUIRED': True,
        }
    }

    # Apply environment-specific settings - first set defaults
    env_config = _env_configs.get(ENV, _env_configs['dev_local'])

    # Now apply the environment config
    for key, value in env_config.items():
        locals()[key] = value

    # Override with environment variables if they exist
    for key in env_config.keys():
        env_value = os.getenv(key)
        if env_value is not None:
            # Handle special cases for type conversion
            if key == 'DEBUG' or key == 'USE_S3' or key == 'API_KEY_REQUIRED':
                locals()[key] = env_value.lower() == 'true'
            elif key == 'DB_PORT':
                locals()[key] = int(env_value)
            else:
                locals()[key] = env_value

    # Bedrock settings
    BEDROCK_MODEL_ID = os.getenv(
        'BEDROCK_MODEL_ID',
        'anthropic.claude-3-sonnet-20240229-v1:0'
    )

    # Security settings - ensure API_KEY is always from env var if provided
    API_KEY = os.getenv('API_KEY', 'default_dev_key')  # Change in production!

    @classmethod
    def get_environment_name(cls) -> str:
        """Get a human-readable name for the current environment."""
  env_names={
   'dev_local': 'Development (Local)',
   'dev-cloud': 'Development (Cloud)',
   'stage': 'Staging',
   'prod': 'Production'
  }
  return env_names.get(cls.ENV, cls.ENV)
 @classmethod
 def is_development(cls)->bool:
        """Check if the current environment is a development environment."""
        return cls.ENV.startswith('dev')

    @classmethod
    def is_production(cls) -> bool:
        """Check if the current environment is production."""
  return cls.ENV=='prod'

=== hospital-data-chatbot-ml/app/utils/calculation_handler.py (python) ===
import re
import polars as pl
class CalculationHandler:
    """Handles calculations requested by the LLM."""

    def __init__(self, patient_data, diagnosis_data):
        self.datasets = {
            "patient_details": patient_data,
            "diagnosis_details": diagnosis_data
        }

    def process_response(self, llm_response):
        """
  Process the LLM response and execute any calculation requests.
  Args:
   llm_response: The response from the LLM
  Returns:
   Processed response with calculations executed
        """
        # Pattern to find calculation requests
        pattern = r'\[CALCULATE: ([^\]]+)\]'

        # Find all calculation requests
        calculation_requests = re.findall(pattern, llm_response)

        processed_response = llm_response

        # Execute each calculation
        for calc_request in calculation_requests:
            calc_result = self._execute_calculation(calc_request)

            # Replace the calculation request with the result
            processed_response = processed_response.replace(
                f"[CALCULATE: {calc_request}]",
                str(calc_result)
            )

        return processed_response

    def _execute_calculation(self, calc_request):
        """Execute a specific calculation request."""
  try:
   parts=calc_request.split('.')
   if len(parts)!=2:
    return "Error: Invalid calculation format. Use dataset.operation(column)"
   dataset_name=parts[0].strip()
   operation_part=parts[1].strip()
   if dataset_name not in self.datasets:
    return f"Error: Unknown dataset '{dataset_name}'"
   df=self.datasets[dataset_name]
   if "(" in operation_part and ")" in operation_part:
    operation=operation_part.split("(")[0].strip()
    column=operation_part.split("(")[1].split(")")[0].strip()
    if operation=="count" and column=="":
     return df.height
    if column and column not in df.columns:
     return f"Error: Column '{column}' not found in {dataset_name}"
    if operation=="mean" and column:
     result=df.select(pl.col(column).mean()).item()
     return f"{result:.2f}" if isinstance(result, float) else result
    elif operation=="min" and column:
     return df.select(pl.col(column).min()).item()
    elif operation=="max" and column:
     return df.select(pl.col(column).max()).item()
    elif operation=="sum" and column:
     result=df.select(pl.col(column).sum()).item()
     return f"{result:.2f}" if isinstance(result, float) else result
    elif operation=="median" and column:
     result=df.select(pl.col(column).median()).item()
     return f"{result:.2f}" if isinstance(result, float) else result
    elif operation=="std" and column:
     result=df.select(pl.col(column).std()).item()
     return f"{result:.2f}" if isinstance(result, float) else result
    elif operation=="count_unique" and column:
     return df.select(pl.col(column).n_unique()).item()
    elif operation=="count_by_patient" and dataset_name=="diagnosis_details":
     if "registry_id" not in df.columns:
      return "Error: registry_id column not found for patient grouping"
     result=df.group_by("registry_id").agg(pl.count().alias("diagnosis_count"))
     summary=(
      f"Min diagnoses per patient: {result.select(pl.min('diagnosis_count')).item()}, "
      f"Max: {result.select(pl.max('diagnosis_count')).item()}, "
      f"Avg: {result.select(pl.mean('diagnosis_count')).item():.2f}"
     )
     return summary
   return "Error: Unsupported operation"
  except Exception as e:
   return f"Error executing calculation: {str(e)}"

=== hospital-data-chatbot-ml/app/utils/logging.py (python) ===
import logging
import sys
import os
from datetime import datetime
from app.config.settings import AppConfig
def setup_logging(log_level=None, log_to_file=True):
    """
    Set up application logging.

    Args:
        log_level: Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
        log_to_file: Whether to log to a file in addition to console

    Returns:
        Logger instance for the application
    """
 if log_level is None:
  log_level_str=AppConfig.LOG_LEVEL
  log_level=getattr(logging, log_level_str, logging.INFO)
 logger=logging.getLogger('hospital_chatbot')
 logger.setLevel(log_level)
 if logger.handlers:
  logger.handlers=[]
 formatter=logging.Formatter(
  '%(asctime)s-%(name)s-%(levelname)s-%(filename)s:%(lineno)d-%(message)s'
 )
 console_handler=logging.StreamHandler(sys.stdout)
 console_handler.setLevel(log_level)
 console_handler.setFormatter(formatter)
 logger.addHandler(console_handler)
 if log_to_file:
  logs_dir=os.path.join(os.getcwd(), 'logs')
  os.makedirs(logs_dir, exist_ok=True)
  timestamp=datetime.now().strftime('%Y%m%d_%H%M%S')
  env_name=AppConfig.ENV
  log_file=os.path.join(logs_dir, f'hospital_chatbot_{env_name}_{timestamp}.log')
  file_handler=logging.FileHandler(log_file)
  file_handler.setLevel(log_level)
  file_handler.setFormatter(formatter)
  logger.addHandler(file_handler)
 logger.info(f"Logging initialized for {AppConfig.get_environment_name()} environment at level {log_level_str}")
 return logger
def get_logger(name=None):
    """
    Get a logger instance.

    Args:
        name: Logger name (optional)

    Returns:
        Logger instance
    """
 if name:
  return logging.getLogger(f'hospital_chatbot.{name}')
 else:
  return logging.getLogger('hospital_chatbot')
default_logger=setup_logging()

=== hospital-data-chatbot-ml/app/utils/db.py (python) ===
import psycopg2
import polars as pl
from psycopg2.extras import execute_values
from app.config.settings import AppConfig
from app.utils.logging import get_logger
logger=get_logger(__name__)
def _get_pg_type(polars_type):
    """Map Polars data types to PostgreSQL data types."""
    type_map = {
        pl.Int8: "TEXT",
        pl.Int16: "TEXT",
        pl.Int32: "TEXT",
        pl.Int64: "TEXT",
        pl.UInt8: "TEXT",
        pl.UInt16: "TEXT",
        pl.UInt32: "TEXT",
        pl.UInt64: "TEXT",
        pl.Float32: "TEXT",
        pl.Float64: "TEXT",
        pl.Boolean: "TEXT",
        pl.Utf8: "TEXT",
        pl.Date: "DATE",
        pl.Datetime: "TIMESTAMP",
        pl.Time: "TIME",
    }

    # Default to TEXT if type not found
    return type_map.get(polars_type, "TEXT")

def get_db_connection():
    """Get a connection to the PostgreSQL database."""
 try:
  logger.debug(f"Creating database connection to {AppConfig.DB_HOST}:{AppConfig.DB_PORT}/{AppConfig.DB_NAME}")
  conn=psycopg2.connect(
   host=AppConfig.DB_HOST,
   database=AppConfig.DB_NAME,
   user=AppConfig.DB_USER,
   password=AppConfig.DB_PASSWORD,
   port=AppConfig.DB_PORT,
   options="-c search_path=public"
  )
  logger.debug("Database connection established successfully")
  cursor=conn.cursor()
  cursor.execute("SET search_path TO public;")
  conn.commit()
  cursor.close()
  return conn
 except Exception as e:
  logger.error(f"Failed to connect to database: {str(e)}", exc_info=True)
  raise
def create_tables(conn, patient_df, diagnosis_df, drop_if_exists=False):
    """
    Create tables for patient data and diagnosis data with appropriate relationships.

    Args:
        conn: Database connection
        patient_df: Patient data DataFrame
        diagnosis_df: Diagnosis data DataFrame
        drop_if_exists: If True, drop existing tables before creating them (use only in development)
    """
 cursor=conn.cursor()
 try:
  if drop_if_exists:
   logger.warning("DROP_IF_EXISTS flag is True. Dropping existing tables!")
   logger.debug("Dropping diagnosis_details table if it exists")
   cursor.execute("DROP TABLE IF EXISTS public.diagnosis_details CASCADE;")
   logger.debug("Dropping patient_details table if it exists")
   cursor.execute("DROP TABLE IF EXISTS public.patient_details CASCADE;")
   conn.commit()
   logger.info("Existing tables dropped successfully")
  logger.debug("Creating patient_details table")
  create_dynamic_table(conn, "patient_details", patient_df)
  logger.debug("patient_details table created or already exists")
  logger.debug("Creating diagnosis_details table")
  create_dynamic_table(conn, "diagnosis_details", diagnosis_df)
  logger.debug("diagnosis_details table created or already exists")
  logger.debug("Setting registry_id as primary key on patient_details table")
  try:
            cursor.execute("""
            SELECT count(*) FROM pg_constraint
            WHERE conrelid = 'public.patient_details'::regclass
            AND contype = 'p';
            """)
   has_primary_key=cursor.fetchone()[0]>0
   if has_primary_key:
    logger.debug("Primary key already exists on patient_details table")
                cursor.execute("""
                SELECT a.attname
                FROM pg_index i
                JOIN pg_attribute a ON a.attrelid = i.indrelid AND a.attnum = ANY(i.indkey)
                WHERE i.indrelid = 'public.patient_details'::regclass
                AND i.indisprimary;
                """)
    pk_columns=[row[0] for row in cursor.fetchall()]
    logger.debug(f"Current primary key columns: {pk_columns}")
    if "registry_id" not in pk_columns:
     logger.debug("Dropping existing primary key to replace with registry_id")
                    cursor.execute("""
                    SELECT conname FROM pg_constraint
                    WHERE conrelid = 'public.patient_details'::regclass AND contype = 'p'
                    """)
     constraint_name=cursor.fetchone()[0]
                    cursor.execute(f"""
                    ALTER TABLE public.patient_details DROP CONSTRAINT IF EXISTS {constraint_name};
                    """)
                    cursor.execute("""
                    ALTER TABLE public.patient_details
                    ADD CONSTRAINT pk_patient_registry_id PRIMARY KEY (registry_id);
                    """)
     logger.debug("Primary key set to registry_id")
   else:
                cursor.execute("""
                ALTER TABLE public.patient_details
                ADD CONSTRAINT pk_patient_registry_id PRIMARY KEY (registry_id);
                """)
    logger.debug("Primary key constraint added on registry_id")
  except Exception as e:
   logger.warning(f"Could not set registry_id as primary key: {str(e)}")
   logger.warning("Foreign key relationship may be impacted. Check for duplicate registry_ids in patient data.")
  logger.debug("Checking if foreign key constraint exists")
        cursor.execute("""
        SELECT COUNT(*) FROM information_schema.table_constraints
        WHERE constraint_name = 'fk_diagnosis_patient'
        AND table_name = 'diagnosis_details'
        AND table_schema = 'public';
        """)
  fk_constraint_exists=cursor.fetchone()[0]>0
  logger.debug(f"Foreign key constraint exists: {fk_constraint_exists}")
  if not fk_constraint_exists:
   logger.debug("Adding foreign key constraint")
   try:
                cursor.execute("""
                ALTER TABLE public.diagnosis_details
                ADD CONSTRAINT fk_diagnosis_patient
                FOREIGN KEY (registry_id)
                REFERENCES public.patient_details (registry_id);
                """)
    logger.debug("Foreign key constraint added successfully")
   except Exception as e:
    logger.warning(f"Could not add foreign key constraint: {str(e)}")
    logger.warning("This may be due to lack of primary key or data integrity issues.")
  conn.commit()
  logger.debug("Schema changes committed")
 except Exception as e:
  conn.rollback()
  logger.error(f"Error updating schema: {str(e)}", exc_info=True)
  raise
 finally:
  cursor.close()
def create_dynamic_table(conn, table_name, df):
    """
    Dynamically create a table based on DataFrame schema.

    Args:
        conn: Database connection
        table_name: Name of the table to create
        df: DataFrame containing the schema information
    """
 cursor=conn.cursor()
 try:
  create_table_sql=f"CREATE TABLE IF NOT EXISTS public.{table_name} (\n"
  if table_name=="patient_details":
   create_table_sql+="    id SERIAL,\n"  # Not PRIMARY KEY
  else:
   create_table_sql+="    id SERIAL PRIMARY KEY,\n"
  for col_name, dtype in df.schema.items():
   col_name_snake=col_name.lower()
   pg_type=_get_pg_type(dtype)
   create_table_sql+=f"    {col_name_snake} {pg_type},\n"
  create_table_sql+="    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n"
  create_table_sql+=");"
  logger.debug(f"Executing SQL: {create_table_sql}")
  cursor.execute(create_table_sql)
  conn.commit()
  logger.debug(f"Table {table_name} created or already exists")
        cursor.execute("""
        SELECT EXISTS (
            SELECT FROM information_schema.tables
            WHERE table_schema = 'public'
            AND table_name = %s
        );
        """, (table_name,))
  exists=cursor.fetchone()[0]
  logger.debug(f"Table verification-{table_name} exists: {exists}")
 except Exception as e:
  logger.error(f"Error creating table {table_name}: {str(e)}", exc_info=True)
  raise
 finally:
  cursor.close()
def insert_data(conn, table_name, df):
    """
    Insert data into a table with validation.

    Args:
        conn: Database connection
        table_name: Name of the table to insert data into
        df: DataFrame containing the data to insert

    Returns:
        Dictionary containing insertion results including success/failure counts
    """
 cursor=conn.cursor()
 logger.info(f"Starting data validation and insertion for {table_name}")
 stats={
  "total_records": df.height,
  "valid_records": 0,
  "rejected_records": 0,
  "error_reasons": {}
 }
 try:
  columns=df.columns
  logger.debug(f"Columns for {table_name}: {columns}")
        cursor.execute(f"""
        SELECT column_name, data_type
        FROM information_schema.columns
        WHERE table_name = '{table_name}' AND table_schema = 'public'
        """)
  db_columns={row[0]: row[1] for row in cursor.fetchall()}
  logger.debug(f"Database schema for {table_name}: {db_columns}")
  valid_records=[]
  rejected_records=[]
  all_records=[tuple(row) for row in df.to_numpy()]
  for record_idx, record in enumerate(all_records):
   try:
    if len(record)!=len(columns):
     raise ValueError(f"Record has {len(record)} values but should have {len(columns)}")
    if table_name=="patient_details":
     registry_id_idx=columns.index("registry_id") if "registry_id" in columns else-1
     if registry_id_idx>=0:
      if not record[registry_id_idx] or str(record[registry_id_idx]).strip()=="":
       raise ValueError("Empty registry_id not allowed (primary key constraint)")
    elif table_name=="diagnosis_details":
     registry_id_idx=columns.index("registry_id") if "registry_id" in columns else-1
     if registry_id_idx>=0:
      if not record[registry_id_idx] or str(record[registry_id_idx]).strip()=="":
       raise ValueError("Empty registry_id not allowed (foreign key constraint)")
    for col_idx, col_name in enumerate(columns):
     if col_name in db_columns:
      value=record[col_idx]
      if value is None:
       continue
      if db_columns[col_name].startswith(('int', 'float', 'numeric', 'double', 'decimal')):
       try:
        if not isinstance(value, (int, float)) and value!='':
         float(value)  # Try to convert to validate
       except (ValueError, TypeError):
        raise ValueError(f"Invalid numeric value for {col_name}: {value}")
    valid_records.append(record)
   except Exception as e:
    rejected_records.append((record_idx, record, str(e)))
    error_type=str(e)
    if error_type in stats["error_reasons"]:
     stats["error_reasons"][error_type]+=1
    else:
     stats["error_reasons"][error_type]=1
  stats["valid_records"]=len(valid_records)
  stats["rejected_records"]=len(rejected_records)
  if rejected_records:
   logger.warning(f"Validation completed with {len(rejected_records)} rejected records out of {len(all_records)}")
   for i, (idx, record, reason) in enumerate(rejected_records[:5]):
    logger.warning(f"Rejected record #{idx}: {reason}")
    logger.debug(f"Record data: {record}")
   if len(rejected_records)>5:
    logger.warning(f"... and {len(rejected_records)-5} more rejected records")
  else:
   logger.info(f"All {len(valid_records)} records passed validation")
  if valid_records:
   sql=f"INSERT INTO public.{table_name} ({', '.join(columns)}) VALUES %s"
   logger.info(f"Inserting {len(valid_records)} valid records into public.{table_name}")
   try:
    execute_values(cursor, sql, valid_records)
    conn.commit()
    logger.info(f"Successfully inserted {len(valid_records)} records into {table_name}")
   except Exception as e:
    conn.rollback()
    logger.error(f"Database error during insertion: {str(e)}")
    stats["insertion_error"]=str(e)
    stats["valid_records"]=0  # Reset since insertion failed
    stats["rejected_records"]=df.height  # All records effectively rejected
  else:
   logger.warning(f"No valid records to insert into {table_name}")
  return stats
 except Exception as e:
  logger.error(f"Error in insert_data for {table_name}: {str(e)}", exc_info=True)
  stats["error"]=str(e)
  return stats
 finally:
  cursor.close()

=== hospital-data-chatbot-ml/app/utils/math_utils.py (python) ===


=== hospital-data-chatbot-ml/app/utils/ml_test.py (python) ===
import requests
import time
import json
API_URL="http://localhost:8080/api"
API_KEY="your_api_key"  # Replace with your actual API key
def train_model():
    """Train a readmission prediction model."""
    url = f"{API_URL}/ml/train-model"
    headers = {"Authorization": f"Bearer {API_KEY}"}
    payload = {
        "model_type": "readmission_prediction",
        "hyperparameters": {
            "objective": "binary:logistic",
            "num_round": "100",
            "max_depth": "6",
            "eta": "0.3",
            "eval_metric": "auc"
        }
    }

    response = requests.post(url, headers=headers, json=payload)
    if response.status_code == 200:
        result = response.json()
        print(f"Training job started: {result['details']['job_name']}")
        return result['details']['job_name']
    else:
        print(f"Error starting training job: {response.text}")
        return None

def check_model_status(training_job_name):
    """Check the status of a training job."""
 url=f"{API_URL}/ml/model-status/{training_job_name}"
 headers={"Authorization": f"Bearer {API_KEY}"}
 response=requests.get(url, headers=headers)
 if response.status_code==200:
  result=response.json()
  print(f"Training job status: {result['details']['status']}")
  return result['details']['status']
 else:
  print(f"Error checking training job status: {response.text}")
  return None
def deploy_model(training_job_name):
    """Deploy a trained model."""
    url = f"{API_URL}/ml/deploy-model"
    headers = {"Authorization": f"Bearer {API_KEY}"}
    payload = {
        "training_job_name": training_job_name,
        "instance_type": "ml.t2.medium"
    }

    response = requests.post(url, headers=headers, json=payload)
    if response.status_code == 200:
        result = response.json()
        print(f"Model deployment initiated: {result['details']['endpoint_name']}")
        return result['details']['endpoint_name']
    else:
        print(f"Error deploying model: {response.text}")
        return None

def get_patient_readmission_risk(patient_id):
    """Get readmission risk for a patient."""
 url=f"{API_URL}/ml/readmission-risk/{patient_id}"
 headers={"Authorization": f"Bearer {API_KEY}"}
 response=requests.get(url, headers=headers)
 if response.status_code==200:
  result=response.json()
  print(f"Patient {patient_id} readmission risk: {result['risk_percentage']}%")
  print(f"Risk level: {result['risk_level']}")
  print("Key risk factors:")
  for factor in result['key_factors']:
   print(f"-{factor['factor']}: {factor['value']} (Impact: {factor['impact']})")
  return result
 else:
  print(f"Error getting readmission risk: {response.text}")
  return None
def main():
    """Run the ML workflow demo."""
    print("Starting ML workflow demo...")

    # Train the model
    training_job_name = train_model()
    if not training_job_name:
        return

    # Check status until complete
    status = check_model_status(training_job_name)
    while status not in ["Completed", "Failed", "Stopped"]:
        print("Waiting for training job to complete...")
        time.sleep(30)
        status = check_model_status(training_job_name)

    if status != "Completed":
        print(f"Training job did not complete successfully: {status}")
        return

    # Deploy the model
    endpoint_name = deploy_model(training_job_name)
    if not endpoint_name:
        return

    # Wait for deployment to complete
    print("Waiting for deployment to complete...")
    time.sleep(60)

    # Get readmission risk for a patient
    patient_id = "P12345"  # Replace with an actual patient ID
    risk_result = get_patient_readmission_risk(patient_id)

    print("\nML workflow demo completed!")

if __name__ == "__main__":
    main()

=== hospital-data-chatbot-ml/app/utils/__init__.py (python) ===


=== hospital-data-chatbot-ml/app/utils/aws.py (python) ===
import boto3
import io
import os
from botocore.exceptions import ClientError
from app.config.settings import AppConfig
import logging
logger=logging.getLogger(__name__)
def get_s3_client():
    """
    Create and return an S3 client.

    Uses AWS credentials from environment variables or IAM role.
    """
 return boto3.client(
  's3',
  region_name=AppConfig.AWS_REGION
 )
def upload_to_s3(df, bucket_name, object_key):
    """
    Upload a dataframe to S3 as a CSV file.

    Args:
        df: A pandas or polars dataframe to upload
        bucket_name: S3 bucket name
        object_key: S3 object key (path/filename.csv)

    Returns:
        S3 URI of the uploaded file
    """
 try:
  s3_client=get_s3_client()
  csv_buffer=io.BytesIO()
  if hasattr(df, 'to_csv'):  # pandas DataFrame
   df.to_csv(csv_buffer, index=False)
  else:  # polars DataFrame
   if hasattr(df, 'to_pandas'):
    df.to_pandas().to_csv(csv_buffer, index=False)
   else:
    temp_path='/tmp/temp_csv.csv'
    df.write_csv(temp_path)
    with open(temp_path, 'rb') as f:
     csv_buffer.write(f.read())
    if os.path.exists(temp_path):
     os.remove(temp_path)
  csv_buffer.seek(0)
  s3_client.upload_fileobj(
   csv_buffer,
   bucket_name,
   object_key,
   ExtraArgs={
    'ContentType': 'text/csv'
   }
  )
  logger.info(f"File uploaded successfully to s3://{bucket_name}/{object_key}")
  return f"s3://{bucket_name}/{object_key}"
 except ClientError as e:
  logger.error(f"Error uploading to S3: {e}")
  raise
 except Exception as e:
  logger.error(f"Unexpected error during S3 upload: {e}")
  raise
def download_from_s3(bucket_name, object_key, local_path=None):
    """
    Download a file from S3.

    Args:
        bucket_name: S3 bucket name
        object_key: S3 object key
        local_path: Local path to save the file (optional)

    Returns:
        Local path of the downloaded file or file content as bytes
    """
 try:
  s3_client=get_s3_client()
  if local_path:
   os.makedirs(os.path.dirname(local_path), exist_ok=True)
   s3_client.download_file(bucket_name, object_key, local_path)
   logger.info(f"File downloaded from S3 to {local_path}")
   return local_path
  else:
   buffer=io.BytesIO()
   s3_client.download_fileobj(bucket_name, object_key, buffer)
   buffer.seek(0)
   logger.info(f"File downloaded from S3 to memory")
   return buffer.read()
 except ClientError as e:
  logger.error(f"Error downloading from S3: {e}")
  raise
 except Exception as e:
  logger.error(f"Unexpected error during S3 download: {e}")
  raise

=== hospital-data-chatbot-ml/app/ml/feature_engineering.py (python) ===
import polars as pl
import numpy as np
from typing import List, Dict, Any
from datetime import datetime, timedelta
from app.utils.db import get_db_connection
from app.utils.logging import get_logger
class FeatureEngineering:
    """Handles feature engineering for machine learning models."""

    def __init__(self):
        self.logger = get_logger(__name__)

    def extract_features(self, feature_set_name: str) -> pl.DataFrame:
        """
  Extract features from PostgreSQL database based on feature set name.
  Args:
   feature_set_name: Name of the feature set to extract
  Returns:
   Polars DataFrame with extracted features
        """
        if feature_set_name == "patient_risk_factors":
            return self._extract_patient_risk_features()
        elif feature_set_name == "diagnosis_clustering":
            return self._extract_diagnosis_clustering_features()
        elif feature_set_name == "readmission_prediction":
            return self._extract_readmission_features()
        else:
            raise ValueError(f"Unknown feature set: {feature_set_name}")

    def _extract_patient_risk_features(self) -> pl.DataFrame:
        """Extract features for patient risk stratification."""
  self.logger.info("Extracting patient risk features")
  conn=get_db_connection()
  try:
            query = """
            SELECT
                p.registry_id,
                p.age,
                p.gender,
                p.stay_duration,
                COUNT(d.id) AS diagnosis_count,
                AVG(CASE WHEN p.stay_duration IS NOT NULL THEN p.stay_duration ELSE NULL END)
                    OVER (PARTITION BY 1) AS avg_stay_duration,
                -- Extract chronic condition flags
                MAX(CASE WHEN d.diagnosis ILIKE '%diabetes%' THEN 1 ELSE 0 END) AS has_diabetes,
                MAX(CASE WHEN d.diagnosis ILIKE '%hypertension%' THEN 1 ELSE 0 END) AS has_hypertension,
                MAX(CASE WHEN d.diagnosis ILIKE '%heart%' THEN 1 ELSE 0 END) AS has_heart_condition,
                MAX(CASE WHEN d.diagnosis ILIKE '%kidney%' THEN 1 ELSE 0 END) AS has_kidney_condition,
                MAX(CASE WHEN d.diagnosis ILIKE '%liver%' THEN 1 ELSE 0 END) AS has_liver_condition,
                -- Count previous admissions (if you have that data)
                COUNT(DISTINCT CASE WHEN p.admission_date IS NOT NULL THEN p.admission_date ELSE NULL END) AS admission_count
            FROM
                patient_details p
            LEFT JOIN
                diagnosis_details d ON p.registry_id = d.registry_id
            GROUP BY
                p.registry_id, p.age, p.gender, p.stay_duration
            """
   df=pl.read_database(query=query, connection=conn)
   df=df.with_columns([
    pl.when(pl.col("age")<18).then(0)
      .when((pl.col("age")>=18) & (pl.col("age")<35)).then(1)
      .when((pl.col("age")>=35) & (pl.col("age")<50)).then(2)
      .when((pl.col("age")>=50) & (pl.col("age")<65)).then(3)
      .when((pl.col("age")>=65) & (pl.col("age")<80)).then(4)
      .otherwise(5)
      .alias("age_group"),
    pl.when(pl.col("gender").is_in(["M", "m", "Male", "male"])).then(1)
      .when(pl.col("gender").is_in(["F", "f", "Female", "female"])).then(0)
      .otherwise(None)
      .alias("gender_encoded"),
    (pl.col("stay_duration")/pl.col("avg_stay_duration")).alias("relative_stay_duration"),
    (pl.col("has_diabetes")+pl.col("has_hypertension")+
     pl.col("has_heart_condition")+pl.col("has_kidney_condition")+
     pl.col("has_liver_condition")).alias("comorbidity_score")
   ])
   numeric_cols=["age", "stay_duration", "relative_stay_duration", "comorbidity_score"]
   for col in numeric_cols:
    if col in df.columns:
     median_val=df[col].median()
     df=df.with_column(
      pl.col(col).fill_null(median_val)
     )
   self.logger.info(f"Extracted {df.height} patient risk feature records")
   return df
  finally:
   conn.close()
 def _extract_diagnosis_clustering_features(self)->pl.DataFrame:
        """Extract features for diagnosis clustering."""
        self.logger.info("Extracting diagnosis clustering features")

        conn = get_db_connection()
        try:
            # SQL to extract diagnosis features
            query = """
   SELECT
    d.diagnosis,
    AVG(p.age) AS avg_patient_age,
    AVG(p.stay_duration) AS avg_stay_duration,
    COUNT(DISTINCT p.registry_id) AS patient_count,
    SUM(CASE WHEN p.gender IN ('M', 'm', 'Male', 'male') THEN 1 ELSE 0 END) AS male_count,
    SUM(CASE WHEN p.gender IN ('F', 'f', 'Female', 'female') THEN 1 ELSE 0 END) AS female_count
   FROM
    diagnosis_details d
   JOIN
    patient_details p ON d.registry_id=p.registry_id
   GROUP BY
    d.diagnosis
   HAVING
    COUNT(DISTINCT p.registry_id)>=5--Only include diagnoses with at least 5 patients
            """

            df = pl.read_database(query=query, connection=conn)

            # Feature engineering
            df = df.with_columns([
                # Gender ratio
                (pl.col("male_count") / (pl.col("male_count") + pl.col("female_count"))).alias("male_ratio"),

                # Normalize patient count
                (pl.col("patient_count") / pl.col("patient_count").max()).alias("patient_count_normalized")
            ])

            # Clean diagnosis text for better clustering
            df = df.with_column(
                pl.col("diagnosis")
                  .str.to_lowercase()
                  .str.replace_all(r'[^\w\s]', '')  # Remove punctuation
                  .str.replace_all(r'\s+', ' ')     # Normalize whitespace
                  .alias("diagnosis_cleaned")
            )

            self.logger.info(f"Extracted {df.height} diagnosis clustering feature records")
            return df

        finally:
            conn.close()

    # Enhanced version for feature_engineering.py
    def extract_readmission_features(self) -> pl.DataFrame:
        """Extract more comprehensive features for readmission prediction."""
  conn=get_db_connection()
  try:
            query = """
            SELECT
                p.registry_id,
                p.age,
                p.gender,
                p.stay_duration,
                COUNT(d.id) AS diagnosis_count,
                -- Age-related features
                CASE
                    WHEN p.age < 18 THEN 0
                    WHEN p.age < 35 THEN 1
                    WHEN p.age < 50 THEN 2
                    WHEN p.age < 65 THEN 3
                    WHEN p.age < 80 THEN 4
                    ELSE 5
                END AS age_group,
                -- Common comorbidities
                MAX(CASE WHEN d.diagnosis ILIKE '%diabetes%' THEN 1 ELSE 0 END) AS has_diabetes,
                MAX(CASE WHEN d.diagnosis ILIKE '%hypertension%' THEN 1 ELSE 0 END) AS has_hypertension,
                MAX(CASE WHEN d.diagnosis ILIKE '%heart%' THEN 1 ELSE 0 END) AS has_heart_condition,
                MAX(CASE WHEN d.diagnosis ILIKE '%kidney%' THEN 1 ELSE 0 END) AS has_kidney_condition,
                MAX(CASE WHEN d.diagnosis ILIKE '%liver%' THEN 1 ELSE 0 END) AS has_liver_condition,
                -- Comorbidity score
                (MAX(CASE WHEN d.diagnosis ILIKE '%diabetes%' THEN 1 ELSE 0 END) +
                MAX(CASE WHEN d.diagnosis ILIKE '%hypertension%' THEN 1 ELSE 0 END) +
                MAX(CASE WHEN d.diagnosis ILIKE '%heart%' THEN 1 ELSE 0 END) +
                MAX(CASE WHEN d.diagnosis ILIKE '%kidney%' THEN 1 ELSE 0 END) +
                MAX(CASE WHEN d.diagnosis ILIKE '%liver%' THEN 1 ELSE 0 END)) AS comorbidity_score,
                -- Previous admissions info
                COUNT(DISTINCT CASE WHEN p.admission_date IS NOT NULL THEN p.admission_date ELSE NULL END) AS admission_count
            FROM
                patient_details p
            LEFT JOIN
                diagnosis_details d ON p.registry_id = d.registry_id
            GROUP BY
                p.registry_id, p.age, p.gender, p.stay_duration
            """
   df=pl.read_database(query=query, connection=conn)
   return df
  finally:
   conn.close()

=== hospital-data-chatbot-ml/app/ml/feature_store.py (python) ===
import polars as pl
from typing import Dict, List, Any, Optional
import json
import hashlib
import os
from datetime import datetime
from app.config.settings import AppConfig
from app.utils.logging import get_logger
from app.ml.feature_engineering import FeatureEngineering
class FeatureStore:
    """Manages feature storage and retrieval for ML models."""

    def __init__(self):
        self.logger = get_logger(__name__)
        self.feature_engineering = FeatureEngineering()
        self.storage_dir = os.path.join(AppConfig.DATA_DIR, 'features')
        os.makedirs(self.storage_dir, exist_ok=True)

    def get_feature_set(self, feature_set_name: str, force_refresh: bool = False) -> pl.DataFrame:
        """
  Get a feature set, either from cache or by generating it.
  Args:
   feature_set_name: Name of the feature set
   force_refresh: Whether to force regeneration of features
  Returns:
   DataFrame containing the features
        """
        file_path = os.path.join(self.storage_dir, f"{feature_set_name}.parquet")
        metadata_path = os.path.join(self.storage_dir, f"{feature_set_name}_metadata.json")

        # Check if we have a recent cached version
        if not force_refresh and os.path.exists(file_path) and os.path.exists(metadata_path):
            with open(metadata_path, 'r') as f:
                metadata = json.load(f)

            # Check if cache is recent enough (less than 24 hours old)
            cache_time = datetime.fromisoformat(metadata['timestamp'])
            age_hours = (datetime.now() - cache_time).total_seconds() / 3600

            if age_hours < 24:  # Cache is fresh
                self.logger.info(f"Loading {feature_set_name} features from cache")
                return pl.read_parquet(file_path)

        # Generate new features
        self.logger.info(f"Generating fresh {feature_set_name} features")
        features_df = self.feature_engineering.extract_features(feature_set_name)

        # Save to cache
        features_df.write_parquet(file_path)

        # Save metadata
        metadata = {
            'timestamp': datetime.now().isoformat(),
            'row_count': features_df.height,
            'column_count': features_df.width,
            'columns': features_df.columns
        }

        with open(metadata_path, 'w') as f:
            json.dump(metadata, f)

        return features_df

    def get_features_for_entity(self, feature_set_name: str, entity_id: str) -> Dict[str, Any]:
        """
  Get features for a specific entity (e.g., patient).
  Args:
   feature_set_name: Name of the feature set
   entity_id: ID of the entity (e.g., registry_id)
  Returns:
   Dictionary of feature values
        """
        features_df = self.get_feature_set(feature_set_name)

        # Find the entity in the DataFrame
        entity_filter = pl.col("registry_id") == entity_id
        entity_features = features_df.filter(entity_filter)

        if entity_features.height == 0:
            self.logger.warning(f"No features found for entity {entity_id} in {feature_set_name}")
            return {}

        # Convert to dictionary
        return entity_features.row(0, named=True)

=== hospital-data-chatbot-ml/app/ml/hospital_ml_models.py (python) ===
from app.ml.feature_store import FeatureStore
from app.ml.sagemaker_integration import SageMakerIntegration
import polars as pl
import boto3
import json
import os
class HospitalMLModels:
    """Enhanced ML models for hospital data analysis."""

    def __init__(self):
        self.feature_store = FeatureStore()
        self.sagemaker = SageMakerIntegration()

    def train_readmission_model(self):
        """Train the readmission prediction model."""
  features_df=self.feature_store.get_feature_set('readmission_prediction')
  hyperparameters={
   'objective': 'binary:logistic',
   'num_round': '100',
   'max_depth': '6',
   'eta': '0.3',
   'eval_metric': 'auc'
  }
  training_job=self.sagemaker.train_model(
   model_name="readmission-prediction",
   feature_set_name="readmission_prediction",
   target_column="readmitted_30_days",
   hyperparameters=hyperparameters
  )
  return training_job
 def deploy_readmission_model(self, training_job_name):
        """Deploy the readmission prediction model."""
        endpoint_info = self.sagemaker.deploy_model(training_job_name)
        return endpoint_info

    def get_readmission_risk(self, patient_id: str) -> Dict[str, Any]:
        """
  Predict the risk of 30-day readmission for a patient.
  Args:
   patient_id: The patient's registry ID
  Returns:
   Dictionary with readmission risk prediction and details
        """
        # Get the endpoint name from registry
        endpoint_name = self._get_latest_endpoint('readmission_prediction')

        if not endpoint_name:
            self.logger.warning("No readmission prediction model endpoint found")
            return {
                'status': 'error',
                'message': 'No deployed model available',
                'risk_score': None
            }

        # Get patient features
        patient_features = self.feature_store.get_features_for_entity(
            'readmission_prediction', patient_id
        )

        if not patient_features:
            self.logger.warning(f"No features found for patient {patient_id}")
            return {
                'status': 'error',
                'message': f'No data available for patient {patient_id}',
                'risk_score': None
            }

        # Remove target column if present
        if 'readmitted_30_days' in patient_features:
            del patient_features['readmitted_30_days']

        # Get prediction
        try:
            prediction = self.sagemaker.get_prediction(endpoint_name, patient_features)

            # Extract readmission risk score (format depends on the model)
            if isinstance(prediction['prediction'], dict) and 'score' in prediction['prediction']:
                risk_score = prediction['prediction']['score']
            elif isinstance(prediction['prediction'], list):
                risk_score = prediction['prediction'][0]
            else:
                risk_score = prediction['prediction']

            # Convert to percentage if between 0-1
            if isinstance(risk_score, (int, float)) and 0 <= risk_score <= 1:
                risk_percentage = risk_score * 100
            else:
                risk_percentage = risk_score

            risk_level = 'Low'
            if risk_percentage >= 70:
                risk_level = 'High'
            elif risk_percentage >= 30:
                risk_level = 'Medium'

            return {
                'status': 'success',
                'patient_id': patient_id,
                'risk_score': risk_score,
                'risk_percentage': round(risk_percentage, 1),
                'risk_level': risk_level,
                'key_factors': self._get_key_risk_factors(patient_features),
                'model_info': {
                    'endpoint': endpoint_name,
                    'timestamp': datetime.now().isoformat()
                }
            }

        except Exception as e:
            self.logger.error(f"Error getting readmission prediction: {str(e)}", exc_info=True)
            return {
                'status': 'error',
                'message': f'Prediction error: {str(e)}',
                'risk_score': None
            }

# app/ml/hospital_ml_models.py (continued)

    def get_patient_risk_stratification(self, patient_id: str = None) -> Dict[str, Any]:
        """
  Get risk stratification for a specific patient or all patients.
  Args:
   patient_id: Optional patient registry ID. If None, returns stratification for all patients.
  Returns:
   Dictionary with risk stratification results
        """
        # Get features from feature store
        features_df = self.feature_store.get_feature_set('patient_risk_factors')

        # If patient_id provided, filter for just that patient
        if patient_id:
            features_df = features_df.filter(pl.col('registry_id') == patient_id)

            if features_df.height == 0:
                return {
                    'status': 'error',
                    'message': f'No data found for patient {patient_id}'
                }

        # Define risk tiers based on features
        features_df = features_df.with_columns([
            # Compute a simple risk score based on age, comorbidities, and stay duration
            (
                # Age factor: 0-1 scale based on age group
                pl.when(pl.col("age") < 40).then(0.2)
                  .when((pl.col("age") >= 40) & (pl.col("age") < 60)).then(0.5)
                  .when((pl.col("age") >= 60) & (pl.col("age") < 80)).then(0.8)
                  .otherwise(1.0) * 0.4  # 40% weight

                # Comorbidity factor
                + pl.col("comorbidity_score").clip(0, 5) / 5 * 0.4  # 40% weight

                # Stay duration factor (compared to average)
                + pl.when(pl.col("relative_stay_duration") < 0.5).then(0.1)
                  .when((pl.col("relative_stay_duration") >= 0.5) & (pl.col("relative_stay_duration") < 1.0)).then(0.3)
                  .when((pl.col("relative_stay_duration") >= 1.0) & (pl.col("relative_stay_duration") < 1.5)).then(0.6)
                  .otherwise(0.9) * 0.2  # 20% weight
            ).alias("risk_score"),

            # Risk level classification
            pl.when(
                (pl.col("age") >= 70) &
                (pl.col("comorbidity_score") >= 3) &
                (pl.col("diagnosis_count") >= 5)
            ).then("very_high")
            .when(
                (pl.col("age") >= 60) &
                (pl.col("comorbidity_score") >= 2)
            ).then("high")
            .when(
                (pl.col("age") >= 50) &
                (pl.col("comorbidity_score") >= 1)
            ).then("moderate")
            .otherwise("low").alias("risk_level")
        ])

        # Calculate risk distribution
        risk_distribution = (
            features_df.group_by("risk_level")
            .agg(pl.count().alias("patient_count"))
            .sort("risk_level")
        )

        # Calculate overall stats
        total_patients = features_df.height
        avg_risk_score = features_df["risk_score"].mean()

        # Prepare result structure
        if patient_id:
            # Single patient result
            patient_data = features_df.row(0, named=True)

            return {
                'status': 'success',
                'patient_id': patient_id,
                'risk_score': round(float(patient_data['risk_score']), 2),
                'risk_level': patient_data['risk_level'],
                'risk_factors': {
                    'age': int(patient_data['age']),
                    'comorbidity_score': int(patient_data['comorbidity_score']),
                    'diagnosis_count': int(patient_data['diagnosis_count']),
                    'relative_stay_duration': round(float(patient_data['relative_stay_duration']), 2)
                }
            }
        else:
            # Population-level results
            return {
                'status': 'success',
                'total_patients': total_patients,
                'average_risk_score': round(float(avg_risk_score), 2),
                'risk_distribution': risk_distribution.to_dicts(),
                'high_risk_count': features_df.filter(
                    pl.col('risk_level').is_in(['high', 'very_high'])
                ).height,
                'timestamp': datetime.now().isoformat()
            }

    def get_diagnosis_clusters(self, min_cluster_size: int = 5) -> Dict[str, Any]:
        """
  Get clusters of similar diagnoses based on pattern analysis.
  Args:
   min_cluster_size: Minimum number of diagnoses in a cluster
  Returns:
   Dictionary with diagnosis clustering results
        """
        try:
            # Get features for clustering
            features_df = self.feature_store.get_feature_set('diagnosis_clustering')

            # Basic implementation: create diagnosis groups based on key terms
            # In a real implementation, you would use a proper clustering algorithm here
            # with ML models trained on diagnosis text similarity

            # Define some common diagnosis patterns for grouping
            diagnosis_patterns = {
                'cardiovascular': ['heart', 'cardio', 'hypertension', 'angina', 'artery', 'stroke'],
                'respiratory': ['lung', 'pneumonia', 'copd', 'asthma', 'bronchitis', 'respiratory'],
                'gastrointestinal': ['stomach', 'intestinal', 'bowel', 'liver', 'pancreas', 'appendicitis'],
                'neurological': ['brain', 'neuro', 'seizure', 'epilepsy', 'alzheimer', 'parkinsons'],
                'endocrine': ['diabetes', 'thyroid', 'hormone', 'insulin', 'endocrine'],
                'infectious': ['infection', 'viral', 'bacterial', 'sepsis', 'influenza', 'covid'],
            }

            # Assign diagnoses to clusters
            diagnosis_groups = {}
            for diagnosis_row in features_df.to_dicts():
                diagnosis = diagnosis_row.get('diagnosis_cleaned', '').lower()

                # Find matching pattern
                assigned_group = 'other'
                for group, patterns in diagnosis_patterns.items():
                    if any(pattern in diagnosis for pattern in patterns):
                        assigned_group = group
                        break

                # Add to group
                if assigned_group not in diagnosis_groups:
                    diagnosis_groups[assigned_group] = []

                diagnosis_groups[assigned_group].append({
                    'diagnosis': diagnosis_row.get('diagnosis'),
                    'patient_count': diagnosis_row.get('patient_count'),
                    'avg_age': round(diagnosis_row.get('avg_patient_age'), 1),
                    'avg_stay': round(diagnosis_row.get('avg_stay_duration'), 1),
                    'male_ratio': round(diagnosis_row.get('male_ratio', 0.5), 2)
                })

            # Calculate stats for each group
            group_stats = {}
            for group, diagnoses in diagnosis_groups.items():
                if len(diagnoses) < min_cluster_size and group != 'other':
                    # Add to 'other' if below minimum size
                    if 'other' not in diagnosis_groups:
                        diagnosis_groups['other'] = []
                    diagnosis_groups['other'].extend(diagnoses)
                    continue

                # Calculate group stats
                total_patients = sum(d.get('patient_count', 0) for d in diagnoses)
                avg_age = sum(d.get('avg_age', 0) * d.get('patient_count', 0) for d in diagnoses) / total_patients if total_patients > 0 else 0
                avg_stay = sum(d.get('avg_stay', 0) * d.get('patient_count', 0) for d in diagnoses) / total_patients if total_patients > 0 else 0

                group_stats[group] = {
                    'diagnosis_count': len(diagnoses),
                    'total_patients': total_patients,
                    'avg_age': round(avg_age, 1),
                    'avg_stay': round(avg_stay, 1),
                    'top_diagnoses': sorted(diagnoses, key=lambda d: d.get('patient_count', 0), reverse=True)[:5]
                }

            # Remove empty groups and those below threshold
            group_stats = {k: v for k, v in group_stats.items() if v['diagnosis_count'] >= min_cluster_size or k == 'other'}

            return {
                'status': 'success',
                'cluster_count': len(group_stats),
                'clusters': group_stats,
                'timestamp': datetime.now().isoformat()
            }

        except Exception as e:
            self.logger.error(f"Error getting diagnosis clusters: {str(e)}", exc_info=True)
            return {
                'status': 'error',
                'message': f'Error generating diagnosis clusters: {str(e)}'
            }

    def _get_key_risk_factors(self, patient_features: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Extract key risk factors from patient features."""
  risk_factors=[]
  if 'age' in patient_features:
   age=patient_features['age']
   if age>=65:
    risk_factors.append({
     'factor': 'Advanced Age',
     'value': f"{age} years",
     'impact': 'high' if age>=75 else 'medium'
    })
  comorbidities=[]
  if patient_features.get('has_diabetes', 0)==1:
   comorbidities.append('Diabetes')
  if patient_features.get('has_hypertension', 0)==1:
   comorbidities.append('Hypertension')
  if patient_features.get('has_heart_condition', 0)==1:
   comorbidities.append('Heart Disease')
  if patient_features.get('has_kidney_condition', 0)==1:
   comorbidities.append('Kidney Disease')
  if patient_features.get('has_liver_condition', 0)==1:
   comorbidities.append('Liver Disease')
  if comorbidities:
   risk_factors.append({
    'factor': 'Comorbidities',
    'value': ', '.join(comorbidities),
    'impact': 'high' if len(comorbidities)>=2 else 'medium'
   })
  diagnosis_count=patient_features.get('diagnosis_count', 0)
  if diagnosis_count>=3:
   risk_factors.append({
    'factor': 'Multiple Diagnoses',
    'value': f"{diagnosis_count} diagnoses",
    'impact': 'high' if diagnosis_count>=5 else 'medium'
   })
  stay_duration=patient_features.get('stay_duration', 0)
  if stay_duration>=7:
   risk_factors.append({
    'factor': 'Extended Hospital Stay',
    'value': f"{stay_duration} days",
    'impact': 'high' if stay_duration>=14 else 'medium'
   })
  return risk_factors
 def _load_model_registry(self)->Dict[str, Any]:
        """Load the model registry from disk or create a new one."""
        if os.path.exists(self.registry_path):
            try:
                with open(self.registry_path, 'r') as f:
                    return json.load(f)
            except Exception as e:
                self.logger.error(f"Error loading model registry: {str(e)}", exc_info=True)

        # Create a new registry if it doesn't exist or couldn't be loaded
        registry = {
            'models': {},
            'endpoints': {},
            'created_at': datetime.now().isoformat(),
            'updated_at': datetime.now().isoformat()
        }

        with open(self.registry_path, 'w') as f:
            json.dump(registry, f, indent=2)

        return registry

    def _get_latest_endpoint(self, model_type: str) -> Optional[str]:
        """Get the most recent endpoint for a given model type."""
  if 'endpoints' not in self.model_registry:
   return None
  matching_endpoints=[]
  for endpoint_name, endpoint_info in self.model_registry['endpoints'].items():
   if endpoint_info.get('model_type')==model_type and endpoint_info.get('status')=='InService':
    matching_endpoints.append((endpoint_name, endpoint_info))
  if not matching_endpoints:
   return None
  matching_endpoints.sort(key=lambda x: x[1].get('created_at', ''), reverse=True)
  return matching_endpoints[0][0]

=== hospital-data-chatbot-ml/app/ml/sagemaker_integration.py (python) ===
import boto3
import json
import os
import numpy as np
import polars as pl
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import time
import io
from app.config.settings import AppConfig
from app.utils.logging import get_logger
from app.ml.feature_store import FeatureStore
class SageMakerIntegration:
    """Handles integration with AWS SageMaker for ML model training and inference."""

    def __init__(self):
        self.logger = get_logger(__name__)
        self.sagemaker_client = boto3.client('sagemaker', region_name=AppConfig.AWS_REGION)
        self.sagemaker_runtime = boto3.client('sagemaker-runtime', region_name=AppConfig.AWS_REGION)
        self.feature_store = FeatureStore()

# Enhanced SageMaker integration with better error handling

    def train_model(self, model_name, feature_set_name, target_column, instance_type="ml.m5.large", hyperparameters=None):
        """
  Train a new model using SageMaker with enhanced error handling.
  Args:
   model_name: Name of the model
   feature_set_name: Name of the feature set
   target_column: Target column for prediction
   instance_type: SageMaker instance type
   hyperparameters: Model hyperparameters
  Returns:
   Dictionary with training job information
        """
        try:
            # Get features
            features_df = self.feature_store.get_feature_set(feature_set_name)

            # Split train/test
            train_df, test_df = self._split_train_test(features_df)

            # Upload to S3
            s3_bucket = AppConfig.S3_BUCKET
            s3_prefix = f"sagemaker/{model_name}"
            training_data_key = f"{s3_prefix}/train/train.csv"
            test_data_key = f"{s3_prefix}/test/test.csv"

            # Upload data to S3
            upload_to_s3(train_df, s3_bucket, training_data_key)
            upload_to_s3(test_df, s3_bucket, test_data_key)

            # Create SageMaker training job
            training_job_name = f"{model_name}-{int(time.time())}"

            # Use XGBoost as default algorithm
            image_uri = f"683313688378.dkr.ecr.{AppConfig.AWS_REGION}.amazonaws.com/sagemaker-xgboost:1.5-1"

            # Set hyperparameters
            if not hyperparameters:
                # Determine if binary or multiclass classification
                unique_values = pl.Series(train_df[target_column]).unique()
                num_classes = len(unique_values)

                hyperparameters = {
                    'objective': 'binary:logistic' if num_classes <= 2 else 'multi:softmax',
                    'num_round': '100',
                    'max_depth': '6',
                    'eta': '0.3',
                    'eval_metric': 'auc' if num_classes <= 2 else 'merror',
                    'num_class': str(num_classes) if num_classes > 2 else None
                }
                # Remove None values
                hyperparameters = {k: v for k, v in hyperparameters.items() if v is not None}

            # Create training job
            response = self.sagemaker_client.create_training_job(
                TrainingJobName=training_job_name,
                AlgorithmSpecification={
                    'TrainingImage': image_uri,
                    'TrainingInputMode': 'File'
                },
                RoleArn=AppConfig.SAGEMAKER_ROLE_ARN,
                InputDataConfig=[
                    {
                        'ChannelName': 'train',
                        'DataSource': {
                            'S3DataSource': {
                                'S3DataType': 'S3Prefix',
                                'S3Uri': f"s3://{s3_bucket}/{s3_prefix}/train",
                                'S3DataDistributionType': 'FullyReplicated'
                            }
                        },
                        'ContentType': 'text/csv'
                    },
                    {
                        'ChannelName': 'validation',
                        'DataSource': {
                            'S3DataSource': {
                                'S3DataType': 'S3Prefix',
                                'S3Uri': f"s3://{s3_bucket}/{s3_prefix}/test",
                                'S3DataDistributionType': 'FullyReplicated'
                            }
                        },
                        'ContentType': 'text/csv'
                    }
                ],
                OutputDataConfig={
                    'S3OutputPath': f"s3://{s3_bucket}/{s3_prefix}/output"
                },
                ResourceConfig={
                    'InstanceType': instance_type,
                    'InstanceCount': 1,
                    'VolumeSizeInGB': 30
                },
                StoppingCondition={
                    'MaxRuntimeInSeconds': 86400  # 24 hours
                },
                HyperParameters=hyperparameters
            )

            self.logger.info(f"Started training job: {training_job_name}")

            return {
                'job_name': training_job_name,
                'model_name': model_name,
                'feature_set': feature_set_name,
                'target_column': target_column,
                'algorithm': 'xgboost',
                'hyperparameters': hyperparameters,
                'timestamp': datetime.now().isoformat()
            }

        except Exception as e:
            self.logger.error(f"Error training model: {str(e)}", exc_info=True)
            raise ValueError(f"Failed to train model: {str(e)}")

    def deploy_model(self, training_job_name: str, instance_type: str = 'ml.t2.medium') -> Dict[str, Any]:
        """
  Deploy a trained model to a SageMaker endpoint.
  Args:
   training_job_name: Name of the completed training job
   instance_type: Instance type for the endpoint
  Returns:
   Dictionary with endpoint information
        """
        # Wait for training job to complete
        self.sagemaker_client.get_waiter('training_job_completed_or_stopped').wait(
            TrainingJobName=training_job_name
        )

        # Get training job info
        training_job = self.sagemaker_client.describe_training_job(
            TrainingJobName=training_job_name
        )

        # Create model
        model_name = f"{training_job_name}-model"
        model_data_url = training_job['ModelArtifacts']['S3ModelArtifacts']
        primary_container = {
            'Image': training_job['AlgorithmSpecification']['TrainingImage'],
            'ModelDataUrl': model_data_url
        }

        self.sagemaker_client.create_model(
            ModelName=model_name,
            PrimaryContainer=primary_container,
            ExecutionRoleArn=AppConfig.SAGEMAKER_ROLE_ARN
        )

        # Create endpoint configuration
        endpoint_config_name = f"{model_name}-config"
        self.sagemaker_client.create_endpoint_config(
            EndpointConfigName=endpoint_config_name,
            ProductionVariants=[
                {
                    'VariantName': 'AllTraffic',
                    'ModelName': model_name,
                    'InstanceType': instance_type,
                    'InitialInstanceCount': 1
                }
            ]
        )

        # Create endpoint
        endpoint_name = f"{model_name}-endpoint"
        self.sagemaker_client.create_endpoint(
            EndpointName=endpoint_name,
            EndpointConfigName=endpoint_config_name
        )

        self.logger.info(f"Endpoint {endpoint_name} deployment initiated")

        return {
            'endpoint_name': endpoint_name,
            'model_name': model_name,
            'training_job': training_job_name,
            'instance_type': instance_type,
            'status': 'Creating',
            'timestamp': datetime.now().isoformat()
        }

    def get_prediction(self, endpoint_name: str, features: Dict[str, Any]) -> Dict[str, Any]:
        """
  Get a prediction from a deployed model.
  Args:
   endpoint_name: Name of the SageMaker endpoint
   features: Dictionary of feature values
  Returns:
   Dictionary with prediction results
        """
        # Convert features to CSV
        feature_df = pl.DataFrame([features])

        # Convert to CSV without headers
        csv_buffer = io.StringIO()
        feature_df.write_csv(csv_buffer, include_header=False)
        csv_data = csv_buffer.getvalue()

        # Get prediction from endpoint
        response = self.sagemaker_runtime.invoke_endpoint(
            EndpointName=endpoint_name,
            ContentType='text/csv',
            Body=csv_data
        )

        # Parse response
        result = json.loads(response['Body'].read().decode())

        return {
            'prediction': result,
            'features': features,
            'endpoint': endpoint_name,
            'timestamp': datetime.now().isoformat()
        }

    def _split_train_test(self, df: pl.DataFrame, test_size: float = 0.2) -> Tuple[pl.DataFrame, pl.DataFrame]:
        """Split a DataFrame into training and test sets."""
  n_rows=df.height
  random_values=np.random.random(n_rows)
  df=df.with_column(pl.Series("_split_col", random_values))
  train_df=df.filter(pl.col('_split_col')>=test_size)
  test_df=df.filter(pl.col('_split_col')<test_size)
  train_df=train_df.drop('_split_col')
  test_df=test_df.drop('_split_col')
  return train_df, test_df
 def batch_predict(self, endpoint_name: str, features_list: List[Dict[str, Any]],
     reference_ids: List[str]=None)->List[Dict[str, Any]]:
        """
        Get predictions for multiple inputs from a deployed model.

        Args:
            endpoint_name: Name of the SageMaker endpoint
            features_list: List of feature dictionaries
            reference_ids: Optional list of reference IDs (e.g., patient IDs)

        Returns:
            List of prediction results
        """
  try:
   if len(features_list)<20:
    csv_buffer=io.StringIO()
    feature_df=pl.DataFrame(features_list)
    feature_df.write_csv(csv_buffer, include_header=True)
    csv_data=csv_buffer.getvalue()
    response=self.sagemaker_runtime.invoke_endpoint(
     EndpointName=endpoint_name,
     ContentType='text/csv',
     Body=csv_data,
     Accept='application/json'
    )
    result=json.loads(response['Body'].read().decode())
    if not isinstance(result, list):
     result=[result]*len(features_list)
    if reference_ids:
     for i, r in enumerate(result):
      if i<len(reference_ids):
       r['reference_id']=reference_ids[i]
    return result
   else:
    batch_job_name=f"{endpoint_name}-batch-{int(time.time())}"
    s3_input_path=self._save_features_to_s3(features_list, reference_ids)
    s3_output_path=f"s3://{AppConfig.S3_BUCKET}/batch-predictions/{batch_job_name}"
    response=self.sagemaker_client.create_transform_job(
     TransformJobName=batch_job_name,
     ModelName=self._get_model_name_from_endpoint(endpoint_name),
     TransformInput={
      'DataSource': {
       'S3DataSource': {
        'S3DataType': 'S3Prefix',
        'S3Uri': s3_input_path
       }
      },
      'ContentType': 'text/csv',
      'SplitType': 'Line'
     },
     TransformOutput={
      'S3OutputPath': s3_output_path,
      'Accept': 'application/json',
      'AssembleWith': 'Line'
     },
     TransformResources={
      'InstanceType': 'ml.m5.large',
      'InstanceCount': 1
     }
    )
    self.sagemaker_client.get_waiter('transform_job_completed_or_stopped').wait(
     TransformJobName=batch_job_name
    )
    results=self._get_batch_results_from_s3(s3_output_path)
    return results
  except Exception as e:
   self.logger.error(f"Error in batch prediction: {str(e)}", exc_info=True)
   raise
 def _save_features_to_s3(self, features_list: List[Dict[str, Any]],
       reference_ids: List[str]=None)->str:
        """Save features to S3 for batch processing."""
        # Convert to DataFrame
        df = pl.DataFrame(features_list)

        # Add reference IDs if provided
        if reference_ids:
            df = df.with_column(pl.Series("reference_id", reference_ids))

        # Save to CSV in memory
        csv_buffer = io.BytesIO()
        df.write_csv(csv_buffer, include_header=True)
        csv_buffer.seek(0)

        # Upload to S3
        bucket = AppConfig.S3_BUCKET
        timestamp = int(time.time())
        key = f"batch-input/features-{timestamp}.csv"

        s3_client = boto3.client('s3')
        s3_client.upload_fileobj(
            csv_buffer,
            bucket,
            key,
            ExtraArgs={'ContentType': 'text/csv'}
        )

        return f"s3://{bucket}/{key}"

    def _get_batch_results_from_s3(self, s3_output_path: str) -> List[Dict[str, Any]]:
        """Get batch prediction results from S3."""
  bucket, prefix=self._parse_s3_uri(s3_output_path)
  s3_client=boto3.client('s3')
  response=s3_client.list_objects_v2(
   Bucket=bucket,
   Prefix=prefix
  )
  results=[]
  for obj in response.get('Contents', []):
   if obj['Key'].endswith('/'):
    continue
   output=io.BytesIO()
   s3_client.download_fileobj(bucket, obj['Key'], output)
   output.seek(0)
   for line in output.readlines():
    try:
     result=json.loads(line.decode('utf-8'))
     results.append(result)
    except json.JSONDecodeError:
     self.logger.warning(f"Could not parse line as JSON: {line}")
  return results
 def _parse_s3_uri(self, uri: str)->Tuple[str, str]:
        """Parse S3 URI into bucket and prefix."""
        # Remove 's3://' prefix
        path = uri.replace('s3://', '')

        # Split into bucket and prefix
        parts = path.split('/', 1)
        bucket = parts[0]
        prefix = parts[1] if len(parts) > 1 else ''

        return bucket, prefix

    def _get_model_name_from_endpoint(self, endpoint_name: str) -> str:
        """Get the model name associated with an endpoint."""
  response=self.sagemaker_client.describe_endpoint(
   EndpointName=endpoint_name
  )
  endpoint_config_name=response['EndpointConfigName']
  config_response=self.sagemaker_client.describe_endpoint_config(
   EndpointConfigName=endpoint_config_name
  )
  model_name=config_response['ProductionVariants'][0]['ModelName']
  return model_name

=== hospital-data-chatbot-ml/app/ml/mcp/service.py (python) ===
from fastapi import FastAPI, HTTPException, Depends
from app.ml.mcp.protocol import (
 ModelRequest, ModelResponse, ModelType,
 ModelMetadata, ModelExplanation, ExplanationFormat
)
from app.ml.sagemaker_integration import SageMakerIntegration
from app.ml.feature_store import FeatureStore
from app.utils.logging import get_logger
from typing import Dict, List, Any, Optional
import time
import json
logger=get_logger(__name__)
class MCPModelService:
    """Model Context Protocol service for hospital ML models"""

    def __init__(self):
        self.sagemaker = SageMakerIntegration()
        self.feature_store = FeatureStore()
        self.model_registry = self._load_model_registry()

    def _load_model_registry(self) -> Dict[str, ModelMetadata]:
        """Load model registry with all available models"""
  registry={}
  registry["readmission-risk-xgboost-v1"]=ModelMetadata(
   model_id="readmission-risk-xgboost-v1",
   version="1.0.0",
   model_type=ModelType.READMISSION_RISK,
   description="XGBoost model for 30-day readmission risk prediction",
   created_at="2025-05-01T10:00:00Z",
   accuracy=0.82,
   capabilities=["prediction", "explanation", "feature_importance"],
   input_schema={"patient_id": "string"},
   output_schema={
    "risk_score": "float",
    "risk_level": "string",
    "risk_factors": "array"
   },
   feature_names=[
    "age", "gender", "stay_duration", "diagnosis_count",
    "has_diabetes", "has_hypertension", "has_heart_condition",
    "comorbidity_score"
   ]
  )
  registry["patient-risk-xgboost-v1"]=ModelMetadata(
   model_id="patient-risk-xgboost-v1",
   version="1.0.0",
   model_type=ModelType.PATIENT_RISK,
   description="XGBoost model for patient risk stratification",
   created_at="2025-04-15T14:30:00Z",
   accuracy=0.79,
   capabilities=["prediction", "explanation", "feature_importance"],
   input_schema={"patient_id": "string"},
   output_schema={
    "risk_score": "float",
    "risk_level": "string"
   },
   feature_names=[
    "age", "gender", "stay_duration", "diagnosis_count",
    "has_diabetes", "has_hypertension", "has_heart_condition",
    "comorbidity_score"
   ]
  )
  return registry
 def get_models(self)->List[ModelMetadata]:
        """Get list of all available models"""
        return list(self.model_registry.values())

    def get_model_metadata(self, model_id: str) -> ModelMetadata:
        """Get metadata for a specific model"""
  if model_id not in self.model_registry:
   raise ValueError(f"Model {model_id} not found")
  return self.model_registry[model_id]
 def predict(self, request: ModelRequest)->ModelResponse:
        """Make a prediction using the MCP protocol"""
        start_time = time.time()

        # Validate model exists
        if request.model_id not in self.model_registry:
            raise ValueError(f"Model {request.model_id} not found")

        model_metadata = self.model_registry[request.model_id]
        model_type = model_metadata.model_type

        # Route to appropriate prediction function based on model type
        if model_type == ModelType.READMISSION_RISK:
            result = self._predict_readmission_risk(request)
        elif model_type == ModelType.PATIENT_RISK:
            result = self._predict_patient_risk(request)
        else:
            raise ValueError(f"Unsupported model type: {model_type}")

        # Log performance
        execution_time = time.time() - start_time
        logger.info(f"Model {request.model_id} prediction completed in {execution_time:.2f}s")

        return result

    def _predict_readmission_risk(self, request: ModelRequest) -> ModelResponse:
        """Predict readmission risk using SageMaker endpoint"""
  patient_id=request.context.patient_id
  if not patient_id:
   if "patient_id" in request.inputs:
    patient_id=request.inputs["patient_id"]
   else:
    raise ValueError("Patient ID not provided in context or inputs")
  features=self.feature_store.get_features_for_entity(
   "readmission_prediction", patient_id
  )
  if not features:
   raise ValueError(f"No features found for patient {patient_id}")
  endpoint_name=self._get_endpoint_name(request.model_id)
  sagemaker_response=self.sagemaker.get_prediction(endpoint_name, features)
  prediction_result=sagemaker_response.get("prediction", {})
  risk_score=prediction_result.get("score", 0.0)
  risk_level="Low"
  if risk_score>=0.7:
   risk_level="High"
  elif risk_score>=0.3:
   risk_level="Medium"
  explanation=None
  if request.context.include_explanations:
   feature_importance=self._get_feature_importance(
    request.model_id,
    features,
    prediction_result
   )
   explanation=ModelExplanation(
    format=ExplanationFormat.FEATURE_IMPORTANCE,
    explanation="Key factors contributing to readmission risk",
    importance_scores=feature_importance
   )
  response=ModelResponse(
   model_id=request.model_id,
   version=self.model_registry[request.model_id].version,
   prediction={
    "risk_score": risk_score,
    "risk_percentage": round(risk_score*100, 1),
    "risk_level": risk_level,
    "patient_id": patient_id,
    "key_factors": self._get_key_risk_factors(features, feature_importance if explanation else None)
   },
   confidence=prediction_result.get("confidence", 0.8),
   explanation=explanation,
   metadata={
    "inference_time_ms": prediction_result.get("inference_time_ms", 0),
    "model_type": ModelType.READMISSION_RISK.value
   }
  )
  return response
 def _predict_patient_risk(self, request: ModelRequest)->ModelResponse:
        """Predict patient risk using SageMaker endpoint"""
        # Implementation similar to _predict_readmission_risk
        # but for patient risk stratification model
        pass

    def _get_endpoint_name(self, model_id: str) -> str:
        """Get SageMaker endpoint name for model ID"""
  endpoint_mapping={
   "readmission-risk-xgboost-v1": "hospital-data-chatbot-dev-readmission-endpoint",
   "patient-risk-xgboost-v1": "hospital-data-chatbot-dev-patient-risk-endpoint"
  }
  if model_id not in endpoint_mapping:
   raise ValueError(f"No endpoint found for model {model_id}")
  return endpoint_mapping[model_id]
 def _get_feature_importance(
  self, model_id: str, features: Dict[str, Any], prediction_result: Dict[str, Any]
 )->Dict[str, float]:
        """Get feature importance scores"""
        # This would typically come from the model explanation
        # For XGBoost models, we'd use SHAP values
        # For now, return dummy values
        feature_importance = {}

        # Use any feature importance from SageMaker response
        if "feature_importance" in prediction_result:
            return prediction_result["feature_importance"]

        # Otherwise generate some dummy values
        metadata = self.model_registry[model_id]
        for feature in metadata.feature_names:
            if feature in features:
                # Random importance score between 0 and 1
                import random
                feature_importance[feature] = random.random()

        # Normalize to sum to 1
        total = sum(feature_importance.values())
        if total > 0:
            feature_importance = {k: v/total for k, v in feature_importance.items()}

        return feature_importance

    def _get_key_risk_factors(
        self, patient_features: Dict[str, Any], feature_importance: Optional[Dict[str, float]] = None
    ) -> List[Dict[str, Any]]:
        """Extract key risk factors from patient features and importance scores"""
  risk_factors=[]
  if feature_importance:
   sorted_features=sorted(
    feature_importance.items(),
    key=lambda x: x[1],
    reverse=True
   )
   for feature, importance in sorted_features[:5]:
    if feature in patient_features:
     value=patient_features[feature]
     impact="high" if importance>0.3 else "medium" if importance>0.1 else "low"
     if feature=="age":
      risk_factors.append({
       "factor": "Age",
       "value": f"{value} years",
       "impact": impact
      })
     elif feature=="comorbidity_score":
      risk_factors.append({
       "factor": "Comorbidity Score",
       "value": str(value),
       "impact": impact
      })
     elif feature.startswith("has_") and value==1:
      condition=feature[4:].replace("_", " ").title()
      risk_factors.append({
       "factor": condition,
       "value": "Present",
       "impact": impact
      })
     elif feature=="stay_duration":
      risk_factors.append({
       "factor": "Previous Stay Duration",
       "value": f"{value} days",
       "impact": impact
      })
  else:
   if patient_features.get("age", 0)>=65:
    risk_factors.append({
     "factor": "Advanced Age",
     "value": f"{patient_features['age']} years",
     "impact": "high" if patient_features.get("age", 0)>=75 else "medium"
    })
   comorbidities=[]
   if patient_features.get("has_diabetes", 0)==1:
    comorbidities.append("Diabetes")
   if patient_features.get("has_hypertension", 0)==1:
    comorbidities.append("Hypertension")
   if patient_features.get("has_heart_condition", 0)==1:
    comorbidities.append("Heart Disease")
   if comorbidities:
    risk_factors.append({
     "factor": "Comorbidities",
     "value": ", ".join(comorbidities),
     "impact": "high" if len(comorbidities)>=2 else "medium"
    })
  return risk_factors
 def batch_predict(self, requests: List[ModelRequest])->List[ModelResponse]:
        """Make predictions for multiple requests using the MCP protocol."""
        start_time = time.time()

        # Group requests by model type for batch processing
        grouped_requests = {}
        for request in requests:
            model_id = request.model_id
            if model_id not in grouped_requests:
                grouped_requests[model_id] = []
            grouped_requests[model_id].append(request)

        # Process each group of requests
        responses = []
        for model_id, model_requests in grouped_requests.items():
            # Validate model exists
            if model_id not in self.model_registry:
                raise ValueError(f"Model {model_id} not found")

            model_metadata = self.model_registry[model_id]
            model_type = model_metadata.model_type

            # Route to appropriate batch prediction function based on model type
            if model_type == ModelType.READMISSION_RISK:
                batch_responses = self._batch_predict_readmission_risk(model_requests)
            elif model_type == ModelType.PATIENT_RISK:
                batch_responses = self._batch_predict_patient_risk(model_requests)
            else:
                # Fallback to individual processing if no batch function available
                batch_responses = [self.predict(request) for request in model_requests]

            responses.extend(batch_responses)

        # Log performance
        execution_time = time.time() - start_time
        logger.info(f"Batch prediction completed for {len(requests)} requests in {execution_time:.2f}s")

        return responses

    def _batch_predict_readmission_risk(self, requests: List[ModelRequest]) -> List[ModelResponse]:
        """Batch predict readmission risk for multiple patients."""
  patient_ids=[]
  request_map={}  # Map patient_id to request
  for request in requests:
   patient_id=request.context.patient_id
   if not patient_id:
    if "patient_id" in request.inputs:
     patient_id=request.inputs["patient_id"]
    else:
     raise ValueError("Patient ID not provided in context or inputs")
   patient_ids.append(patient_id)
   request_map[patient_id]=request
  features_list=[]
  valid_patient_ids=[]
  for patient_id in patient_ids:
   features=self.feature_store.get_features_for_entity(
    "readmission_prediction", patient_id
   )
   if features:
    features_list.append(features)
    valid_patient_ids.append(patient_id)
  if not valid_patient_ids:
   raise ValueError("No valid patients found for batch prediction")
  model_id=requests[0].model_id
  endpoint_name=self._get_endpoint_name(model_id)
  sagemaker_responses=self.sagemaker.batch_predict(
   endpoint_name, features_list, valid_patient_ids
  )
  responses=[]
  for patient_id, prediction_result in zip(valid_patient_ids, sagemaker_responses):
   request=request_map[patient_id]
   risk_score=prediction_result.get("score", 0.0)
   risk_level="Low"
   if risk_score>=0.7:
    risk_level="High"
   elif risk_score>=0.3:
    risk_level="Medium"
   explanation=None
   if request.context.include_explanations:
    feature_importance=prediction_result.get("feature_importance", {})
    if not feature_importance:
     features=features_list[valid_patient_ids.index(patient_id)]
     feature_importance=self._get_feature_importance(
      request.model_id,
      features,
      prediction_result
     )
    explanation=ModelExplanation(
     format=ExplanationFormat.FEATURE_IMPORTANCE,
     explanation="Key factors contributing to readmission risk",
     importance_scores=feature_importance
    )
   response=ModelResponse(
    model_id=request.model_id,
    version=self.model_registry[request.model_id].version,
    prediction={
     "risk_score": risk_score,
     "risk_percentage": round(risk_score*100, 1),
     "risk_level": risk_level,
     "patient_id": patient_id,
     "key_factors": self._get_key_risk_factors(
      features_list[valid_patient_ids.index(patient_id)],
      feature_importance if explanation else None
     )
    },
    confidence=prediction_result.get("confidence", 0.8),
    explanation=explanation,
    metadata={
     "inference_time_ms": prediction_result.get("inference_time_ms", 0),
     "model_type": ModelType.READMISSION_RISK.value,
     "batch_processed": True
    }
   )
   responses.append(response)
  return responses

=== hospital-data-chatbot-ml/app/ml/mcp/protocol.py (python) ===
from enum import Enum
from typing import Dict, List, Any, Optional
from pydantic import BaseModel, Field
class ModelType(str, Enum):
 READMISSION_RISK="readmission_risk"
 PATIENT_RISK="patient_risk"
 DIAGNOSIS_CLUSTER="diagnosis_cluster"
 LENGTH_OF_STAY="length_of_stay"
class ModelMetadata(BaseModel):
    """Model metadata as per MCP specification"""
    model_id: str
    version: str
    model_type: ModelType
    description: str
    created_at: str
    accuracy: float
    capabilities: List[str]
    input_schema: Dict[str, Any]
    output_schema: Dict[str, Any]
    feature_names: List[str]

class ModelContext(BaseModel):
    """Context information passed to the model"""
 patient_id: Optional[str]=None
 time_range: Optional[str]=None
 include_explanations: bool=True
 confidence_threshold: Optional[float]=None
 additional_context: Optional[Dict[str, Any]]=None
class ModelRequest(BaseModel):
    """Standard MCP request format"""
    model_id: str
    context: ModelContext
    inputs: Dict[str, Any]
    options: Optional[Dict[str, Any]] = None

class ExplanationFormat(str, Enum):
    FEATURE_IMPORTANCE = "feature_importance"
    LIME = "lime"
    SHAP = "shap"
    TEXT = "text"

class ModelExplanation(BaseModel):
    """Explanation of model prediction"""
 format: ExplanationFormat
 explanation: Any
 importance_scores: Optional[Dict[str, float]]=None
class ModelResponse(BaseModel):
    """Standard MCP response format"""
    model_id: str
    version: str
    prediction: Any
    confidence: float
    explanation: Optional[ModelExplanation] = None
    metadata: Optional[Dict[str, Any]] = None

=== hospital-data-chatbot-ml/app/ml/mcp/client.py (python) ===
import requests
from typing import Dict, List, Any, Optional
from app.ml.mcp.protocol import (
 ModelRequest, ModelResponse, ModelContext,
 ModelMetadata, ModelType
)
from app.utils.logging import get_logger
logger=get_logger(__name__)
class MCPClient:
    """Client for interacting with MCP-compliant ML API."""

    def __init__(self, base_url: str, api_key: Optional[str] = None):
        self.base_url = base_url
        self.api_key = api_key

    def get_models(self) -> List[ModelMetadata]:
        """Get list of available models."""
  url=f"{self.base_url}/models"
  headers=self._get_headers()
  response=requests.get(url, headers=headers)
  if response.status_code!=200:
   self._handle_error(response)
  return [ModelMetadata(**model) for model in response.json()]
 def get_model_metadata(self, model_id: str)->ModelMetadata:
        """Get metadata for a specific model."""
        url = f"{self.base_url}/models/{model_id}"
        headers = self._get_headers()

        response = requests.get(url, headers=headers)
        if response.status_code != 200:
            self._handle_error(response)

        return ModelMetadata(**response.json())

    def predict(self, request: ModelRequest) -> ModelResponse:
        """Make a prediction using the Model Context Protocol."""
  url=f"{self.base_url}/predict"
  headers=self._get_headers()
  response=requests.post(url, headers=headers, json=request.dict())
  if response.status_code!=200:
   self._handle_error(response)
  return ModelResponse(**response.json())
 def predict_readmission_risk(self, patient_id: str, include_explanations: bool=True)->ModelResponse:
        """Helper method to predict readmission risk for a patient."""
        request = ModelRequest(
            model_id="readmission-risk-xgboost-v1",
            context=ModelContext(
                patient_id=patient_id,
                include_explanations=include_explanations
            ),
            inputs={"patient_id": patient_id}
        )

        return self.predict(request)

    def predict_patient_risk(self, patient_id: str, include_explanations: bool = True) -> ModelResponse:
        """Helper method to predict risk stratification for a patient."""
  request=ModelRequest(
   model_id="patient-risk-xgboost-v1",
   context=ModelContext(
    patient_id=patient_id,
    include_explanations=include_explanations
   ),
   inputs={"patient_id": patient_id}
  )
  return self.predict(request)
 def _get_headers(self)->Dict[str, str]:
        """Get headers for API requests."""
        headers = {"Content-Type": "application/json"}
        if self.api_key:
            headers["Authorization"] = f"Bearer {self.api_key}"
        return headers

    def _handle_error(self, response):
        """Handle error responses from the API."""
  try:
   error_msg=response.json().get("detail", f"HTTP error {response.status_code}")
  except:
   error_msg=f"HTTP error {response.status_code}: {response.text}"
  logger.error(f"ML API error: {error_msg}")
  raise ValueError(f"ML API error: {error_msg}")

=== hospital-data-chatbot-ml/app/ml/mcp/api.py (python) ===
from fastapi import FastAPI, HTTPException, Depends
from app.ml.mcp.protocol import ModelRequest, ModelResponse, ModelMetadata
from app.ml.mcp.service import MCPModelService
from typing import List
app=FastAPI(
 title="Hospital Data ML API",
 description="ML API for Hospital Data Chatbot with Model Context Protocol",
 version="1.0.0"
)
mcp_service=MCPModelService()
@app.get("/models", response_model=List[ModelMetadata])
async def get_models():
    """Get a list of available models."""
    try:
        return mcp_service.get_models()
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/models/{model_id}", response_model=ModelMetadata)
async def get_model_metadata(model_id: str):
    """Get metadata for a specific model."""
 try:
  return mcp_service.get_model_metadata(model_id)
 except ValueError as e:
  raise HTTPException(status_code=404, detail=str(e))
 except Exception as e:
  raise HTTPException(status_code=500, detail=str(e))
@app.post("/predict", response_model=ModelResponse)
async def predict(request: ModelRequest):
    """Make a prediction using the Model Context Protocol."""
    try:
        return mcp_service.predict(request)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

=== hospital-data-chatbot-ml/app/api/ml_routes.py (python) ===
from fastapi import APIRouter, Depends, HTTPException, Request, Query
from pydantic import BaseModel
from typing import Dict, List, Any, Optional
from app.ml.mcp.client import MCPClient
from app.ml.mcp.protocol import ModelContext
from app.config.settings import AppConfig
from app.utils.logging import get_logger
router=APIRouter()
logger=get_logger(__name__)
ML_API_URL=AppConfig.ML_API_URL
ML_API_KEY=AppConfig.ML_API_KEY
@router.get("/patient-risk")
async def get_patient_risk(
 request: Request,
 patient_id: Optional[str]=Query(None, description="Patient registry ID. If not provided, returns risk for all patients")
):
    """
    Get risk stratification for patients.
    If patient_id is provided, returns risk for that specific patient.
    Otherwise, returns population-level risk statistics.
    """
 try:
  if not hasattr(request.app.state, "mcp_client"):
   logger.info("Initializing MCP client")
   request.app.state.mcp_client=MCPClient(ML_API_URL, ML_API_KEY)
  mcp_client=request.app.state.mcp_client
  if patient_id:
   response=mcp_client.predict_patient_risk(patient_id)
   return response.prediction
  else:
   if not hasattr(request.app.state, "ml_models"):
    logger.info("Initializing Hospital ML Models")
    request.app.state.ml_models=HospitalMLModels()
   ml_models=request.app.state.ml_models
   result=ml_models.get_patient_risk_stratification()
   if result.get('status')=='error':
    raise HTTPException(
     status_code=404,
     detail=result.get('message', 'Error getting population risk data')
    )
   return result
 except ValueError as e:
  logger.error(f"Error getting patient risk: {str(e)}")
  raise HTTPException(
   status_code=400,
   detail=str(e)
  )
 except Exception as e:
  logger.error(f"Error getting patient risk: {str(e)}", exc_info=True)
  raise HTTPException(
   status_code=500,
   detail=f"Failed to get patient risk: {str(e)}"
  )
@router.get("/readmission-risk/{patient_id}")
async def get_readmission_risk(request: Request, patient_id: str):
    """
    Get 30-day readmission risk prediction for a specific patient.
    """
 try:
  if not hasattr(request.app.state, "mcp_client"):
   logger.info("Initializing MCP client")
   request.app.state.mcp_client=MCPClient(ML_API_URL, ML_API_KEY)
  mcp_client=request.app.state.mcp_client
  response=mcp_client.predict_readmission_risk(patient_id)
  return response.prediction
 except ValueError as e:
  logger.error(f"Error getting readmission risk: {str(e)}")
  raise HTTPException(
   status_code=400,
   detail=str(e)
  )
 except Exception as e:
  logger.error(f"Error getting readmission risk: {str(e)}", exc_info=True)
  raise HTTPException(
   status_code=500,
   detail=f"Failed to get readmission risk: {str(e)}"
  )
@router.post("/analyze-patient")
async def analyze_patient(request: Request, patient_id: str):
    """
    Analyze patient data using ML models through MCP.
    Returns comprehensive analysis including risk factors and predictions.
    """
 try:
  if not hasattr(request.app.state, "mcp_client"):
   logger.info("Initializing MCP client")
   request.app.state.mcp_client=MCPClient(AppConfig.ML_API_URL, AppConfig.ML_API_KEY)
  mcp_client=request.app.state.mcp_client
  from app.utils.db import get_db_connection
  import polars as pl
  conn=get_db_connection()
        query = f"""
        SELECT * FROM patient_details
        WHERE registry_id = '{patient_id}'
        """
  patient_df=pl.read_database(query=query, connection=conn)
  if patient_df.height==0:
   raise HTTPException(
    status_code=404,
    detail=f"Patient {patient_id} not found"
   )
        query = f"""
        SELECT * FROM diagnosis_details
        WHERE registry_id = '{patient_id}'
        """
  diagnosis_df=pl.read_database(query=query, connection=conn)
  readmission_response=mcp_client.predict_readmission_risk(patient_id)
  risk_response=mcp_client.predict_patient_risk(patient_id)
  result={
   "patient": patient_df.to_dicts()[0],
   "diagnoses": diagnosis_df.to_dicts(),
   "readmission_risk": readmission_response.prediction,
   "risk_stratification": risk_response.prediction,
   "analysis_timestamp": datetime.now().isoformat()
  }
  return result
 except HTTPException:
  raise
 except Exception as e:
  logger.error(f"Error analyzing patient: {str(e)}", exc_info=True)
  raise HTTPException(
   status_code=500,
   detail=f"Failed to analyze patient: {str(e)}"
  )

=== hospital-data-chatbot-ml/app/notebooks/model_evaluation.ipynb (text) ===


=== hospital-data-chatbot-ml/app/notebooks/data_exploration.ipynb (text) ===


=== hospital-data-chatbot-ml/scripts/train_deploy_model.py (python) ===
import argparse
import json
import boto3
import time
from datetime import datetime
import sagemaker
from sagemaker.xgboost.estimator import XGBoost
def parse_args():
 parser=argparse.ArgumentParser(description='Train and deploy a SageMaker model')
 parser.add_argument('--model-type', type=str, required=True,
      choices=['readmission_risk', 'patient_risk', 'diagnosis_cluster'],
      help='Type of model to train')
 parser.add_argument('--environment', type=str, required=True,
      choices=['dev-cloud', 'stage', 'prod'],
      help='Environment to deploy to')
 return parser.parse_args()
def get_feature_data(model_type, environment):
    """Get feature data for training."""
    # Get S3 bucket name based on environment
    project_name = 'hospital-data-chatbot'
    bucket_name = f"{project_name}-{environment}-data"

    # Map model type to feature set name
    feature_sets = {
        'readmission_risk': 'readmission_prediction',
        'patient_risk': 'patient_risk_factors',
        'diagnosis_cluster': 'diagnosis_clustering'
    }

    feature_set = feature_sets.get(model_type)
    if not feature_set:
        raise ValueError(f"Unknown model type: {model_type}")

    # Check if feature data exists
    s3 = boto3.client('s3')
    prefix = f"features/{feature_set}"

    try:
        s3.head_object(Bucket=bucket_name, Key=f"{prefix}/features.csv")
        return bucket_name, prefix
    except Exception:
        # If features don't exist, use a Python script to generate them
        print(f"Features for {feature_set} not found in S3. Generating...")
        # This would call your feature engineering code
        return bucket_name, prefix

def train_model(model_type, environment):
    """Train a model using SageMaker."""
 bucket_name, prefix=get_feature_data(model_type, environment)
 sagemaker_session=sagemaker.Session()
 role=sagemaker.get_execution_role()
 hyperparameters={
  'readmission_risk': {
   'objective': 'binary:logistic',
   'num_round': 100,
   'max_depth': 6,
   'eta': 0.3,
   'eval_metric': 'auc'
  },
  'patient_risk': {
   'objective': 'multi:softmax',
   'num_class': 4,  # low, moderate, high, very_high
   'num_round': 100,
   'max_depth': 6,
   'eta': 0.3,
   'eval_metric': 'mlogloss'
  },
  'diagnosis_cluster': {
   'objective': 'reg:squarederror',
   'num_round': 100,
   'max_depth': 6,
   'eta': 0.3,
   'eval_metric': 'rmse'
  }
 }.get(model_type)
 xgb=XGBoost(
  entry_point='xgboost_script.py',
  framework_version='1.5-1',
  hyperparameters=hyperparameters,
  role=role,
  instance_count=1,
  instance_type='ml.m5.large',
  output_path=f's3://{bucket_name}/{prefix}/output'
 )
 print(f"Starting training job for {model_type} model...")
 xgb.fit({
  'train': f's3://{bucket_name}/{prefix}/train',
  'validation': f's3://{bucket_name}/{prefix}/validation'
 })
 print(f"Deploying model to endpoint...")
 predictor=xgb.deploy(
  initial_instance_count=1,
  instance_type='ml.t2.medium',
  endpoint_name=f"hospital-data-chatbot-{environment}-{model_type.replace('_', '-')}"
 )
 with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
  f.write(f"training_job={xgb.latest_training_job.name}\n")
  f.write(f"endpoint={predictor.endpoint_name}\n")
  f.write(f"metrics=AUC: 0.85, Accuracy: 0.82\n")  # These would be actual metrics from the model
 print(f"Model training and deployment completed successfully!")
 return xgb, predictor
def main():
 args=parse_args()
 train_model(args.model_type, args.environment)
if __name__=="__main__":
 main()

=== hospital-data-chatbot-ml/scripts/test_ml_api.py (python) ===
import requests
import json
import argparse
import time
def parse_args():
 parser=argparse.ArgumentParser(description='Test ML API integration')
 parser.add_argument('--api-url', type=str, required=True,
      help='ML API URL')
 parser.add_argument('--api-key', type=str, required=True,
      help='ML API key')
 parser.add_argument('--patient-id', type=str, default='P12345',
      help='Patient ID to test')
 return parser.parse_args()
def test_models_endpoint(api_url, api_key):
    """Test the /models endpoint."""
    print("\nTesting /models endpoint...")
    url = f"{api_url}/models"
    headers = {
        "Content-Type": "application/json",
        "x-api-key": api_key
    }

    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        models = response.json()
        print(f"Success! Found {len(models)} models:")
        for model in models:
            print(f"  - {model['model_id']} (Type: {model['model_type']}, Accuracy: {model['accuracy']})")
        return True
    else:
        print(f"Error: {response.status_code} - {response.text}")
        return False

def test_model_metadata(api_url, api_key, model_id="readmission-risk-xgboost-v1"):
    """Test the /models/{model_id} endpoint."""
 print(f"\nTesting/models/{model_id} endpoint...")
 url=f"{api_url}/models/{model_id}"
 headers={
  "Content-Type": "application/json",
  "x-api-key": api_key
 }
 response=requests.get(url, headers=headers)
 if response.status_code==200:
  metadata=response.json()
  print(f"Success! Model details:")
  print(f"-ID: {metadata['model_id']}")
  print(f"-Version: {metadata['version']}")
  print(f"-Type: {metadata['model_type']}")
  print(f"-Description: {metadata['description']}")
  print(f"-Accuracy: {metadata['accuracy']}")
  print(f"-Features: {', '.join(metadata['feature_names'][:5])}...")
  return True
 else:
  print(f"Error: {response.status_code}-{response.text}")
  return False
def test_prediction(api_url, api_key, patient_id, model_id="readmission-risk-xgboost-v1"):
    """Test the /predict endpoint."""
    print(f"\nTesting /predict endpoint with patient {patient_id}...")
    url = f"{api_url}/predict"
    headers = {
        "Content-Type": "application/json",
        "x-api-key": api_key
    }

    payload = {
        "model_id": model_id,
        "context": {
            "patient_id": patient_id,
            "include_explanations": True
        },
        "inputs": {
            "patient_id": patient_id
        }
    }

    response = requests.post(url, headers=headers, json=payload)
    if response.status_code == 200:
        result = response.json()
        print(f"Success! Prediction results:")
        print(f"  - Risk Score: {result['prediction']['risk_percentage']}%")
        print(f"  - Risk Level: {result['prediction']['risk_level']}")
        print(f"  - Confidence: {result['confidence']}")

        if result.get('explanation'):
            print(f"  - Explanation Format: {result['explanation']['format']}")
            if result['explanation'].get('importance_scores'):
                scores = result['explanation']['importance_scores']
                print("  - Feature Importance:")
                for feature, score in list(scores.items())[:5]:
                    print(f"    - {feature}: {score:.4f}")

        return True
    else:
        print(f"Error: {response.status_code} - {response.text}")
        return False

def main():
    args = parse_args()
    api_url = args.api_url
    api_key = args.api_key
    patient_id = args.patient_id

    print(f"Testing ML API at {api_url}")

    # Test each endpoint
    models_success = test_models_endpoint(api_url, api_key)
    metadata_success = test_model_metadata(api_url, api_key)
    prediction_success = test_prediction(api_url, api_key, patient_id)

    # Print summary
    print("\nTest Summary:")
    print(f"  - Models Endpoint: {' PASS' if models_success else ' FAIL'}")
    print(f"  - Model Metadata Endpoint: {' PASS' if metadata_success else ' FAIL'}")
    print(f"  - Prediction Endpoint: {' PASS' if prediction_success else ' FAIL'}")

    # Overall status
    if models_success and metadata_success and prediction_success:
        print("\n All tests passed! ML API is working correctly.")
        return 0
    else:
        print("\n Some tests failed. Please check the ML API.")
        return 1

if __name__ == "__main__":
    main()

=== hospital-data-chatbot-infrastructure/ReadMe.md (markdown) ===
![Terraform Logo](https://www.terraform.io/assets/images/logo-hashicorp-3f10732f.svg)
This repository contains a comprehensive AWS infrastructure management system powered by Terraform. It provides a streamlined approach to deploying and managing multi-environment AWS infrastructure (development, staging, and production) using infrastructure as code.
- **Multi-environment Support**: Maintain separate configurations for development, staging, and production
- **Infrastructure as Code**: Define all your infrastructure components in version-controlled code
- **Automated Deployment**: Deploy your entire infrastructure stack with a single command
- **Environment Isolation**: Keep resources separated by environment to prevent cross-contamination
- **Consistent Configuration**: Ensure infrastructure consistency across all environments
- **Complete AWS Stack**: Includes VPC, subnets, security groups, EC2 instances, S3 buckets, and monitoring
- **Scalable Architecture**: Easily extend with additional AWS resources as needed
- **AI/ML Integration**: Built-in support for AWS Bedrock and SageMaker
```
terraform/
 main.tf                # Primary infrastructure definition
 variables.tf           # Variable declarations
 outputs.tf             # Output definitions
 versions.tf            # Terraform version constraints
 environments/
    dev.tfvars         # Development environment configuration
    staging.tfvars     # Staging environment configuration
    prod.tfvars        # Production environment configuration
    backend-config/
        dev.hcl        # Remote state config for development
        staging.hcl    # Remote state config for staging
        prod.hcl       # Remote state config for production
 modules/               # Reusable Terraform modules
    networking/        # VPC, subnets, security groups
    database/          # RDS PostgreSQL configuration
    storage/           # S3 buckets, ECR repositories
    app_deployment/    # ECS Fargate, ALB, auto-scaling
    monitoring/        # CloudWatch, SNS
    bedrock/           # AI model integration
    sagemaker/         # ML infrastructure
 bootstrap.sh           # Script to initialize backend infrastructure
 terraform-infra-manager.sh  # Management script
```
- **AWS CLI**: Configured with appropriate credentials
- **Terraform**: Version 1.0.0 or later
- **Bash**: For running the management script
- **AWS Account**: With permissions to create all required resources
- **AWS IAM User**: With programmatic access and appropriate permissions
1. **Clone this repository**
```bash
git clone https://github.com/your-org/aws-terraform-infra.git
cd aws-terraform-infra
```
2. **Make the management scripts executable**
```bash
chmod +x terraform-infra-manager.sh bootstrap.sh
```
3. **Set up AWS credentials**
Choose one of the following methods to configure AWS credentials:
```bash
aws configure
```
Enter your AWS Access Key, Secret Key, default region, and output format when prompted.
```bash
export AWS_ACCESS_KEY_ID="your-access-key"
export AWS_SECRET_ACCESS_KEY="your-secret-key"
export AWS_DEFAULT_REGION="ap-south-1"
```
Create a `.awsconfig` file in your project root:
```bash
AWS_ACCESS_KEY_ID=your-access-key-here
AWS_SECRET_ACCESS_KEY=your-secret-key-here
AWS_DEFAULT_REGION=ap-south-1
```
Add to your `.gitignore`:
```
.awsconfig
```
4. **Bootstrap your Terraform state infrastructure**
```bash
./bootstrap.sh --environment dev-cloud --region ap-south-1 --profile your-aws-profile
```
This sets up the required S3 bucket and DynamoDB table for Terraform state management.
5. **Update configuration files**
Modify the `.tfvars` files in the `environments` directory to match your requirements:
- Update AWS region and account ID
- Configure networking (CIDR blocks, subnets)
- Set appropriate security group rules
- Adjust instance types and counts for each environment
6. **Set up sensitive variables**
```bash
vim terraform-secrets.sh
chmod +x terraform-secrets.sh
source ./terraform-secrets.sh
```
7. **Initialize Terraform**
```bash
./terraform-infra-manager.sh init
```
8. **Deploy to your desired environment**
```bash
./terraform-infra-manager.sh -e dev-cloud apply
```
The `terraform-infra-manager.sh` script is a powerful utility for managing your Terraform infrastructure. It provides a streamlined workflow for initializing, planning, applying, and destroying infrastructure across different environments.
- **Environment Management**: Easily switch between development, staging, and production environments
- **AWS Profile Handling**: Use different AWS credentials for different environments
- **S3 Backend Verification**: Automatically checks for and offers to create required S3 buckets
- **DynamoDB State Locking**: Ensures safe concurrent access to Terraform state
- **Performance Timing**: Tracks and reports the duration of Terraform operations
- **Verbose Mode**: Provides detailed output for troubleshooting
- **Backend Configuration**: Automatically manages backend configurations for different environments
- **Auto-approve Option**: Supports non-interactive deployment for CI/CD pipelines
- **Full Command Support**: Supports all Terraform commands including initialization, plan, apply, destroy, and more
```bash
Usage: ./terraform-infra-manager.sh [OPTIONS] COMMAND
A utility script to manage Terraform infrastructure
Commands:
init        Initialize Terraform working directory
plan        Generate and show an execution plan
apply       Build or change infrastructure
destroy     Destroy previously-created infrastructure
output      Show output values from your root module
validate    Check whether the configuration is valid
workspace   Switch between workspaces
fmt         Reformat your configuration in the standard style
all         Run init, validate, plan, and apply in sequence
Options:
-d, --directory DIR   Terraform directory (default: ./terraform)
-e, --environment ENV Environment to deploy (dev-cloud, staging, prod)
-a, --auto-approve    Skip interactive approval for apply/destroy
-v, --verbose         Show detailed output
-h, --help            Display this help message
```
The script automatically checks if the required S3 bucket and DynamoDB table for Terraform state management exist:
```bash
BUCKET_NAME="${PROJECT_NAME}-terraform-state-${BUCKET_ENV}-${AWS_ACCOUNT_ID}"
if check_s3_bucket_exists "$BUCKET_NAME" "$AWS_REGION"; then
echo " S3 bucket '$BUCKET_NAME' exists and is accessible"
else
echo " S3 bucket '$BUCKET_NAME' does not exist or is not accessible"
read -p "Do you want to create the S3 bucket? (y/n): " CREATE_BUCKET
fi
DYNAMO_TABLE="${PROJECT_NAME}-terraform-locks-${BUCKET_ENV}"
if aws dynamodb describe-table --table-name "$DYNAMO_TABLE" --region "$AWS_REGION" &>/dev/null; then
echo " DynamoDB table '$DYNAMO_TABLE' exists and is accessible"
else
echo " DynamoDB table '$DYNAMO_TABLE' does not exist or is not accessible"
fi
```
The script automatically generates the correct backend configuration for each environment:
```bash
if [ "$DYNAMO_TABLE_EXISTS" = true ]; then
cat > "$BACKEND_DIR/${ENV}.hcl" << EOF
bucket         = "${BUCKET_NAME}"
key            = "${ENV}/terraform.tfstate"
region         = "${AWS_REGION}"
dynamodb_table = "${DYNAMO_TABLE}"
encrypt        = true
EOF
else
fi
```
The script includes built-in timing functionality to track the duration of Terraform operations:
```bash
function timer() {
if [[ $# -eq 0 ]]; then
echo $(date '+%s')
else
local start_time=$1
local end_time=$(date '+%s')
local elapsed=$((end_time - start_time))
local mins=$((elapsed / 60))
local secs=$((elapsed % 60))
echo "Time elapsed: ${mins}m ${secs}s"
fi
}
start_time=$(timer)
terraform $cmd $args
timer $start_time
```
The script supports a complete deployment pipeline with a single command:
```bash
./terraform-infra-manager.sh -e dev-cloud all
```
This runs `init`, `validate`, `plan`, and `apply` in sequence, providing a complete deployment workflow.
1. **Development First**: Deploy changes to the development environment
```bash
./terraform-infra-manager.sh -e dev-cloud apply
```
2. **Validate in Staging**: Once tested in development, promote to staging
```bash
./terraform-infra-manager.sh -e staging apply
```
3. **Production Deployment**: After thorough testing, deploy to production
```bash
./terraform-infra-manager.sh -e prod apply
```
This infrastructure uses remote state management to enable team collaboration:
```bash
./terraform-infra-manager.sh -e dev-cloud init
./terraform-infra-manager.sh -e staging init
./terraform-infra-manager.sh -e prod init
```
To generate a visual representation of your infrastructure:
```bash
terraform graph | dot -Tpng > infrastructure.png
```
1. Add resource definitions to `main.tf`
2. Declare any new variables in `variables.tf`
3. Update environment configuration in `.tfvars` files
4. Add outputs if needed in `outputs.tf`
1. Create a new `.tfvars` file in the `environments` directory
2. Create a new backend configuration file if using remote state
3. Deploy using the new environment name:
```bash
./terraform-infra-manager.sh -e new-environment apply
```
- **Version Control**: Always commit changes to your Terraform files
- **Code Review**: Use pull requests to review infrastructure changes
- **Testing**: Test changes in lower environments before promoting
- **Linting**: Use `terraform fmt` to maintain consistent formatting
- **Documentation**: Update comments and README as infrastructure evolves
- **State Backup**: Regularly backup your Terraform state
- **Secret Management**: Avoid storing secrets in Terraform files
The infrastructure manager script supports several methods for handling AWS credentials. Choose the approach that best fits your security requirements and workflow.
Use AWS profiles for the most secure approach:
```bash
aws configure --profile terraform-admin
./terraform-infra-manager.sh --profile terraform-admin -e dev-cloud apply
```
Set credentials for the current session:
```bash
export AWS_ACCESS_KEY_ID="your-access-key"
export AWS_SECRET_ACCESS_KEY="your-secret-key"
export AWS_DEFAULT_REGION="ap-south-1"
./terraform-infra-manager.sh -e dev-cloud apply
```
Create a `.awsconfig` file with your credentials:
```
AWS_ACCESS_KEY_ID=your-access-key-here
AWS_SECRET_ACCESS_KEY=your-secret-key-here
AWS_DEFAULT_REGION=ap-south-1
```
The script will automatically load this file if present.
If running on EC2, use IAM roles for best security:
```bash
./terraform-infra-manager.sh -e prod apply
```
- **Never commit credentials** to version control
- **Rotate keys** regularly
- **Use temporary credentials** when possible
- **Apply least privilege principle** to IAM permissions
- **Enable MFA** for AWS accounts with infrastructure access
- **Audit credential usage** regularly
- **State Lock**: If a state lock persists, check for running operations or use:
```bash
terraform force-unlock LOCK_ID
```
- **Provider Authentication**: Ensure AWS credentials are properly configured:
```bash
aws configure list
```
- **Resource Limits**: Check for AWS service quotas if deployments fail
- **Credential Issues**: Verify your credentials are working:
```bash
aws sts get-caller-identity
```
- **Permission Issues**: Ensure your AWS identity has the necessary permissions:
```bash
Error: creating ECS Cluster: AccessDeniedException: User is not authorized to perform: ecs:CreateCluster
```
Solution: Add the required permissions to your IAM user/role or use a role with appropriate permissions.
Enable verbose logging for more detailed output:
```bash
./terraform-infra-manager.sh -v -e dev-cloud plan
```
To run the infrastructure deployment, your AWS identity needs the following permissions:
```json
{
"Version": "2012-10-17",
"Statement": [
{
"Effect": "Allow",
"Action": [
"ec2:*",
"rds:*",
"s3:*",
"ecr:*",
"ecs:*",
"elasticloadbalancing:*",
"application-autoscaling:*",
"cloudwatch:*",
"logs:*",
"sns:*",
"secretsmanager:*",
"sagemaker:*",
"bedrock:*",
"dynamodb:*",
"route53:*"
],
"Resource": "*"
},
{
"Effect": "Allow",
"Action": [
"iam:GetRole",
"iam:CreateRole",
"iam:DeleteRole",
"iam:PutRolePolicy",
"iam:AttachRolePolicy",
"iam:DetachRolePolicy",
"iam:DeleteRolePolicy",
"iam:CreatePolicy",
"iam:DeletePolicy",
"iam:TagRole",
"iam:ListRoleTags"
],
"Resource": "*"
},
{
"Effect": "Allow",
"Action": "iam:PassRole",
"Resource": [
"arn:aws:iam::ACCOUNT_ID:role/PROJECT_NAME-*-ecs-execution-role",
"arn:aws:iam::ACCOUNT_ID:role/PROJECT_NAME-*-ecs-task-role",
"arn:aws:iam::ACCOUNT_ID:role/PROJECT_NAME-*-bedrock-role",
"arn:aws:iam::ACCOUNT_ID:role/PROJECT_NAME-*-sagemaker-role"
],
"Condition": {
"StringEquals": {
"iam:PassedToService": [
"ecs.amazonaws.com",
"ecs-tasks.amazonaws.com",
"bedrock.amazonaws.com",
"sagemaker.amazonaws.com",
"lambda.amazonaws.com",
"monitoring.rds.amazonaws.com"
]
}
}
}
]
}
```
Replace `ACCOUNT_ID` with your AWS account ID and `PROJECT_NAME` with your project name.
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run `terraform fmt` and `terraform validate`
5. Submit a pull request
This project is licensed under the MIT License - see the LICENSE file for details.
- HashiCorp for creating Terraform
- AWS for their comprehensive cloud infrastructure
- The community for sharing best practices and modules
Built with  by Your Team

=== hospital-data-chatbot-infrastructure/diagram.mermaid (text) ===
graph TD
subgraph "AWS Cloud"
subgraph "VPC"
subgraph "Public Subnets"
ALB["Application Load Balancer"]
IGW["Internet Gateway"]
NAT["NAT Gateway"]
end
subgraph "Private Subnets"
subgraph "ECS Fargate"
APP["Container App Service"]
end
subgraph "Database"
RDS["PostgreSQL RDS"]
end
subgraph "Machine Learning"
SN["SageMaker Notebook\n(dev-cloud only)"]
SM["SageMaker Models"]
end
end
end
subgraph "Storage"
S3["S3 Bucket\n(Application Data)"]
ECR["ECR Repository\n(Container Images)"]
end
subgraph "Parameter Store"
SSM["SSM Parameters\n/project/env/db-password"]
end
subgraph "AI Services"
BEDROCK["AWS Bedrock\n(Claude 3 Sonnet)"]
end
subgraph "Monitoring"
CW["CloudWatch\nLogs & Alarms"]
DASH["CloudWatch\nDashboard"]
SNS["SNS Topics\n(Notifications)"]
end
subgraph "Terraform State"
S3_TF["S3 Bucket\n(Terraform State)"]
DDB["DynamoDB\n(State Locking)"]
end
end
USER["User"] -->|HTTPS| ALB
ALB -->|HTTP| APP
APP -->|Query| RDS
APP -->|Store/Retrieve Data| S3
APP -->|AI Inference| BEDROCK
APP -->|Read Parameters| SSM
APP -->|Training Data| SM
SN -->|ML Development| S3
SN -->|Deploy Models| SM
APP -->|Logs| CW
RDS -->|Metrics| CW
CW -->|Alerts| SNS
CW -->|Visualize| DASH
SNS -->|Email| ADMIN["Admin"]
CI["CI/CD Pipeline"] -->|Push Images| ECR
ECR -->|Pull Images| APP
TERRAFORM["Terraform"] -->|State Management| S3_TF
TERRAFORM -->|State Locking| DDB
IGW -->|Internet Access| PUBLIC["Internet"]
PUBLIC -->|Incoming Traffic| IGW
NAT -->|Outbound Only| PUBLIC
PRIVATE["Private Resources"] -->|Outbound Traffic| NAT

=== hospital-data-chatbot-infrastructure/deploy/.webui_secret_key (text) ===
LQCbbqNJCnVGH5VU

=== hospital-data-chatbot-infrastructure/deploy/terraform-infra-manager.sh (bash) ===
set -e
TF_DIR="./terraform"
ENV="dev-cloud"
COMMAND=""
APPLY_ARGS=""
DESTROY_ARGS=""
VERBOSE=false
DEFAULT_AWS_PROFILE="nash-cli-1"  # <-- Set your existing profile name here
AWS_PROFILE=${AWS_PROFILE:-$DEFAULT_AWS_PROFILE}
function show_usage() {
echo "Usage: $0 [OPTIONS] COMMAND"
echo ""
echo "A utility script to manage Terraform infrastructure with secure parameter handling"
echo ""
echo "Commands:"
echo "  init        Initialize Terraform working directory"
echo "  plan        Generate and show an execution plan"
echo "  apply       Build or change infrastructure"
echo "  destroy     Destroy previously-created infrastructure"
echo "  output      Show output values from your root module"
echo "  validate    Check whether the configuration is valid"
echo "  workspace   Switch between workspaces"
echo "  fmt         Reformat your configuration in the standard style"
echo "  all         Run init, validate, plan, and apply in sequence"
echo "  params      Manage SSM parameters (create/update database password)"
echo ""
echo "Options:"
echo "  -d, --directory DIR   Terraform directory (default: ./terraform)"
echo "  -e, --environment ENV Environment to deploy (dev-cloud, staging, prod)"
echo "  -a, --auto-approve    Skip interactive approval for apply/destroy"
echo "  -v, --verbose         Show detailed output"
echo "  -h, --help            Display this help message"
echo "  --profile PROFILE     AWS profile to use (default: $DEFAULT_AWS_PROFILE)"
echo ""
echo "Examples:"
echo "  $0 init                         # Initialize Terraform"
echo "  $0 -e prod apply                # Deploy to production"
echo "  $0 -a destroy                   # Destroy infra without confirmation"
echo "  $0 -e staging -a all            # Full deployment to staging"
echo "  $0 -e dev-cloud params          # Manage database password parameter"
echo ""
}
function setup_aws_profile() {
export AWS_PROFILE="$AWS_PROFILE"
echo "Using AWS profile: $AWS_PROFILE"
if aws sts get-caller-identity &>/dev/null; then
echo " AWS profile verified successfully"
aws sts get-caller-identity --query "Account" --output text
else
echo " AWS profile verification failed"
echo "Please check that your AWS profile '$AWS_PROFILE' is configured correctly"
exit 1
fi
}
function check_s3_bucket_exists() {
local bucket_name="$1"
local region="$2"
if aws s3api head-bucket --bucket "$bucket_name" --region "$region" 2>/dev/null; then
return 0  # Bucket exists and we have access
else
return 1  # Bucket doesn't exist or we don't have access
fi
}
function check_ssm_parameter_exists() {
local param_name="/$PROJECT_NAME/$BUCKET_ENV/db-password"
if aws ssm get-parameter --name "$param_name" --region "$AWS_REGION" &>/dev/null; then
echo "DEBUG: Parameter $param_name exists"  # Add debugging
return 0  # Parameter exists
else
echo "DEBUG: Parameter $param_name does not exist"  # Add debugging
return 1  # Parameter doesn't exist
fi
}
function manage_db_password_parameter() {
local param_name="/$PROJECT_NAME/$BUCKET_ENV/db-password"
echo "Managing SSM Parameter: $param_name"
if check_ssm_parameter_exists; then
echo "Parameter already exists. Do you want to update it?"
select yn in "Yes" "No"; do
case $yn in
Yes ) break;;
No ) return 0;;
esac
done
fi
echo "Enter a secure password for the database (will not be shown on screen):"
read -s DB_PASSWORD
echo ""
echo "Confirm the password:"
read -s DB_PASSWORD_CONFIRM
echo ""
if [[ "$DB_PASSWORD" != "$DB_PASSWORD_CONFIRM" ]]; then
echo " Passwords do not match. Please try again."
return 1
fi
if aws ssm put-parameter --name "$param_name" \
echo " Parameter $param_name created/updated successfully"
else
echo " Failed to create/update parameter $param_name"
return 1
fi
return 0
}
function ensure_db_password_parameter() {
local param_name="/$PROJECT_NAME/$BUCKET_ENV/db-password"
echo "Checking for database password parameter: $param_name"
if aws ssm get-parameter --name "$param_name" --region "$AWS_REGION" &>/dev/null; then
echo " Database password parameter found in SSM Parameter Store"
return 0
else
echo " Database password parameter not found in SSM Parameter Store"
echo "You need to create this parameter before running Terraform operations"
read -p "Do you want to create the parameter now? (y/n): " CREATE_PARAM
if [[ "$CREATE_PARAM" =~ ^[Yy]$ ]]; then
manage_db_password_parameter
if [ $? -ne 0 ]; then
echo " Parameter creation failed. Cannot proceed."
exit 1
fi
else
echo " Cannot proceed without database password parameter"
echo "You can create it later using: $0 -e $ENV params"
exit 1
fi
fi
}
while [[ $# -gt 0 ]]; do
case "$1" in
-d|--directory)
TF_DIR="$2"
shift 2
;;
-e|--environment)
ENV="$2"
shift 2
;;
-a|--auto-approve)
APPLY_ARGS="-auto-approve"
DESTROY_ARGS="-auto-approve"
shift
;;
-v|--verbose)
VERBOSE=true
shift
;;
AWS_PROFILE="$2"
shift 2
;;
-h|--help)
show_usage
exit 0
;;
init|plan|apply|destroy|output|validate|workspace|fmt|all|params)
COMMAND="$1"
shift
;;
*)
echo "Unknown option: $1"
show_usage
exit 1
;;
esac
done
if [ -z "$COMMAND" ]; then
echo "Error: No command specified."
show_usage
exit 1
fi
if [ ! -d "$TF_DIR" ] && [ "$COMMAND" != "params" ]; then
echo "Error: Terraform directory '$TF_DIR' does not exist."
echo "Create it or specify a different directory with -d option."
exit 1
fi
setup_aws_profile
AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query "Account" --output text)
echo "AWS Account ID: $AWS_ACCOUNT_ID"
AWS_REGION=$(aws configure get region --profile "$AWS_PROFILE" || echo "ap-south-1")
echo "AWS Region: $AWS_REGION"
BUCKET_ENV=$(echo ${ENV} | tr '_' '-' | tr '[:upper:]' '[:lower:]')
PROJECT_NAME=$(basename $(pwd) | tr '_' '-' | tr '[:upper:]' '[:lower:]')
if [ -z "$PROJECT_NAME" ] || [ "$PROJECT_NAME" == "." ]; then
PROJECT_NAME="hospital-data-chatbot"
fi
if [ "$COMMAND" == "params" ]; then
echo " Managing SSM Parameters for $PROJECT_NAME in $ENV environment"
echo "======================================================"
manage_db_password_parameter
echo "======================================================"
echo " Parameter management completed for environment: $ENV"
exit 0
fi
BUCKET_NAME="${PROJECT_NAME}-terraform-state-${BUCKET_ENV}-${AWS_ACCOUNT_ID}"
echo "S3 bucket name: $BUCKET_NAME"
if check_s3_bucket_exists "$BUCKET_NAME" "$AWS_REGION"; then
echo " S3 bucket '$BUCKET_NAME' exists and is accessible"
S3_BUCKET_EXISTS=true
else
echo " S3 bucket '$BUCKET_NAME' does not exist or is not accessible"
read -p "Do you want to create the S3 bucket? (y/n): " CREATE_BUCKET
if [[ "$CREATE_BUCKET" =~ ^[Yy]$ ]]; then
echo "Creating S3 bucket: $BUCKET_NAME"
aws s3 mb s3://$BUCKET_NAME --region $AWS_REGION
echo "Enabling versioning on S3 bucket..."
aws s3api put-bucket-versioning \
S3_BUCKET_EXISTS=true
else
echo " Cannot proceed without S3 bucket for Terraform state"
exit 1
fi
fi
DYNAMO_TABLE="${PROJECT_NAME}-terraform-locks-${BUCKET_ENV}"
if aws dynamodb describe-table --table-name "$DYNAMO_TABLE" --region "$AWS_REGION" &>/dev/null; then
echo " DynamoDB table '$DYNAMO_TABLE' exists and is accessible"
DYNAMO_TABLE_EXISTS=true
else
echo " DynamoDB table '$DYNAMO_TABLE' does not exist or is not accessible"
read -p "Do you want to create the DynamoDB table for state locking? (y/n): " CREATE_TABLE
if [[ "$CREATE_TABLE" =~ ^[Yy]$ ]]; then
echo "Creating DynamoDB table: $DYNAMO_TABLE"
aws dynamodb create-table \
DYNAMO_TABLE_EXISTS=true
else
echo " Warning: Proceeding without DynamoDB table for state locking"
echo "Multiple users may modify infrastructure simultaneously, which could cause conflicts"
fi
fi
BACKEND_DIR="$TF_DIR/environments/backend-config"
mkdir -p "$BACKEND_DIR"
if [ "$S3_BUCKET_EXISTS" = true ]; then
if [ "$DYNAMO_TABLE_EXISTS" = true ]; then
cat > "$BACKEND_DIR/${ENV}.hcl" << EOF
bucket         = "${BUCKET_NAME}"
key            = "${ENV}/terraform.tfstate"
region         = "${AWS_REGION}"
dynamodb_table = "${DYNAMO_TABLE}"
encrypt        = true
EOF
else
cat > "$BACKEND_DIR/${ENV}.hcl" << EOF
bucket         = "${BUCKET_NAME}"
key            = "${ENV}/terraform.tfstate"
region         = "${AWS_REGION}"
encrypt        = true
EOF
fi
echo " Created/updated backend config: $BACKEND_DIR/${ENV}.hcl"
fi
if [[ "$COMMAND" == "apply" || "$COMMAND" == "all" ]]; then
ensure_db_password_parameter
fi
cd "$TF_DIR"
if [ "$VERBOSE" = true ]; then
export TF_LOG="DEBUG"
else
export TF_LOG="ERROR"
fi
TF_VAR_FILE=""
if [ -f "environments/${ENV}.tfvars" ]; then
TF_VAR_FILE="-var-file=environments/${ENV}.tfvars"
echo "Using environment config: environments/${ENV}.tfvars"
fi
BACKEND_CONFIG=""
if [ -f "environments/backend-config/${ENV}.hcl" ]; then
BACKEND_CONFIG="-backend-config=environments/backend-config/${ENV}.hcl"
echo "Using backend config: environments/backend-config/${ENV}.hcl"
fi
function timer() {
if [[ $# -eq 0 ]]; then
echo $(date '+%s')
else
local start_time=$1
local end_time=$(date '+%s')
local elapsed=$((end_time - start_time))
local mins=$((elapsed / 60))
local secs=$((elapsed % 60))
echo "Time elapsed: ${mins}m ${secs}s"
fi
}
function run_terraform() {
local cmd="$1"
local args="$2"
echo " Running: terraform $cmd $args"
start_time=$(timer)
if ! terraform $cmd $args; then
echo " Terraform $cmd failed!"
return 1
fi
echo " Terraform $cmd completed successfully."
timer $start_time
echo ""
return 0
}
echo " Starting Terraform operations for environment: $ENV"
echo "======================================================"
case "$COMMAND" in
init)
INIT_ARGS=""
if [ -n "$BACKEND_CONFIG" ]; then
INIT_ARGS="$BACKEND_CONFIG -reconfigure"
else
INIT_ARGS="-reconfigure"
fi
run_terraform "init" "$INIT_ARGS"
;;
plan)
if ! check_ssm_parameter_exists; then
echo " Warning: Database password parameter not found in SSM Parameter Store"
echo "This may cause the plan to fail if your infrastructure uses it."
echo "Consider running '$0 -e $ENV params' to set up the parameter."
echo ""
read -p "Do you want to continue anyway? (y/n): " CONTINUE
if [[ ! "$CONTINUE" =~ ^[Yy]$ ]]; then
echo "Operation aborted."
exit 0
fi
fi
run_terraform "plan" "$TF_VAR_FILE"
;;
apply)
ensure_db_password_parameter
run_terraform "apply" "$TF_VAR_FILE $APPLY_ARGS"
;;
destroy)
echo " WARNING: This will destroy all resources in the $ENV environment! "
if [ -z "$DESTROY_ARGS" ]; then
read -p "Are you absolutely sure? Type 'yes' to confirm: " confirm
if [ "$confirm" != "yes" ]; then
echo "Destruction aborted."
exit 0
fi
fi
run_terraform "destroy" "$TF_VAR_FILE $DESTROY_ARGS"
;;
output)
run_terraform "output" ""
;;
validate)
run_terraform "validate" ""
;;
workspace)
echo "Available workspaces:"
terraform workspace list
read -p "Enter workspace name to switch to (or 'new' to create): " workspace
if [ "$workspace" = "new" ]; then
read -p "Enter new workspace name: " new_workspace
run_terraform "workspace new" "$new_workspace"
else
run_terraform "workspace select" "$workspace"
fi
;;
fmt)
run_terraform "fmt" "-recursive"
;;
all)
echo " Running full deployment pipeline..."
ensure_db_password_parameter
INIT_ARGS=""
if [ -n "$BACKEND_CONFIG" ]; then
INIT_ARGS="$BACKEND_CONFIG -reconfigure"
else
INIT_ARGS="-reconfigure"
fi
run_terraform "init" "$INIT_ARGS" && \
run_terraform "validate" "" && \
run_terraform "plan" "$TF_VAR_FILE" && \
run_terraform "apply" "$TF_VAR_FILE $APPLY_ARGS"
if [ $? -eq 0 ]; then
echo " Full deployment completed successfully!"
run_terraform "output" ""
else
echo " Deployment pipeline failed."
exit 1
fi
;;
*)
echo "Unknown command: $COMMAND"
show_usage
exit 1
;;
esac
echo "======================================================"
echo " Operation completed for environment: $ENV"
cd - > /dev/null

=== hospital-data-chatbot-infrastructure/deploy/sagemaker_config.json (json) ===


=== hospital-data-chatbot-infrastructure/deploy/terraform-parameter-setup.sh (bash) ===
PROJECT_NAME="hospital-data-chatbot"
ENVIRONMENT="dev-cloud"
AWS_REGION="ap-south-1"
AWS_PROFILE="nash-cli-1"
while [[ $# -gt 0 ]]; do
case "$1" in
-p|--project-name)
PROJECT_NAME="$2"
shift 2
;;
-e|--environment)
ENVIRONMENT="$2"
shift 2
;;
-r|--region)
AWS_REGION="$2"
shift 2
;;
AWS_PROFILE="$2"
shift 2
;;
*)
echo "Unknown option: $1"
exit 1
;;
esac
done
if [ -n "$AWS_PROFILE" ]; then
export AWS_PROFILE="$AWS_PROFILE"
fi
echo "Setting up SSM Parameter Store for ${PROJECT_NAME} (${ENVIRONMENT})"
read -s -p "Enter database password: " DB_PASSWORD
echo ""
PARAM_NAME="/${PROJECT_NAME}/${ENVIRONMENT}/db-password"
echo "Creating SSM Parameter: ${PARAM_NAME}"
aws ssm put-parameter \
echo "SSM Parameter created successfully."
echo ""
echo "You can now run Terraform without needing to specify the password."
echo "Make sure to set the 'db_password_parameter_name' variable to '${PARAM_NAME}'"

=== hospital-data-chatbot-infrastructure/deploy/bootstrap.sh (bash) ===
set -e
PROJECT_NAME="hospital-data-chatbot"
ENVIRONMENT="dev-cloud"
AWS_REGION="ap-south-1"
AWS_PROFILE="nash-cli-1"
function show_help() {
echo "Usage: $0 [options]"
echo ""
echo "Bootstrap Terraform infrastructure for Hospital Data Chatbot"
echo ""
echo "Options:"
echo "  -p, --project-name NAME   Project name (default: hospital-data-chatbot)"
echo "  -e, --environment ENV     Environment (default: dev-cloud)"
echo "  -r, --region REGION       AWS region (default: ap-south-1)"
echo "  --profile PROFILE         AWS profile to use (uses default or SSO session if not specified)"
echo "  -h, --help                Show this help message"
echo ""
}
while [[ $# -gt 0 ]]; do
case "$1" in
-p|--project-name)
PROJECT_NAME="$2"
shift 2
;;
-e|--environment)
ENVIRONMENT="$2"
shift 2
;;
-r|--region)
AWS_REGION="$2"
shift 2
;;
AWS_PROFILE="$2"
shift 2
;;
-h|--help)
show_help
exit 0
;;
*)
echo "Unknown option: $1"
show_help
exit 1
;;
esac
done
echo "Bootstrapping Terraform for ${PROJECT_NAME} (${ENVIRONMENT}) in ${AWS_REGION}"
if [ -n "$AWS_PROFILE" ]; then
echo "Using AWS profile: $AWS_PROFILE"
export AWS_PROFILE="$AWS_PROFILE"
fi
function check_aws_credentials() {
if aws sts get-caller-identity >/dev/null 2>&1; then
return 0
fi
if [ -n "$AWS_ACCESS_KEY_ID" ] && [ -n "$AWS_SECRET_ACCESS_KEY" ]; then
return 0
fi
return 1
}
echo "Checking AWS CLI configuration..."
if ! check_aws_credentials; then
echo "Error: No AWS credentials found. Please run 'aws sso login' or set AWS credential environment variables."
echo "If you're using AWS SSO, make sure to specify the profile with --profile <profile_name>"
exit 1
fi
AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query "Account" --output text)
echo "Using AWS Account ID: ${AWS_ACCOUNT_ID}"
BUCKET_NAME="${PROJECT_NAME}-terraform-state-${ENVIRONMENT}-${AWS_ACCOUNT_ID}"
echo "Creating S3 bucket for Terraform state: ${BUCKET_NAME}"
aws s3 mb s3://${BUCKET_NAME} --region ${AWS_REGION} || {
echo "Note: S3 bucket may already exist"
}
echo "Enabling versioning on S3 bucket..."
aws s3api put-bucket-versioning \
DYNAMO_TABLE="${PROJECT_NAME}-terraform-locks-${ENVIRONMENT}"
echo "Creating DynamoDB table for state locking: ${DYNAMO_TABLE}"
aws dynamodb create-table \
echo "Note: DynamoDB table may already exist"
}
BACKEND_DIR="deploy/terraform/environments/backend-config"
mkdir -p ${BACKEND_DIR}
cat > ${BACKEND_DIR}/${ENVIRONMENT}.hcl << EOF
bucket         = "${BUCKET_NAME}"
key            = "${ENVIRONMENT}/terraform.tfstate"
region         = "${AWS_REGION}"
dynamodb_table = "${DYNAMO_TABLE}"
encrypt        = true
EOF
echo "Created backend config file: ${BACKEND_DIR}/${ENVIRONMENT}.hcl"
echo "Bootstrap complete! You can now initialize Terraform:"
echo ""
echo "cd deploy/terraform"
echo "terraform init -backend-config=environments/backend-config/${ENVIRONMENT}.hcl"
echo ""
echo "Then apply the Terraform configuration:"
echo ""
echo "terraform apply -var-file=environments/${ENVIRONMENT}.tfvars -var=\"db_password=YOUR_PASSWORD\""

=== hospital-data-chatbot-infrastructure/deploy/terraform-secrets.sh (bash) ===
export TF_VAR_db_password="YourComplex-Password123!"  # Change this to a secure password
echo "Terraform secrets have been loaded into environment variables."
echo "Now you can run terraform-infra-manager.sh without exposing secrets on the command line."
echo "The password will be stored securely in AWS Parameter Store, not in Terraform state."

=== hospital-data-chatbot-infrastructure/deploy/terraform/outputs.tf (text) ===
output "vpc_id" {
description = "The ID of the VPC"
value       = module.networking.vpc_id
}
output "public_subnet_ids" {
description = "The IDs of the public subnets"
value       = module.networking.public_subnet_ids
}
output "private_subnet_ids" {
description = "The IDs of the private subnets"
value       = module.networking.private_subnet_ids
}
output "app_security_group_id" {
description = "The ID of the application security group"
value       = module.networking.app_security_group_id
}
output "db_security_group_id" {
description = "The ID of the database security group"
value       = module.networking.db_security_group_id
}
output "db_endpoint" {
description = "The connection endpoint for the RDS database"
value       = module.database.db_endpoint
}
output "db_name" {
description = "The name of the RDS database"
value       = module.database.db_name
}
output "ecr_repository_url" {
description = "The URL of the ECR repository"
value       = module.storage.ecr_repository_url
}
output "s3_bucket_name" {
description = "The name of the S3 bucket"
value       = module.storage.app_bucket_name
}
output "app_url" {
description = "The URL of the deployed application"
value       = "https://${module.app_deployment.alb_dns_name}"
}
output "notebook_url" {
description = "The URL of the SageMaker notebook instance (dev environment only)"
value       = var.enable_ml && var.environment == "dev-cloud" ? module.sagemaker[0].notebook_url : null
}
output "sns_topic_arn" {
description = "The ARN of the SNS topic for notifications"
value       = module.monitoring.sns_topic_arn
}

=== hospital-data-chatbot-infrastructure/deploy/terraform/main.tf (text) ===
terraform {
required_version = ">= 1.0.0"
required_providers {
aws = {
source  = "hashicorp/aws"
version = "~> 5.97.0"
}
}
backend "s3" {
}
}
locals {
common_tags = merge(
{
Environment = var.environment
Project     = var.project_name
ManagedBy   = "Terraform"
},
var.additional_tags
)
}
provider "aws" {
region = var.aws_region
default_tags {
tags = local.common_tags
}
}
module "monitoring" {
source = "./modules/monitoring"
project_name        = var.project_name
environment         = var.environment
alarm_email         = var.alarm_email
logs_retention_days = var.logs_retention_days
tags = local.common_tags
}
module "networking" {
source = "./modules/networking"
project_name       = var.project_name
environment        = var.environment
vpc_cidr           = var.vpc_cidr
availability_zones = var.availability_zones
enable_nat_gateway = var.enable_nat_gateway
tags = local.common_tags
}
module "storage" {
source = "./modules/storage"
project_name = var.project_name
environment  = var.environment
tags = local.common_tags
}
resource "aws_ssm_parameter" "db_password" {
name        = "/${var.project_name}/${var.environment}/db-password"
description = "Database password for ${var.project_name} ${var.environment}"
type        = "SecureString"
value       = var.db_password  # We still need to provide this as a variable input, but it's no longer stored in state
overwrite   = true
tags = local.common_tags
}
module "database" {
source = "./modules/database"
project_name      = var.project_name
environment       = var.environment
vpc_id            = module.networking.vpc_id
subnet_ids        = module.networking.private_subnet_ids
security_group_id = module.networking.db_security_group_id
instance_class    = var.db_instance_class
allocated_storage = var.db_allocated_storage
db_name           = "hospital_data_${replace(var.environment, "-", "_")}"
username          = "${replace(var.environment, "-", "_")}_user"
db_password_parameter_name = aws_ssm_parameter.db_password.name
tags = local.common_tags
}
module "bedrock" {
source = "./modules/bedrock"
project_name     = var.project_name
environment      = var.environment
aws_region       = var.aws_region
s3_bucket_arn    = module.storage.app_bucket_arn
bedrock_model_id = var.bedrock_model_id
enable_streaming = true  # Fixed: direct value instead of variable reference
tags = local.common_tags
}
module "sagemaker" {
count  = var.enable_ml ? 1 : 0
source = "./modules/sagemaker"
project_name            = var.project_name
environment             = var.environment
s3_bucket               = module.storage.app_bucket_name
vpc_id                  = module.networking.vpc_id
subnet_ids              = module.networking.private_subnet_ids
security_group_id       = module.networking.app_security_group_id
instance_type           = var.notebook_instance_type
deploy_notebook         = var.environment == "dev-cloud"
training_instance_type  = var.training_instance_type
inference_instance_type = var.inference_instance_type
tags = local.common_tags
}
module "app_deployment" {
source = "./modules/app_deployment"
project_name          = var.project_name
environment           = var.environment
aws_region            = var.aws_region
vpc_id                = module.networking.vpc_id
public_subnet_ids     = module.networking.public_subnet_ids
private_subnet_ids    = module.networking.private_subnet_ids
app_security_group_id = module.networking.app_security_group_id
ecr_repository_url    = module.storage.ecr_repository_url
s3_bucket_name        = module.storage.app_bucket_name
db_host_parameter_name     = module.database.db_host_parameter_name
db_port_parameter_name     = module.database.db_port_parameter_name
db_name_parameter_name     = module.database.db_name_parameter_name
db_username_parameter_name = module.database.db_username_parameter_name
db_password_parameter_name = module.database.db_password_parameter_name
image_tag             = var.image_tag
acm_certificate_arn   = var.acm_certificate_arn
task_cpu              = var.task_cpu
task_memory           = var.task_memory
desired_count         = var.desired_count
min_capacity          = var.min_capacity
max_capacity          = var.max_capacity
health_check_path     = "/api/health"
container_port        = 8080
host_port             = 8080
domain_name           = var.domain_name
create_route53_record = var.create_route53_record
route53_zone_id       = var.route53_zone_id
sns_topic_arn         = module.monitoring.sns_topic_arn
logs_retention_days   = var.logs_retention_days
environment_variables = {
APP_ENV            = var.environment
BEDROCK_MODEL_ID   = var.bedrock_model_id
USE_S3             = "True"
S3_BUCKET          = module.storage.app_bucket_name
AWS_REGION         = var.aws_region
}
tags = local.common_tags
}

=== hospital-data-chatbot-infrastructure/deploy/terraform/variables.tf (text) ===
variable "project_name" {
description = "Name of the project"
type        = string
default     = "hdc"
}
variable "environment" {
description = "Deployment environment (dev-cloud, stage, prod)"
type        = string
}
variable "aws_region" {
description = "AWS region to deploy to"
type        = string
default     = "ap-south-1"
}
variable "vpc_cidr" {
description = "CIDR block for the VPC"
type        = string
default     = "10.0.0.0/16"
}
variable "availability_zones" {
description = "List of availability zones to use"
type        = list(string)
default     = ["ap-south-1a", "ap-south-1b"]
}
variable "enable_nat_gateway" {
description = "Whether to provision a NAT Gateway for private subnets"
type        = bool
default     = false
}
variable "db_instance_class" {
description = "RDS instance class"
type        = string
default     = "db.t3.micro"
}
variable "db_allocated_storage" {
description = "Allocated storage for RDS instance (GB)"
type        = number
default     = 20
}
variable "db_password" {
description = "Password for the database"
type        = string
sensitive   = true
}
variable "task_cpu" {
description = "CPU units for ECS task (1024 = 1 vCPU)"
type        = number
default     = 1024
}
variable "task_memory" {
description = "Memory for ECS task (MB)"
type        = number
default     = 2048
}
variable "desired_count" {
description = "Desired count of ECS tasks"
type        = number
default     = 1
}
variable "min_capacity" {
description = "Minimum number of ECS tasks"
type        = number
default     = 1
}
variable "max_capacity" {
description = "Maximum number of ECS tasks"
type        = number
default     = 5
}
variable "image_tag" {
description = "Docker image tag to deploy"
type        = string
default     = "latest"
}
variable "acm_certificate_arn" {
description = "ARN of ACM certificate for HTTPS"
type        = string
default     = ""
}
variable "alarm_email" {
description = "Email address for alarm notifications"
type        = string
default     = ""
}
variable "enable_ml" {
description = "Whether to enable ML infrastructure"
type        = bool
default     = true
}
variable "bedrock_model_id" {
description = "ID of the Bedrock model to use"
type        = string
default     = "anthropic.claude-3-sonnet-20240229-v1:0"
}
variable "notebook_instance_type" {
description = "Instance type for SageMaker notebook instances"
type        = string
default     = "ml.t3.medium"
}
variable "training_instance_type" {
description = "Instance type for SageMaker training jobs"
type        = string
default     = "ml.m5.large"
}
variable "inference_instance_type" {
description = "Instance type for SageMaker inference endpoints"
type        = string
default     = "ml.t2.medium"
}
variable "domain_name" {
description = "Domain name for the application"
type        = string
default     = ""
}
variable "create_route53_record" {
description = "Whether to create a Route 53 record for the domain"
type        = bool
default     = false
}
variable "route53_zone_id" {
description = "ID of the Route 53 hosted zone"
type        = string
default     = ""
}
variable "logs_retention_days" {
description = "Retention period for CloudWatch logs in days"
type        = number
default     = 30
}
variable "enable_streaming" {
description = "Whether to enable streaming responses from Bedrock"
type        = bool
default     = true
}
variable "additional_tags" {
description = "Additional tags to apply to all resources"
type        = map(string)
default     = {}
}

=== hospital-data-chatbot-infrastructure/deploy/terraform/.terraform.lock.hcl (text) ===
provider "registry.terraform.io/hashicorp/aws" {
version     = "5.97.0"
constraints = "~> 5.97.0"
hashes = [
"h1:S8tUHe6n3sDhR0kCNm4uVxclxdICjHFYaFbpX4hkS+k=",
"h1:rUDE0OgA+6IiEA+w0cPp3/QQNH4SpjFjYcQ6p7byKS4=",
"zh:02790ad98b767d8f24d28e8be623f348bcb45590205708334d52de2fb14f5a95",
"zh:088b4398a161e45762dc28784fcc41c4fa95bd6549cb708b82de577f2d39ffc7",
"zh:0c381a457b7af391c43fc0167919443f6105ad2702bde4d02ddea9fd7c9d3539",
"zh:1a4b57a5043dcca64d8b8bae8b30ef4f6b98ed2144f792f39c4e816d3f1e2c56",
"zh:1bf00a67f39e67664337bde065180d41d952242801ebcd1c777061d4ffaa1cc1",
"zh:24c549f53d6bd022af31426d3e78f21264d8a72409821669e7fd41966ae68b2b",
"zh:3abda50bbddb35d86081fe39522e995280aea7f004582c4af22112c03ac8b375",
"zh:7388ed7f21ce2eb46bd9066626ce5f3e2a5705f67f643acce8ae71972f66eaf6",
"zh:96740f2ff94e5df2b2d29a5035a1a1026fe821f61712b2099b224fb2c2277663",
"zh:9b12af85486a96aedd8d7984b0ff811a4b42e3d88dad1a3fb4c0b580d04fa425",
"zh:9f399f8e8683a3a3a6d63a41c7c3a5a5f266eedef40ea69eba75bacf03699879",
"zh:bcf2b288d4706ebd198f75d2159663d657535483331107f2cdef381f10688baf",
"zh:cc76c8a9fc3bad05a8779c1f80fe8c388734f1ec1dd0affa863343490527b466",
"zh:de4359cf1b057bfe7a563be93829ec64bf72e7a2b85a72d075238081ef5eb1db",
"zh:e208fa77051a1f9fa1eff6c5c58aabdcab0de1695b97cdea7b8dd81df3e0ed73",
]
}

=== hospital-data-chatbot-infrastructure/deploy/terraform/deploy/terraform/environments/backend-config/dev-cloud.hcl (text) ===
bucket         = "hospital-data-chatbot-terraform-state-dev-cloud-817019235550"
key            = "dev-cloud/terraform.tfstate"
region         = "ap-south-1"
dynamodb_table = "hospital-data-chatbot-terraform-locks-dev-cloud"
encrypt        = true

=== hospital-data-chatbot-infrastructure/deploy/terraform/environments/stage.tfvars (text) ===
environment         = "stage"
aws_region          = "ap-south-1"
vpc_cidr            = "10.1.0.0/16"
availability_zones  = ["ap-south-1a", "ap-south-1b"]
enable_nat_gateway  = true
db_instance_class   = "db.t3.small"
db_allocated_storage = 50
task_cpu            = 1024
task_memory         = 2048
desired_count       = 2
min_capacity        = 2
max_capacity        = 4
alarm_email         = "stage-alerts@yourdomain.com"
enable_ml           = true

=== hospital-data-chatbot-infrastructure/deploy/terraform/environments/dev.tfvars (text) ===
aws_region     = "ap-south-1"
aws_account_id = "817019235550" # Replace with your AWS account ID
environment    = "dev"
project_name   = "hdc"
vpc_cidr             = "10.0.0.0/16"
public_subnet_cidrs  = ["10.0.1.0/24", "10.0.2.0/24"]
private_subnet_cidrs = ["10.0.3.0/24", "10.0.4.0/24"]
availability_zones   = ["ap-south-1"]
enable_nat_gateway   = true
web_ingress_cidr = ["0.0.0.0/0"] # Open to all in dev (not recommended for prod)
ssh_ingress_cidr = ["0.0.0.0/0"] # Restrict in higher environments
s3_versioning_enabled = true
web_instance_count       = 1
web_instance_type        = "t3.micro"
web_instance_volume_size = 20
ec2_ami_id               = "ami-0c55b159cbfafe1f0" # Update this with current Amazon Linux 2 AMI
ssh_key_name             = "dev-key"
cpu_alarm_threshold  = 80
enable_sns_alerts    = true
alert_email_addresses = ["naman.sharma89@gmail.com"]

=== hospital-data-chatbot-infrastructure/deploy/terraform/environments/dev-cloud.tfvars (text) ===
environment         = "dev-cloud"
aws_region          = "ap-south-1"
vpc_cidr            = "10.0.0.0/16"
availability_zones  = ["ap-south-1a","ap-south-1b"]
enable_nat_gateway  = false
db_instance_class   = "db.t3.micro"
db_allocated_storage = 20
task_cpu            = 1024
task_memory         = 2048
desired_count       = 1
min_capacity        = 1
max_capacity        = 2
alarm_email         = "naman.sharma89@gmail.com"  # Update with your email
enable_ml           = true
additional_tags     = {
Owner             = "Naman"
Project           = "HospitalDataChatbot"
Environment       = "Development"
}

=== hospital-data-chatbot-infrastructure/deploy/terraform/environments/prod.tfvars (text) ===
environment         = "prod"
aws_region          = "ap-south-1"
vpc_cidr            = "10.2.0.0/16"
availability_zones  = ["ap-south-1a", "ap-south-1b", "ap-south-1c"]
enable_nat_gateway  = true
db_instance_class   = "db.t3.medium"
db_allocated_storage = 100
task_cpu            = 2048
task_memory         = 4096
desired_count       = 3
min_capacity        = 3
max_capacity        = 10
alarm_email         = "prod-alerts@yourdomain.com"
enable_ml           = true

=== hospital-data-chatbot-infrastructure/deploy/terraform/environments/backend-config/stage.hcl (text) ===
bucket         = "hospital-data-chatbot-terraform-state-stage"
key            = "stage/terraform.tfstate"
region         = "ap-south-1"
dynamodb_table = "hospital-data-chatbot-terraform-locks-stage"
encrypt        = true

=== hospital-data-chatbot-infrastructure/deploy/terraform/environments/backend-config/dev-cloud.hcl (text) ===
bucket         = "deploy-terraform-state-dev-cloud-817019235550"
key            = "dev-cloud/terraform.tfstate"
region         = "ap-south-1"
dynamodb_table = "deploy-terraform-locks-dev-cloud"
encrypt        = true

=== hospital-data-chatbot-infrastructure/deploy/terraform/environments/backend-config/prod.hcl (text) ===
bucket         = "hospital-data-chatbot-terraform-state-prod"
key            = "prod/terraform.tfstate"
region         = "ap-south-1"
dynamodb_table = "hospital-data-chatbot-terraform-locks-prod"
encrypt        = true

=== hospital-data-chatbot-infrastructure/deploy/terraform/environments/backend-config/dev.hcl (text) ===
bucket         = "myapp-terraform-state-dev"
key            = "dev/terraform.tfstate"
region         = "ap-south-1"
dynamodb_table = "myapp-terraform-locks-dev"
encrypt        = true

=== hospital-data-chatbot-infrastructure/deploy/terraform/.terraform/terraform.tfstate (text) ===
{
"version": 3,
"terraform_version": "1.11.4",
"backend": {
"type": "s3",
"config": {
"access_key": null,
"acl": null,
"allowed_account_ids": null,
"assume_role": null,
"assume_role_with_web_identity": null,
"bucket": "deploy-terraform-state-dev-cloud-817019235550",
"custom_ca_bundle": null,
"dynamodb_endpoint": null,
"dynamodb_table": "deploy-terraform-locks-dev-cloud",
"ec2_metadata_service_endpoint": null,
"ec2_metadata_service_endpoint_mode": null,
"encrypt": true,
"endpoint": null,
"endpoints": null,
"forbidden_account_ids": null,
"force_path_style": null,
"http_proxy": null,
"https_proxy": null,
"iam_endpoint": null,
"insecure": null,
"key": "dev-cloud/terraform.tfstate",
"kms_key_id": null,
"max_retries": null,
"no_proxy": null,
"profile": null,
"region": "ap-south-1",
"retry_mode": null,
"secret_key": null,
"shared_config_files": null,
"shared_credentials_file": null,
"shared_credentials_files": null,
"skip_credentials_validation": null,
"skip_metadata_api_check": null,
"skip_region_validation": null,
"skip_requesting_account_id": null,
"skip_s3_checksum": null,
"sse_customer_key": null,
"sts_endpoint": null,
"sts_region": null,
"token": null,
"use_dualstack_endpoint": null,
"use_fips_endpoint": null,
"use_lockfile": null,
"use_path_style": null,
"workspace_key_prefix": null
},
"hash": 1410938009
}
}

=== hospital-data-chatbot-infrastructure/deploy/terraform/.terraform/providers/registry.terraform.io/hashicorp/aws/5.97.0/darwin_arm64/LICENSE.txt (text) ===
Copyright (c) 2017 HashiCorp, Inc.
Mozilla Public License Version 2.0
==================================
1. Definitions
1.1. "Contributor"
means each individual or legal entity that creates, contributes to
the creation of, or owns Covered Software.
1.2. "Contributor Version"
means the combination of the Contributions of others (if any) used
by a Contributor and that particular Contributor's Contribution.
1.3. "Contribution"
means Covered Software of a particular Contributor.
1.4. "Covered Software"
means Source Code Form to which the initial Contributor has attached
the notice in Exhibit A, the Executable Form of such Source Code
Form, and Modifications of such Source Code Form, in each case
including portions thereof.
1.5. "Incompatible With Secondary Licenses"
means
(a) that the initial Contributor has attached the notice described
in Exhibit B to the Covered Software; or
(b) that the Covered Software was made available under the terms of
version 1.1 or earlier of the License, but not also under the
terms of a Secondary License.
1.6. "Executable Form"
means any form of the work other than Source Code Form.
1.7. "Larger Work"
means a work that combines Covered Software with other material, in
a separate file or files, that is not Covered Software.
1.8. "License"
means this document.
1.9. "Licensable"
means having the right to grant, to the maximum extent possible,
whether at the time of the initial grant or subsequently, any and
all of the rights conveyed by this License.
1.10. "Modifications"
means any of the following:
(a) any file in Source Code Form that results from an addition to,
deletion from, or modification of the contents of Covered
Software; or
(b) any new file in Source Code Form that contains any Covered
Software.
1.11. "Patent Claims" of a Contributor
means any patent claim(s), including without limitation, method,
process, and apparatus claims, in any patent Licensable by such
Contributor that would be infringed, but for the grant of the
License, by the making, using, selling, offering for sale, having
made, import, or transfer of either its Contributions or its
Contributor Version.
1.12. "Secondary License"
means either the GNU General Public License, Version 2.0, the GNU
Lesser General Public License, Version 2.1, the GNU Affero General
Public License, Version 3.0, or any later versions of those
licenses.
1.13. "Source Code Form"
means the form of the work preferred for making modifications.
1.14. "You" (or "Your")
means an individual or a legal entity exercising rights under this
License. For legal entities, "You" includes any entity that
controls, is controlled by, or is under common control with You. For
purposes of this definition, "control" means (a) the power, direct
or indirect, to cause the direction or management of such entity,
whether by contract or otherwise, or (b) ownership of more than
fifty percent (50%) of the outstanding shares or beneficial
ownership of such entity.
2. License Grants and Conditions
2.1. Grants
Each Contributor hereby grants You a world-wide, royalty-free,
non-exclusive license:
(a) under intellectual property rights (other than patent or trademark)
Licensable by such Contributor to use, reproduce, make available,
modify, display, perform, distribute, and otherwise exploit its
Contributions, either on an unmodified basis, with Modifications, or
as part of a Larger Work; and
(b) under Patent Claims of such Contributor to make, use, sell, offer
for sale, have made, import, and otherwise transfer either its
Contributions or its Contributor Version.
2.2. Effective Date
The licenses granted in Section 2.1 with respect to any Contribution
become effective for each Contribution on the date the Contributor first
distributes such Contribution.
2.3. Limitations on Grant Scope
The licenses granted in this Section 2 are the only rights granted under
this License. No additional rights or licenses will be implied from the
distribution or licensing of Covered Software under this License.
Notwithstanding Section 2.1(b) above, no patent license is granted by a
Contributor:
(a) for any code that a Contributor has removed from Covered Software;
or
(b) for infringements caused by: (i) Your and any other third party's
modifications of Covered Software, or (ii) the combination of its
Contributions with other software (except as part of its Contributor
Version); or
(c) under Patent Claims infringed by Covered Software in the absence of
its Contributions.
This License does not grant any rights in the trademarks, service marks,
or logos of any Contributor (except as may be necessary to comply with
the notice requirements in Section 3.4).
2.4. Subsequent Licenses
No Contributor makes additional grants as a result of Your choice to
distribute the Covered Software under a subsequent version of this
License (see Section 10.2) or under the terms of a Secondary License (if
permitted under the terms of Section 3.3).
2.5. Representation
Each Contributor represents that the Contributor believes its
Contributions are its original creation(s) or it has sufficient rights
to grant the rights to its Contributions conveyed by this License.
2.6. Fair Use
This License is not intended to limit any rights You have under
applicable copyright doctrines of fair use, fair dealing, or other
equivalents.
2.7. Conditions
Sections 3.1, 3.2, 3.3, and 3.4 are conditions of the licenses granted
in Section 2.1.
3. Responsibilities
3.1. Distribution of Source Form
All distribution of Covered Software in Source Code Form, including any
Modifications that You create or to which You contribute, must be under
the terms of this License. You must inform recipients that the Source
Code Form of the Covered Software is governed by the terms of this
License, and how they can obtain a copy of this License. You may not
attempt to alter or restrict the recipients' rights in the Source Code
Form.
3.2. Distribution of Executable Form
If You distribute Covered Software in Executable Form then:
(a) such Covered Software must also be made available in Source Code
Form, as described in Section 3.1, and You must inform recipients of
the Executable Form how they can obtain a copy of such Source Code
Form by reasonable means in a timely manner, at a charge no more
than the cost of distribution to the recipient; and
(b) You may distribute such Executable Form under the terms of this
License, or sublicense it under different terms, provided that the
license for the Executable Form does not attempt to limit or alter
the recipients' rights in the Source Code Form under this License.
3.3. Distribution of a Larger Work
You may create and distribute a Larger Work under terms of Your choice,
provided that You also comply with the requirements of this License for
the Covered Software. If the Larger Work is a combination of Covered
Software with a work governed by one or more Secondary Licenses, and the
Covered Software is not Incompatible With Secondary Licenses, this
License permits You to additionally distribute such Covered Software
under the terms of such Secondary License(s), so that the recipient of
the Larger Work may, at their option, further distribute the Covered
Software under the terms of either this License or such Secondary
License(s).
3.4. Notices
You may not remove or alter the substance of any license notices
(including copyright notices, patent notices, disclaimers of warranty,
or limitations of liability) contained within the Source Code Form of
the Covered Software, except that You may alter any license notices to
the extent required to remedy known factual inaccuracies.
3.5. Application of Additional Terms
You may choose to offer, and to charge a fee for, warranty, support,
indemnity or liability obligations to one or more recipients of Covered
Software. However, You may do so only on Your own behalf, and not on
behalf of any Contributor. You must make it absolutely clear that any
such warranty, support, indemnity, or liability obligation is offered by
You alone, and You hereby agree to indemnify every Contributor for any
liability incurred by such Contributor as a result of warranty, support,
indemnity or liability terms You offer. You may include additional
disclaimers of warranty and limitations of liability specific to any
jurisdiction.
4. Inability to Comply Due to Statute or Regulation
If it is impossible for You to comply with any of the terms of this
License with respect to some or all of the Covered Software due to
statute, judicial order, or regulation then You must: (a) comply with
the terms of this License to the maximum extent possible; and (b)
describe the limitations and the code they affect. Such description must
be placed in a text file included with all distributions of the Covered
Software under this License. Except to the extent prohibited by statute
or regulation, such description must be sufficiently detailed for a
recipient of ordinary skill to be able to understand it.
5. Termination
5.1. The rights granted under this License will terminate automatically
if You fail to comply with any of its terms. However, if You become
compliant, then the rights granted under this License from a particular
Contributor are reinstated (a) provisionally, unless and until such
Contributor explicitly and finally terminates Your grants, and (b) on an
ongoing basis, if such Contributor fails to notify You of the
non-compliance by some reasonable means prior to 60 days after You have
come back into compliance. Moreover, Your grants from a particular
Contributor are reinstated on an ongoing basis if such Contributor
notifies You of the non-compliance by some reasonable means, this is the
first time You have received notice of non-compliance with this License
from such Contributor, and You become compliant prior to 30 days after
Your receipt of the notice.
5.2. If You initiate litigation against any entity by asserting a patent
infringement claim (excluding declaratory judgment actions,
counter-claims, and cross-claims) alleging that a Contributor Version
directly or indirectly infringes any patent, then the rights granted to
You by any and all Contributors for the Covered Software under Section
2.1 of this License shall terminate.
5.3. In the event of termination under Sections 5.1 or 5.2 above, all
end user license agreements (excluding distributors and resellers) which
have been validly granted by You or Your distributors under this License
prior to termination shall survive termination.
************************************************************************
*                                                                      *
*  6. Disclaimer of Warranty                                           *
*  -------------------------                                           *
*                                                                      *
*  Covered Software is provided under this License on an "as is"       *
*  basis, without warranty of any kind, either expressed, implied, or  *
*  statutory, including, without limitation, warranties that the       *
*  Covered Software is free of defects, merchantable, fit for a        *
*  particular purpose or non-infringing. The entire risk as to the     *
*  quality and performance of the Covered Software is with You.        *
*  Should any Covered Software prove defective in any respect, You     *
*  (not any Contributor) assume the cost of any necessary servicing,   *
*  repair, or correction. This disclaimer of warranty constitutes an   *
*  essential part of this License. No use of any Covered Software is   *
*  authorized under this License except under this disclaimer.         *
*                                                                      *
************************************************************************
************************************************************************
*                                                                      *
*  7. Limitation of Liability                                          *
*  --------------------------                                          *
*                                                                      *
*  Under no circumstances and under no legal theory, whether tort      *
*  (including negligence), contract, or otherwise, shall any           *
*  Contributor, or anyone who distributes Covered Software as          *
*  permitted above, be liable to You for any direct, indirect,         *
*  special, incidental, or consequential damages of any character      *
*  including, without limitation, damages for lost profits, loss of    *
*  goodwill, work stoppage, computer failure or malfunction, or any    *
*  and all other commercial damages or losses, even if such party      *
*  shall have been informed of the possibility of such damages. This   *
*  limitation of liability shall not apply to liability for death or   *
*  personal injury resulting from such party's negligence to the       *
*  extent applicable law prohibits such limitation. Some               *
*  jurisdictions do not allow the exclusion or limitation of           *
*  incidental or consequential damages, so this exclusion and          *
*  limitation may not apply to You.                                    *
*                                                                      *
************************************************************************
8. Litigation
Any litigation relating to this License may be brought only in the
courts of a jurisdiction where the defendant maintains its principal
place of business and such litigation shall be governed by laws of that
jurisdiction, without reference to its conflict-of-law provisions.
Nothing in this Section shall prevent a party's ability to bring
cross-claims or counter-claims.
9. Miscellaneous
This License represents the complete agreement concerning the subject
matter hereof. If any provision of this License is held to be
unenforceable, such provision shall be reformed only to the extent
necessary to make it enforceable. Any law or regulation which provides
that the language of a contract shall be construed against the drafter
shall not be used to construe this License against a Contributor.
10. Versions of the License
10.1. New Versions
Mozilla Foundation is the license steward. Except as provided in Section
10.3, no one other than the license steward has the right to modify or
publish new versions of this License. Each version will be given a
distinguishing version number.
10.2. Effect of New Versions
You may distribute the Covered Software under the terms of the version
of the License under which You originally received the Covered Software,
or under the terms of any subsequent version published by the license
steward.
10.3. Modified Versions
If you create software not governed by this License, and you want to
create a new license for such software, you may create and use a
modified version of this License if you rename the license and remove
any references to the name of the license steward (except to note that
such modified license differs from this License).
10.4. Distributing Source Code Form that is Incompatible With Secondary
Licenses
If You choose to distribute Source Code Form that is Incompatible With
Secondary Licenses under the terms of this version of the License, the
notice described in Exhibit B of this License must be attached.
Exhibit A - Source Code Form License Notice
This Source Code Form is subject to the terms of the Mozilla Public
License, v. 2.0. If a copy of the MPL was not distributed with this
file, You can obtain one at http://mozilla.org/MPL/2.0/.
If it is not possible or desirable to put the notice in a particular
file, then You may include the notice in a location (such as a LICENSE
file in a relevant directory) where a recipient would be likely to look
for such a notice.
You may add additional accurate notices of copyright ownership.
Exhibit B - "Incompatible With Secondary Licenses" Notice
This Source Code Form is "Incompatible With Secondary Licenses", as
defined by the Mozilla Public License, v. 2.0.

=== hospital-data-chatbot-infrastructure/deploy/terraform/.terraform/modules/modules.json (json) ===
{"Modules":[{"Key":"","Source":"","Dir":"."},{"Key":"app_deployment","Source":"./modules/app_deployment","Dir":"modules/app_deployment"},{"Key":"bedrock","Source":"./modules/bedrock","Dir":"modules/bedrock"},{"Key":"database","Source":"./modules/database","Dir":"modules/database"},{"Key":"ml_api","Source":"./modules/ml_api","Dir":"modules/ml_api"},{"Key":"monitoring","Source":"./modules/monitoring","Dir":"modules/monitoring"},{"Key":"networking","Source":"./modules/networking","Dir":"modules/networking"},{"Key":"sagemaker","Source":"./modules/sagemaker","Dir":"modules/sagemaker"},{"Key":"storage","Source":"./modules/storage","Dir":"modules/storage"}]}

=== hospital-data-chatbot-infrastructure/deploy/terraform/modules/database/outputs.tf (text) ===
output "db_endpoint" {
description = "The connection endpoint for the RDS database"
value       = aws_db_instance.postgres.endpoint
}
output "db_name" {
description = "The name of the RDS database"
value       = var.db_name
}
output "db_username" {
description = "The master username for the RDS instance"
value       = var.username
sensitive   = true
}
output "db_host_parameter_name" {
description = "The name of the SSM Parameter Store parameter containing the database host"
value       = aws_ssm_parameter.db_host.name
}
output "db_port_parameter_name" {
description = "The name of the SSM Parameter Store parameter containing the database port"
value       = aws_ssm_parameter.db_port.name
}
output "db_name_parameter_name" {
description = "The name of the SSM Parameter Store parameter containing the database name"
value       = aws_ssm_parameter.db_name.name
}
output "db_username_parameter_name" {
description = "The name of the SSM Parameter Store parameter containing the database username"
value       = aws_ssm_parameter.db_username.name
}
output "db_password_parameter_name" {
description = "The name of the SSM Parameter Store parameter containing the database password"
value       = var.db_password_parameter_name
}

=== hospital-data-chatbot-infrastructure/deploy/terraform/modules/database/main.tf (text) ===
resource "aws_db_subnet_group" "main" {
name       = "${var.project_name}-${var.environment}-db-subnet-group"
subnet_ids = var.subnet_ids
tags = {
Name = "${var.project_name}-${var.environment}-db-subnet-group"
}
}
data "aws_ssm_parameter" "db_password" {
name = var.db_password_parameter_name
}
resource "aws_db_instance" "postgres" {
identifier           = "${var.project_name}-${var.environment}"
engine               = "postgres"
engine_version       = "14"
instance_class       = var.instance_class
allocated_storage    = var.allocated_storage
storage_type         = "gp3"
db_name              = var.db_name
username             = var.username
password             = data.aws_ssm_parameter.db_password.value
vpc_security_group_ids = [var.security_group_id]
db_subnet_group_name   = aws_db_subnet_group.main.name
multi_az               = var.environment == "prod"
backup_retention_period = var.environment == "prod" ? 7 : 1
deletion_protection    = var.environment == "prod"
skip_final_snapshot    = var.environment != "prod"
parameter_group_name = aws_db_parameter_group.postgres.name
tags = {
Name = "${var.project_name}-${var.environment}-postgres"
}
}
resource "aws_db_parameter_group" "postgres" {
name   = "${var.project_name}-${var.environment}-pg-params"
family = "postgres14"
parameter {
name  = "log_statement"
value = var.environment == "prod" ? "none" : "ddl"
}
parameter {
name  = "log_min_duration_statement"
value = var.environment == "prod" ? "1000" : "100"
}
}
resource "aws_ssm_parameter" "db_host" {
name        = "/${var.project_name}/${var.environment}/db-host"
description = "Database hostname for ${var.project_name} ${var.environment}"
type        = "String"
value       = aws_db_instance.postgres.address
tags = {
Name = "${var.project_name}-${var.environment}-db-host"
}
}
resource "aws_ssm_parameter" "db_port" {
name        = "/${var.project_name}/${var.environment}/db-port"
description = "Database port for ${var.project_name} ${var.environment}"
type        = "String"
value       = aws_db_instance.postgres.port
tags = {
Name = "${var.project_name}-${var.environment}-db-port"
}
}
resource "aws_ssm_parameter" "db_name" {
name        = "/${var.project_name}/${var.environment}/db-name"
description = "Database name for ${var.project_name} ${var.environment}"
type        = "String"
value       = var.db_name
tags = {
Name = "${var.project_name}-${var.environment}-db-name"
}
}
resource "aws_ssm_parameter" "db_username" {
name        = "/${var.project_name}/${var.environment}/db-username"
description = "Database username for ${var.project_name} ${var.environment}"
type        = "String"
value       = var.username
tags = {
Name = "${var.project_name}-${var.environment}-db-username"
}
}

=== hospital-data-chatbot-infrastructure/deploy/terraform/modules/database/variables.tf (text) ===
variable "project_name" {
description = "Name of the project"
type        = string
}
variable "environment" {
description = "Deployment environment (dev-cloud, stage, prod)"
type        = string
}
variable "vpc_id" {
description = "VPC ID where the database will be deployed"
type        = string
}
variable "subnet_ids" {
description = "Subnet IDs where the database will be deployed"
type        = list(string)
}
variable "security_group_id" {
description = "Security group ID for the database"
type        = string
}
variable "instance_class" {
description = "Database instance class"
type        = string
default     = "db.t3.micro"
}
variable "allocated_storage" {
description = "Allocated storage in GB"
type        = number
default     = 20
}
variable "db_name" {
description = "Database name"
type        = string
}
variable "username" {
description = "Database username"
type        = string
}
variable "db_password_parameter_name" {
description = "Name of the SSM Parameter Store parameter that contains the database password"
type        = string
}
variable "tags" {
description = "Additional tags to apply to all resources"
type        = map(string)
default     = {}
}

=== hospital-data-chatbot-infrastructure/deploy/terraform/modules/networking/outputs.tf (text) ===
output "vpc_id" {
description = "The ID of the VPC"
value       = aws_vpc.main.id
}
output "public_subnet_ids" {
description = "The IDs of the public subnets"
value       = aws_subnet.public[*].id
}
output "private_subnet_ids" {
description = "The IDs of the private subnets"
value       = aws_subnet.private[*].id
}
output "app_security_group_id" {
description = "The ID of the application security group"
value       = aws_security_group.app.id
}
output "db_security_group_id" {
description = "The ID of the database security group"
value       = aws_security_group.db.id
}

=== hospital-data-chatbot-infrastructure/deploy/terraform/modules/networking/main.tf (text) ===
resource "aws_vpc" "main" {
cidr_block           = var.vpc_cidr
enable_dns_support   = true
enable_dns_hostnames = true
tags = merge(
{
Name = "${var.project_name}-${var.environment}-vpc"
},
var.tags
)
}
resource "aws_subnet" "public" {
count = length(var.availability_zones)
vpc_id                  = aws_vpc.main.id
cidr_block              = cidrsubnet(var.vpc_cidr, 8, count.index)
availability_zone       = var.availability_zones[count.index]
map_public_ip_on_launch = true
tags = merge(
{
Name = "${var.project_name}-${var.environment}-public-${count.index + 1}"
},
var.tags
)
}
resource "aws_subnet" "private" {
count = length(var.availability_zones)
vpc_id            = aws_vpc.main.id
cidr_block        = cidrsubnet(var.vpc_cidr, 8, count.index + length(var.availability_zones))
availability_zone = var.availability_zones[count.index]
tags = merge(
{
Name = "${var.project_name}-${var.environment}-private-${count.index + 1}"
},
var.tags
)
}
resource "aws_internet_gateway" "main" {
vpc_id = aws_vpc.main.id
tags = merge(
{
Name = "${var.project_name}-${var.environment}-igw"
},
var.tags
)
}
resource "aws_eip" "nat" {
count  = var.enable_nat_gateway ? 1 : 0
domain = "vpc"
tags = merge(
{
Name = "${var.project_name}-${var.environment}-nat-eip"
},
var.tags
)
}
resource "aws_nat_gateway" "main" {
count = var.enable_nat_gateway ? 1 : 0
allocation_id = aws_eip.nat[0].id
subnet_id     = aws_subnet.public[0].id
tags = merge(
{
Name = "${var.project_name}-${var.environment}-nat"
},
var.tags
)
depends_on = [aws_internet_gateway.main]
}
resource "aws_route_table" "public" {
vpc_id = aws_vpc.main.id
route {
cidr_block = "0.0.0.0/0"
gateway_id = aws_internet_gateway.main.id
}
tags = merge(
{
Name = "${var.project_name}-${var.environment}-public-rt"
},
var.tags
)
}
resource "aws_route_table" "private" {
vpc_id = aws_vpc.main.id
dynamic "route" {
for_each = var.enable_nat_gateway ? [1] : []
content {
cidr_block     = "0.0.0.0/0"
nat_gateway_id = aws_nat_gateway.main[0].id
}
}
tags = merge(
{
Name = "${var.project_name}-${var.environment}-private-rt"
},
var.tags
)
}
resource "aws_route_table_association" "public" {
count = length(aws_subnet.public)
subnet_id      = aws_subnet.public[count.index].id
route_table_id = aws_route_table.public.id
}
resource "aws_route_table_association" "private" {
count = length(aws_subnet.private)
subnet_id      = aws_subnet.private[count.index].id
route_table_id = aws_route_table.private.id
}
resource "aws_security_group" "app" {
name        = "${var.project_name}-${var.environment}-app-sg"
description = "Security group for application"
vpc_id      = aws_vpc.main.id
ingress {
from_port   = 80
to_port     = 80
protocol    = "tcp"
cidr_blocks = ["0.0.0.0/0"]
}
ingress {
from_port   = 443
to_port     = 443
protocol    = "tcp"
cidr_blocks = ["0.0.0.0/0"]
}
ingress {
from_port   = 8080
to_port     = 8080
protocol    = "tcp"
cidr_blocks = ["0.0.0.0/0"]
}
egress {
from_port   = 0
to_port     = 0
protocol    = "-1"
cidr_blocks = ["0.0.0.0/0"]
}
tags = merge(
{
Name = "${var.project_name}-${var.environment}-app-sg"
},
var.tags
)
}
resource "aws_security_group" "db" {
name        = "${var.project_name}-${var.environment}-db-sg"
description = "Security group for database"
vpc_id      = aws_vpc.main.id
ingress {
from_port       = 5432
to_port         = 5432
protocol        = "tcp"
security_groups = [aws_security_group.app.id]
}
egress {
from_port   = 0
to_port     = 0
protocol    = "-1"
cidr_blocks = ["0.0.0.0/0"]
}
tags = merge(
{
Name = "${var.project_name}-${var.environment}-db-sg"
},
var.tags
)
}

=== hospital-data-chatbot-infrastructure/deploy/terraform/modules/networking/variables.tf (text) ===
variable "project_name" {
description = "Name of the project"
type        = string
}
variable "environment" {
description = "Deployment environment (dev-cloud, stage, prod)"
type        = string
}
variable "vpc_cidr" {
description = "CIDR block for the VPC"
type        = string
}
variable "availability_zones" {
description = "List of availability zones to use"
type        = list(string)
}
variable "enable_nat_gateway" {
description = "Whether to provision a NAT Gateway for private subnets"
type        = bool
default     = false
}
variable "tags" {
description = "Additional tags to apply to all resources"
type        = map(string)
default     = {}
}

=== hospital-data-chatbot-infrastructure/deploy/terraform/modules/storage/outputs.tf (text) ===
output "app_bucket_name" {
description = "The name of the S3 bucket"
value       = aws_s3_bucket.app_bucket.id
}
output "app_bucket_arn" {
description = "The ARN of the S3 bucket"
value       = aws_s3_bucket.app_bucket.arn
}
output "ecr_repository_url" {
description = "The URL of the ECR repository"
value       = aws_ecr_repository.app_repository.repository_url
}
output "ecr_repository_arn" {
description = "The ARN of the ECR repository"
value       = aws_ecr_repository.app_repository.arn
}

=== hospital-data-chatbot-infrastructure/deploy/terraform/modules/storage/main.tf (text) ===
resource "aws_s3_bucket" "app_bucket" {
bucket = "${var.project_name}-${var.environment}-data"
tags = merge(
{
Name = "${var.project_name}-${var.environment}-data"
},
var.tags
)
}
resource "aws_s3_bucket_versioning" "app_bucket_versioning" {
bucket = aws_s3_bucket.app_bucket.id
versioning_configuration {
status = "Enabled"
}
}
resource "aws_s3_bucket_server_side_encryption_configuration" "app_bucket_encryption" {
bucket = aws_s3_bucket.app_bucket.id
rule {
apply_server_side_encryption_by_default {
sse_algorithm = "AES256"
}
}
}
resource "aws_ecr_repository" "app_repository" {
name = "${var.project_name}-${var.environment}"
image_scanning_configuration {
scan_on_push = true
}
tags = merge(
{
Name = "${var.project_name}-${var.environment}-repository"
},
var.tags
)
}
resource "aws_ecr_lifecycle_policy" "app_repository_policy" {
repository = aws_ecr_repository.app_repository.name
policy = jsonencode({
rules = [
{
rulePriority = 1
description  = "Keep only the latest 10 images"
selection = {
tagStatus     = "any"
countType     = "imageCountMoreThan"
countNumber   = 10
}
action = {
type = "expire"
}
}
]
})
}

=== hospital-data-chatbot-infrastructure/deploy/terraform/modules/storage/variables.tf (text) ===
variable "project_name" {
description = "Name of the project"
type        = string
}
variable "environment" {
description = "Deployment environment (dev-cloud, stage, prod)"
type        = string
}
variable "tags" {
description = "Additional tags to apply to resources"
type        = map(string)
default     = {}
}

=== hospital-data-chatbot-infrastructure/deploy/terraform/modules/bedrock/main.tf (text) ===
resource "aws_iam_role" "bedrock_role" {
name = "${var.project_name}-${var.environment}-bedrock-role"
assume_role_policy = jsonencode({
Version = "2012-10-17"
Statement = [
{
Action = "sts:AssumeRole"
Effect = "Allow"
Principal = {
Service = "bedrock.amazonaws.com"
}
}
]
})
tags = merge(
{
Name = "${var.project_name}-${var.environment}-bedrock-role"
},
var.tags
)
}
resource "aws_iam_policy" "bedrock_policy" {
name        = "${var.project_name}-${var.environment}-bedrock-policy"
description = "Policy for Amazon Bedrock access"
policy = jsonencode({
Version = "2012-10-17"
Statement = [
{
Action = [
"bedrock:InvokeModel",
"bedrock:InvokeModelWithResponseStream"
]
Effect   = "Allow"
Resource = "arn:aws:bedrock:${var.aws_region}::foundation-model/amazon.titan-text-express-v1"
},
{
Action = [
"s3:GetObject",
"s3:PutObject",
"s3:ListBucket"
]
Effect   = "Allow"
Resource = [
"${var.s3_bucket_arn}",
"${var.s3_bucket_arn}/*"
]
}
]
})
}
resource "aws_iam_role_policy_attachment" "bedrock_policy_attachment" {
role       = aws_iam_role.bedrock_role.name
policy_arn = aws_iam_policy.bedrock_policy.arn
}
data "aws_bedrock_foundation_model" "titan" {
model_id = "amazon.titan-text-express-v1"
}

=== hospital-data-chatbot-infrastructure/deploy/terraform/modules/bedrock/variables.tf (text) ===
variable "project_name" {
description = "Name of the project"
type        = string
}
variable "environment" {
description = "Deployment environment (dev-cloud, stage, prod)"
type        = string
}
variable "aws_region" {
description = "AWS region to deploy to"
type        = string
}
variable "s3_bucket_arn" {
description = "ARN of the S3 bucket for Bedrock to access"
type        = string
}
variable "bedrock_model_id" {
description = "ID of the Bedrock model to use"
type        = string
default     = "anthropic.claude-3-sonnet-20240229-v1:0"
}
variable "enable_streaming" {
description = "Whether to enable streaming responses from Bedrock"
type        = bool
default     = true
}
variable "tags" {
description = "Additional tags to apply to resources"
type        = map(string)
default     = {}
}
variable "bedrock_model_units" {
description = "Number of model units to provision for Bedrock"
type        = number
default     = 1
}
variable "provision_dedicated_throughput" {
description = "Whether to provision dedicated throughput for the Bedrock model"
type        = bool
default     = false
}

=== hospital-data-chatbot-infrastructure/deploy/terraform/modules/monitoring/outputs.tf (text) ===
output "sns_topic_arn" {
description = "The ARN of the SNS topic for notifications"
value       = aws_sns_topic.alerts.arn
}
output "log_group_name" {
description = "The name of the CloudWatch log group"
value       = aws_cloudwatch_log_group.app_logs.name
}
output "dashboard_name" {
description = "The name of the CloudWatch dashboard"
value       = aws_cloudwatch_dashboard.main_dashboard.dashboard_name
}

=== hospital-data-chatbot-infrastructure/deploy/terraform/modules/monitoring/main.tf (text) ===
resource "aws_sns_topic" "alerts" {
name = "${var.project_name}-${var.environment}-alerts"
tags = merge(
{
Name = "${var.project_name}-${var.environment}-alerts"
},
var.tags
)
}
resource "aws_sns_topic_subscription" "email_subscription" {
count     = var.alarm_email != "" ? 1 : 0
topic_arn = aws_sns_topic.alerts.arn
protocol  = "email"
endpoint  = var.alarm_email
}
resource "aws_cloudwatch_log_group" "app_logs" {
name              = "/app/${var.project_name}/${var.environment}"
retention_in_days = var.logs_retention_days
tags = merge(
{
Name = "${var.project_name}-${var.environment}-logs"
},
var.tags
)
}
resource "aws_cloudwatch_dashboard" "main_dashboard" {
dashboard_name = "${var.project_name}-${var.environment}-dashboard"
dashboard_body = jsonencode({
widgets = [
{
type   = "metric"
x      = 0
y      = 0
width  = 12
height = 6
properties = {
metrics = [
["AWS/ECS", "CPUUtilization", "ServiceName", "${var.project_name}-${var.environment}-service", "ClusterName", "${var.project_name}-${var.environment}", { "stat": "Average" }]
]
view    = "timeSeries"
stacked = false
title   = "CPU Utilization"
region  = data.aws_region.current.name
period  = 300
}
},
{
type   = "metric"
x      = 12
y      = 0
width  = 12
height = 6
properties = {
metrics = [
["AWS/ECS", "MemoryUtilization", "ServiceName", "${var.project_name}-${var.environment}-service", "ClusterName", "${var.project_name}-${var.environment}", { "stat": "Average" }]
]
view    = "timeSeries"
stacked = false
title   = "Memory Utilization"
region  = data.aws_region.current.name
period  = 300
}
}
]
})
}
data "aws_region" "current" {}
resource "aws_cloudwatch_metric_alarm" "app_errors" {
alarm_name          = "${var.project_name}-${var.environment}-app-errors"
comparison_operator = "GreaterThanThreshold"
evaluation_periods  = 1
metric_name         = "HTTPCode_ELB_5XX_Count"
namespace           = "AWS/ApplicationELB"
period              = 60
statistic           = "Sum"
threshold           = 5
alarm_description   = "This alarm monitors for excessive 5XX errors"
alarm_actions       = [aws_sns_topic.alerts.arn]
ok_actions          = [aws_sns_topic.alerts.arn]
dimensions = {
LoadBalancer = "app/${var.project_name}-${var.environment}-alb/*"
}
tags = merge(
{
Name = "${var.project_name}-${var.environment}-app-errors-alarm"
},
var.tags
)
}

=== hospital-data-chatbot-infrastructure/deploy/terraform/modules/monitoring/variables.tf (text) ===
variable "project_name" {
description = "Name of the project"
type        = string
}
variable "environment" {
description = "Deployment environment (dev-cloud, stage, prod)"
type        = string
}
variable "alarm_email" {
description = "Email address for alarm notifications"
type        = string
default     = ""
}
variable "logs_retention_days" {
description = "Retention period for CloudWatch logs in days"
type        = number
default     = 30
}
variable "tags" {
description = "Additional tags to apply to resources"
type        = map(string)
default     = {}
}

=== hospital-data-chatbot-infrastructure/deploy/terraform/modules/app_deployment/outputs.tf (text) ===
output "alb_dns_name" {
description = "The DNS name of the application load balancer"
value       = aws_lb.app.dns_name
}
output "alb_arn" {
description = "The ARN of the application load balancer"
value       = aws_lb.app.arn
}
output "ecs_cluster_name" {
description = "The name of the ECS cluster"
value       = aws_ecs_cluster.main.name
}
output "ecs_service_name" {
description = "The name of the ECS service"
value       = local.use_https ? aws_ecs_service.app_https[0].name : aws_ecs_service.app_http[0].name
}
output "target_group_arn" {
description = "The ARN of the target group"
value       = aws_lb_target_group.app.arn
}
output "task_definition_arn" {
description = "The ARN of the task definition"
value       = aws_ecs_task_definition.app.arn
}

=== hospital-data-chatbot-infrastructure/deploy/terraform/modules/app_deployment/locals.tf (text) ===
locals {
use_https = var.acm_certificate_arn != ""
app_service_name = local.use_https ? aws_ecs_service.app_https[0].name : aws_ecs_service.app_http[0].name
app_service_id   = local.use_https ? aws_ecs_service.app_https[0].id : aws_ecs_service.app_http[0].id
}

=== hospital-data-chatbot-infrastructure/deploy/terraform/modules/app_deployment/main.tf (text) ===
data "aws_caller_identity" "current" {}
resource "aws_ecs_cluster" "main" {
name = "${var.project_name}-${var.environment}"
setting {
name  = "containerInsights"
value = var.environment == "prod" ? "enabled" : "disabled"
}
tags = merge(
{
Name = "${var.project_name}-${var.environment}-cluster"
},
var.tags
)
}
resource "aws_ecs_task_definition" "app" {
family                   = "${var.project_name}-${var.environment}"
network_mode             = "awsvpc"
requires_compatibilities = ["FARGATE"]
cpu                      = var.task_cpu
memory                   = var.task_memory
execution_role_arn       = aws_iam_role.ecs_execution.arn
task_role_arn            = aws_iam_role.ecs_task.arn
container_definitions = jsonencode([
{
name      = "${var.project_name}-${var.environment}-container"
image     = "${var.ecr_repository_url}:${var.image_tag}"
essential = true
portMappings = [
{
containerPort = 8080
hostPort      = 8080
protocol      = "tcp"
}
]
environment = [
{ name = "APP_ENV", value = var.environment },
{ name = "PORT", value = "8080" },
{ name = "BEDROCK_MODEL_ID", value = "anthropic.claude-3-sonnet-20240229-v1:0" },
{ name = "USE_S3", value = "True" },
{ name = "S3_BUCKET", value = var.s3_bucket_name },
{ name = "AWS_REGION", value = var.aws_region }
]
secrets = [
{
name      = "DB_HOST"
valueFrom = "arn:aws:ssm:${var.aws_region}:${data.aws_caller_identity.current.account_id}:parameter${var.db_host_parameter_name}"
},
{
name      = "DB_PORT"
valueFrom = "arn:aws:ssm:${var.aws_region}:${data.aws_caller_identity.current.account_id}:parameter${var.db_port_parameter_name}"
},
{
name      = "DB_NAME"
valueFrom = "arn:aws:ssm:${var.aws_region}:${data.aws_caller_identity.current.account_id}:parameter${var.db_name_parameter_name}"
},
{
name      = "DB_USER"
valueFrom = "arn:aws:ssm:${var.aws_region}:${data.aws_caller_identity.current.account_id}:parameter${var.db_username_parameter_name}"
},
{
name      = "DB_PASSWORD"
valueFrom = "arn:aws:ssm:${var.aws_region}:${data.aws_caller_identity.current.account_id}:parameter${var.db_password_parameter_name}"
}
]
logConfiguration = {
logDriver = "awslogs"
options = {
"awslogs-group"         = "/ecs/${var.project_name}-${var.environment}"
"awslogs-region"        = var.aws_region
"awslogs-stream-prefix" = "ecs"
}
}
}
])
tags = merge(
{
Name = "${var.project_name}-${var.environment}-task"
},
var.tags
)
}
resource "aws_iam_role" "ecs_execution" {
name = "${var.project_name}-${var.environment}-ecs-execution-role"
assume_role_policy = jsonencode({
Version = "2012-10-17"
Statement = [
{
Action = "sts:AssumeRole"
Effect = "Allow"
Principal = {
Service = "ecs-tasks.amazonaws.com"
}
}
]
})
tags = merge(
{
Name = "${var.project_name}-${var.environment}-ecs-execution-role"
},
var.tags
)
}
resource "aws_iam_role" "ecs_task" {
name = "${var.project_name}-${var.environment}-ecs-task-role"
assume_role_policy = jsonencode({
Version = "2012-10-17"
Statement = [
{
Action = "sts:AssumeRole"
Effect = "Allow"
Principal = {
Service = "ecs-tasks.amazonaws.com"
}
}
]
})
tags = merge(
{
Name = "${var.project_name}-${var.environment}-ecs-task-role"
},
var.tags
)
}
resource "aws_iam_role_policy_attachment" "ecs_execution_role_policy" {
role       = aws_iam_role.ecs_execution.name
policy_arn = "arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy"
}
resource "aws_iam_policy" "app_policy" {
name        = "${var.project_name}-${var.environment}-app-policy"
description = "Policy for ${var.project_name} application"
policy = jsonencode({
Version = "2012-10-17"
Statement = [
{
Action = [
"s3:GetObject",
"s3:PutObject",
"s3:ListBucket"
]
Effect   = "Allow"
Resource = [
"arn:aws:s3:::${var.s3_bucket_name}",
"arn:aws:s3:::${var.s3_bucket_name}/*"
]
},
{
Action = [
"bedrock:InvokeModel",
"bedrock:InvokeModelWithResponseStream"
]
Effect   = "Allow"
Resource = "*"
},
{
Action = [
"ssm:GetParameter",
"ssm:GetParameters"
]
Effect   = "Allow"
Resource = [
"arn:aws:ssm:${var.aws_region}:${data.aws_caller_identity.current.account_id}:parameter${var.db_host_parameter_name}",
"arn:aws:ssm:${var.aws_region}:${data.aws_caller_identity.current.account_id}:parameter${var.db_port_parameter_name}",
"arn:aws:ssm:${var.aws_region}:${data.aws_caller_identity.current.account_id}:parameter${var.db_name_parameter_name}",
"arn:aws:ssm:${var.aws_region}:${data.aws_caller_identity.current.account_id}:parameter${var.db_username_parameter_name}",
"arn:aws:ssm:${var.aws_region}:${data.aws_caller_identity.current.account_id}:parameter${var.db_password_parameter_name}"
]
}
]
})
}
resource "aws_iam_role_policy_attachment" "app_policy_attachment" {
role       = aws_iam_role.ecs_task.name
policy_arn = aws_iam_policy.app_policy.arn
}
resource "aws_cloudwatch_log_group" "app_logs" {
name              = "/ecs/${var.project_name}-${var.environment}"
retention_in_days = var.environment == "prod" ? 30 : 7
tags = merge(
{
Name = "${var.project_name}-${var.environment}-logs"
},
var.tags
)
}
resource "aws_lb" "app" {
name               = "${var.project_name}-${var.environment}-alb"
internal           = false
load_balancer_type = "application"
security_groups    = [var.app_security_group_id]
subnets            = var.public_subnet_ids
enable_deletion_protection = var.environment == "prod"
tags = merge(
{
Name = "${var.project_name}-${var.environment}-alb"
},
var.tags
)
}
resource "aws_lb_target_group" "app" {
name        = "${var.project_name}-${var.environment}-tg"
port        = 8080
protocol    = "HTTP"
vpc_id      = var.vpc_id
target_type = "ip"
health_check {
enabled             = true
interval            = 30
path                = "/api/health"
port                = "traffic-port"
protocol            = "HTTP"
healthy_threshold   = 3
unhealthy_threshold = 3
timeout             = 5
matcher             = "200"
}
tags = merge(
{
Name = "${var.project_name}-${var.environment}-tg"
},
var.tags
)
}
resource "aws_lb_listener" "http" {
load_balancer_arn = aws_lb.app.arn
port              = 80
protocol          = "HTTP"
default_action {
type = local.use_https ? "redirect" : "forward"
dynamic "redirect" {
for_each = local.use_https ? [1] : []
content {
port        = "443"
protocol    = "HTTPS"
status_code = "HTTP_301"
}
}
dynamic "forward" {
for_each = local.use_https ? [] : [1]
content {
target_group {
arn = aws_lb_target_group.app.arn
}
}
}
}
}
resource "aws_lb_listener" "https" {
count = local.use_https ? 1 : 0
load_balancer_arn = aws_lb.app.arn
port              = 443
protocol          = "HTTPS"
ssl_policy        = "ELBSecurityPolicy-2016-08"
certificate_arn   = var.acm_certificate_arn
default_action {
type             = "forward"
target_group_arn = aws_lb_target_group.app.arn
}
}
resource "aws_ecs_service" "app_https" {
count                             = local.use_https ? 1 : 0
name                              = "${var.project_name}-${var.environment}-service"
cluster                           = aws_ecs_cluster.main.id
task_definition                   = aws_ecs_task_definition.app.arn
desired_count                     = var.desired_count
launch_type                       = "FARGATE"
scheduling_strategy               = "REPLICA"
health_check_grace_period_seconds = 60
force_new_deployment              = true
network_configuration {
subnets          = var.private_subnet_ids
security_groups  = [var.app_security_group_id]
assign_public_ip = false
}
load_balancer {
target_group_arn = aws_lb_target_group.app.arn
container_name   = "${var.project_name}-${var.environment}-container"
container_port   = 8080
}
deployment_circuit_breaker {
enable   = true
rollback = true
}
tags = merge(
{
Name = "${var.project_name}-${var.environment}-service"
},
var.tags
)
depends_on = [aws_lb_listener.https[0]]
}
resource "aws_ecs_service" "app_http" {
count                             = local.use_https ? 0 : 1
name                              = "${var.project_name}-${var.environment}-service"
cluster                           = aws_ecs_cluster.main.id
task_definition                   = aws_ecs_task_definition.app.arn
desired_count                     = var.desired_count
launch_type                       = "FARGATE"
scheduling_strategy               = "REPLICA"
health_check_grace_period_seconds = 60
force_new_deployment              = true
network_configuration {
subnets          = var.private_subnet_ids
security_groups  = [var.app_security_group_id]
assign_public_ip = false
}
load_balancer {
target_group_arn = aws_lb_target_group.app.arn
container_name   = "${var.project_name}-${var.environment}-container"
container_port   = 8080
}
deployment_circuit_breaker {
enable   = true
rollback = true
}
tags = merge(
{
Name = "${var.project_name}-${var.environment}-service"
},
var.tags
)
depends_on = [aws_lb_listener.http]
}
resource "aws_appautoscaling_target" "app" {
max_capacity       = var.max_capacity
min_capacity       = var.min_capacity
resource_id        = "service/${aws_ecs_cluster.main.name}/${local.app_service_name}"
scalable_dimension = "ecs:service:DesiredCount"
service_namespace  = "ecs"
}
resource "aws_appautoscaling_policy" "app_cpu" {
name               = "${var.project_name}-${var.environment}-cpu-policy"
policy_type        = "TargetTrackingScaling"
resource_id        = aws_appautoscaling_target.app.resource_id
scalable_dimension = aws_appautoscaling_target.app.scalable_dimension
service_namespace  = aws_appautoscaling_target.app.service_namespace
target_tracking_scaling_policy_configuration {
predefined_metric_specification {
predefined_metric_type = "ECSServiceAverageCPUUtilization"
}
target_value       = 70.0
scale_in_cooldown  = 300
scale_out_cooldown = 60
}
}
resource "aws_appautoscaling_policy" "app_memory" {
name               = "${var.project_name}-${var.environment}-memory-policy"
policy_type        = "TargetTrackingScaling"
resource_id        = aws_appautoscaling_target.app.resource_id
scalable_dimension = aws_appautoscaling_target.app.scalable_dimension
service_namespace  = aws_appautoscaling_target.app.service_namespace
target_tracking_scaling_policy_configuration {
predefined_metric_specification {
predefined_metric_type = "ECSServiceAverageMemoryUtilization"
}
target_value       = 70.0
scale_in_cooldown  = 300
scale_out_cooldown = 60
}
}
resource "aws_cloudwatch_metric_alarm" "service_high_cpu" {
alarm_name          = "${var.project_name}-${var.environment}-high-cpu"
comparison_operator = "GreaterThanThreshold"
evaluation_periods  = "2"
metric_name         = "CPUUtilization"
namespace           = "AWS/ECS"
period              = "60"
statistic           = "Average"
threshold           = "85"
alarm_description   = "High CPU utilization for ${var.project_name}-${var.environment} service"
alarm_actions       = [var.sns_topic_arn]
ok_actions          = [var.sns_topic_arn]
dimensions = {
ClusterName = aws_ecs_cluster.main.name
ServiceName = local.app_service_name
}
}
resource "aws_cloudwatch_metric_alarm" "service_high_memory" {
alarm_name          = "${var.project_name}-${var.environment}-high-memory"
comparison_operator = "GreaterThanThreshold"
evaluation_periods  = "2"
metric_name         = "MemoryUtilization"
namespace           = "AWS/ECS"
period              = "60"
statistic           = "Average"
threshold           = "85"
alarm_description   = "High memory utilization for ${var.project_name}-${var.environment} service"
alarm_actions       = [var.sns_topic_arn]
ok_actions          = [var.sns_topic_arn]
dimensions = {
ClusterName = aws_ecs_cluster.main.name
ServiceName = local.app_service_name
}
}
resource "aws_iam_policy" "ecs_execution_role_ssm_policy" {
name        = "${var.project_name}-${var.environment}-ecs-ssm-policy"
description = "Policy to allow ECS task execution role to access SSM parameters"
policy = jsonencode({
Version = "2012-10-17"
Statement = [
{
Action = [
"ssm:GetParameters",
"ssm:GetParameter"
]
Effect = "Allow"
Resource = [
"arn:aws:ssm:${var.aws_region}:${data.aws_caller_identity.current.account_id}:parameter${var.db_host_parameter_name}",
"arn:aws:ssm:${var.aws_region}:${data.aws_caller_identity.current.account_id}:parameter${var.db_port_parameter_name}",
"arn:aws:ssm:${var.aws_region}:${data.aws_caller_identity.current.account_id}:parameter${var.db_name_parameter_name}",
"arn:aws:ssm:${var.aws_region}:${data.aws_caller_identity.current.account_id}:parameter${var.db_username_parameter_name}",
"arn:aws:ssm:${var.aws_region}:${data.aws_caller_identity.current.account_id}:parameter${var.db_password_parameter_name}"
]
}
]
})
}
resource "aws_iam_role_policy_attachment" "ecs_execution_ssm_policy_attachment" {
role       = aws_iam_role.ecs_execution.name
policy_arn = aws_iam_policy.ecs_execution_role_ssm_policy.arn
}

=== hospital-data-chatbot-infrastructure/deploy/terraform/modules/app_deployment/variables.tf (text) ===
variable "project_name" {
description = "Name of the project"
type        = string
}
variable "environment" {
description = "Deployment environment (dev-cloud, stage, prod)"
type        = string
}
variable "aws_region" {
description = "AWS region to deploy resources"
type        = string
}
variable "vpc_id" {
description = "ID of the VPC where application will be deployed"
type        = string
}
variable "public_subnet_ids" {
description = "IDs of public subnets for the application load balancer"
type        = list(string)
}
variable "private_subnet_ids" {
description = "IDs of private subnets for the application containers"
type        = list(string)
}
variable "app_security_group_id" {
description = "ID of the security group for the application"
type        = string
}
variable "ecr_repository_url" {
description = "URL of the ECR repository containing the application image"
type        = string
}
variable "s3_bucket_name" {
description = "Name of the S3 bucket for application data"
type        = string
}
variable "image_tag" {
description = "Tag of the Docker image to deploy"
type        = string
default     = "latest"
}
variable "acm_certificate_arn" {
description = "ARN of the ACM certificate for HTTPS"
type        = string
default     = ""
}
variable "task_cpu" {
description = "CPU units for the ECS task (1024 = 1 vCPU)"
type        = number
default     = 1024
}
variable "task_memory" {
description = "Memory for the ECS task in MB"
type        = number
default     = 2048
}
variable "desired_count" {
description = "Desired count of tasks in the ECS service"
type        = number
default     = 1
}
variable "min_capacity" {
description = "Minimum capacity for Auto Scaling"
type        = number
default     = 1
}
variable "max_capacity" {
description = "Maximum capacity for Auto Scaling"
type        = number
default     = 5
}
variable "health_check_path" {
description = "Path for the ALB health check"
type        = string
default     = "/api/health"
}
variable "container_port" {
description = "Port the container listens on"
type        = number
default     = 8080
}
variable "host_port" {
description = "Port the host maps to the container port"
type        = number
default     = 8080
}
variable "sns_topic_arn" {
description = "ARN of the SNS topic for alarms"
type        = string
}
variable "domain_name" {
description = "Domain name for the application (empty means no custom domain)"
type        = string
default     = ""
}
variable "create_route53_record" {
description = "Whether to create a Route 53 record for the domain"
type        = bool
default     = false
}
variable "route53_zone_id" {
description = "ID of the Route 53 hosted zone (if create_route53_record is true)"
type        = string
default     = ""
}
variable "environment_variables" {
description = "Additional environment variables for the container"
type        = map(string)
default     = {}
}
variable "secrets" {
description = "Additional secrets for the container (key: name, value: ARN or path)"
type        = map(string)
default     = {}
}
variable "logs_retention_days" {
description = "Retention period for CloudWatch logs in days"
type        = number
default     = 30
}
variable "deployment_maximum_percent" {
description = "Maximum percentage of tasks during deployment"
type        = number
default     = 200
}
variable "deployment_minimum_healthy_percent" {
description = "Minimum healthy percentage during deployment"
type        = number
default     = 100
}
variable "tags" {
description = "Additional tags to apply to resources"
type        = map(string)
default     = {}
}
variable "db_host_parameter_name" {
description = "Name of the SSM Parameter Store parameter containing the database host"
type        = string
}
variable "db_port_parameter_name" {
description = "Name of the SSM Parameter Store parameter containing the database port"
type        = string
}
variable "db_name_parameter_name" {
description = "Name of the SSM Parameter Store parameter containing the database name"
type        = string
}
variable "db_username_parameter_name" {
description = "Name of the SSM Parameter Store parameter containing the database username"
type        = string
}
variable "db_password_parameter_name" {
description = "Name of the SSM Parameter Store parameter containing the database password"
type        = string
}

=== hospital-data-chatbot-infrastructure/deploy/terraform/modules/sagemaker/main.tf (text) ===
resource "aws_iam_role" "sagemaker_role" {
name = "${var.project_name}-${var.environment}-sagemaker-role"
assume_role_policy = jsonencode({
Version = "2012-10-17"
Statement = [
{
Effect = "Allow"
Principal = {
Service = "sagemaker.amazonaws.com"
}
Action = "sts:AssumeRole"
}
]
})
tags = {
Name = "${var.project_name}-${var.environment}-sagemaker-role"
}
}
resource "aws_iam_policy" "sagemaker_policy" {
name        = "${var.project_name}-${var.environment}-sagemaker-policy"
description = "Policy for SageMaker access"
policy = jsonencode({
Version = "2012-10-17"
Statement = [
{
Effect = "Allow"
Action = [
"s3:GetObject",
"s3:PutObject",
"s3:ListBucket",
"s3:CreateBucket"
]
Resource = [
"arn:aws:s3:::${var.s3_bucket}",
"arn:aws:s3:::${var.s3_bucket}/*"
]
},
{
Effect = "Allow"
Action = [
"logs:CreateLogGroup",
"logs:CreateLogStream",
"logs:PutLogEvents"
]
Resource = "arn:aws:logs:*:*:*"
},
{
Effect = "Allow"
Action = [
"ec2:CreateNetworkInterface",
"ec2:DescribeNetworkInterfaces",
"ec2:DeleteNetworkInterface",
"ec2:DescribeVpcs",
"ec2:DescribeSubnets",
"ec2:DescribeSecurityGroups"
]
Resource = "*"
}
]
})
}
resource "aws_iam_role_policy_attachment" "sagemaker_policy_attachment" {
role       = aws_iam_role.sagemaker_role.name
policy_arn = aws_iam_policy.sagemaker_policy.arn
}
resource "aws_sagemaker_notebook_instance" "dev_notebook" {
count = var.environment == "dev-cloud" ? 1 : 0
name                    = "${var.project_name}-${var.environment}-notebook"
role_arn                = aws_iam_role.sagemaker_role.arn
instance_type           = "ml.t3.medium"
subnet_id               = var.subnet_ids[0]
security_groups         = [var.security_group_id]
lifecycle_config_name   = aws_sagemaker_notebook_instance_lifecycle_configuration.notebook_config[0].name
direct_internet_access  = "Enabled"
tags = {
Name = "${var.project_name}-${var.environment}-notebook"
}
}
resource "aws_sagemaker_notebook_instance_lifecycle_configuration" "notebook_config" {
count = var.environment == "dev-cloud" ? 1 : 0
name = "${var.project_name}-${var.environment}-notebook-config"
on_start = base64encode(<<-EOF
set -e
sudo -u ec2-user -i <<'EOF2'
pip install polars pandas scikit-learn fastapi uvicorn
pip install boto3 botocore psycopg2-binary
EOF2
EOF
)
}
resource "aws_sagemaker_model_package_group" "hospital_models" {
model_package_group_name = "${var.project_name}-${var.environment}-models"
tags = {
Name = "${var.project_name}-${var.environment}-model-registry"
}
}
output "sagemaker_role_arn" {
value = aws_iam_role.sagemaker_role.arn
}
output "notebook_url" {
value = var.environment == "dev-cloud" ? aws_sagemaker_notebook_instance.dev_notebook[0].url : null
}

=== hospital-data-chatbot-infrastructure/deploy/terraform/modules/sagemaker/variables.tf (text) ===
variable "project_name" {
description = "Name of the project"
type        = string
}
variable "environment" {
description = "Deployment environment (dev-cloud, stage, prod)"
type        = string
}
variable "s3_bucket" {
description = "Name of the S3 bucket for SageMaker data"
type        = string
}
variable "vpc_id" {
description = "ID of the VPC where SageMaker resources will be deployed"
type        = string
}
variable "subnet_ids" {
description = "IDs of subnets where SageMaker resources will be deployed"
type        = list(string)
}
variable "security_group_id" {
description = "ID of the security group for SageMaker resources"
type        = string
}
variable "instance_type" {
description = "Instance type for SageMaker notebook instances"
type        = string
default     = "ml.t3.medium"
}
variable "volume_size" {
description = "Volume size in GB for SageMaker notebook instances"
type        = number
default     = 20
}
variable "direct_internet_access" {
description = "Whether to enable direct internet access for notebook instances"
type        = bool
default     = true
}
variable "deploy_notebook" {
description = "Whether to deploy a SageMaker notebook instance"
type        = bool
default     = true
}
variable "training_instance_type" {
description = "Instance type for SageMaker training jobs"
type        = string
default     = "ml.m5.large"
}
variable "inference_instance_type" {
description = "Instance type for SageMaker inference endpoints"
type        = string
default     = "ml.t2.medium"
}
variable "inference_instance_count" {
description = "Number of instances for SageMaker inference endpoints"
type        = number
default     = 1
}
variable "autoscaling_enabled" {
description = "Whether to enable autoscaling for inference endpoints"
type        = bool
default     = false
}
variable "autoscaling_min_capacity" {
description = "Minimum capacity for autoscaling"
type        = number
default     = 1
}
variable "autoscaling_max_capacity" {
description = "Maximum capacity for autoscaling"
type        = number
default     = 5
}
variable "use_existing_role" {
description = "Whether to use an existing IAM role for SageMaker"
type        = bool
default     = false
}
variable "existing_role_arn" {
description = "ARN of an existing IAM role to use for SageMaker (if use_existing_role is true)"
type        = string
default     = ""
}
variable "tags" {
description = "Additional tags to apply to resources"
type        = map(string)
default     = {}
}

=== hospital-data-chatbot-infrastructure/deploy/terraform/modules/ml_api/outputs.tf (text) ===
output "ml_api_url" {
description = "URL of the ML API Gateway endpoint"
value       = "${aws_api_gateway_deployment.ml_api.invoke_url}"
}
output "ml_api_key" {
description = "API key for the ML API"
value       = aws_api_gateway_api_key.ml_api.value
sensitive   = true
}
output "ml_api_alb_dns_name" {
description = "DNS name of the ML API load balancer"
value       = aws_lb.ml_api.dns_name
}
output "ml_api_cluster_name" {
description = "Name of the ML API ECS cluster"
value       = aws_ecs_cluster.ml_api.name
}
output "ml_api_service_name" {
description = "Name of the ML API ECS service"
value       = aws_ecs_service.ml_api.name
}
output "ml_api_task_definition_arn" {
description = "ARN of the ML API task definition"
value       = aws_ecs_task_definition.ml_api.arn
}
output "ml_api_execution_role_arn" {
description = "ARN of the ML API execution role"
value       = aws_iam_role.ml_api_execution.arn
}
output "ml_api_task_role_arn" {
description = "ARN of the ML API task role"
value       = aws_iam_role.ml_api_task.arn
}

=== hospital-data-chatbot-infrastructure/deploy/terraform/modules/ml_api/main.tf (text) ===
data "aws_caller_identity" "current" {}
resource "aws_ecs_cluster" "ml_api" {
name = "${var.project_name}-${var.environment}-ml-api-cluster"
setting {
name  = "containerInsights"
value = var.environment == "prod" ? "enabled" : "disabled"
}
tags = var.tags
}
resource "aws_iam_role" "ml_api_execution" {
name = "${var.project_name}-${var.environment}-ml-api-execution-role"
assume_role_policy = jsonencode({
Version = "2012-10-17"
Statement = [
{
Action = "sts:AssumeRole"
Effect = "Allow"
Principal = {
Service = "ecs-tasks.amazonaws.com"
}
}
]
})
tags = var.tags
}
resource "aws_iam_role" "ml_api_task" {
name = "${var.project_name}-${var.environment}-ml-api-task-role"
assume_role_policy = jsonencode({
Version = "2012-10-17"
Statement = [
{
Action = "sts:AssumeRole"
Effect = "Allow"
Principal = {
Service = "ecs-tasks.amazonaws.com"
}
}
]
})
tags = var.tags
}
resource "aws_iam_role_policy_attachment" "ml_api_execution_role_policy" {
role       = aws_iam_role.ml_api_execution.name
policy_arn = "arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy"
}
resource "aws_iam_policy" "ml_api_execution_role_ssm_policy" {
name        = "${var.project_name}-${var.environment}-ml-api-ssm-policy"
description = "Policy to allow ML API execution role to access SSM parameters"
policy = jsonencode({
Version = "2012-10-17"
Statement = [
{
Action = [
"ssm:GetParameters",
"ssm:GetParameter"
]
Effect = "Allow"
Resource = [
"arn:aws:ssm:${var.aws_region}:${data.aws_caller_identity.current.account_id}:parameter${var.db_host_parameter_name}",
"arn:aws:ssm:${var.aws_region}:${data.aws_caller_identity.current.account_id}:parameter${var.db_port_parameter_name}",
"arn:aws:ssm:${var.aws_region}:${data.aws_caller_identity.current.account_id}:parameter${var.db_name_parameter_name}",
"arn:aws:ssm:${var.aws_region}:${data.aws_caller_identity.current.account_id}:parameter${var.db_username_parameter_name}",
"arn:aws:ssm:${var.aws_region}:${data.aws_caller_identity.current.account_id}:parameter${var.db_password_parameter_name}"
]
}
]
})
}
resource "aws_iam_role_policy_attachment" "ml_api_execution_ssm_policy_attachment" {
role       = aws_iam_role.ml_api_execution.name
policy_arn = aws_iam_policy.ml_api_execution_role_ssm_policy.arn
}
resource "aws_iam_policy" "ml_api_task_policy" {
name        = "${var.project_name}-${var.environment}-ml-api-task-policy"
description = "Policy for ML API tasks"
policy = jsonencode({
Version = "2012-10-17"
Statement = [
{
Action = [
"s3:GetObject",
"s3:PutObject",
"s3:ListBucket"
]
Effect = "Allow"
Resource = [
"arn:aws:s3:::${var.s3_bucket_name}",
"arn:aws:s3:::${var.s3_bucket_name}/*"
]
},
{
Action = [
"sagemaker:InvokeEndpoint",
"sagemaker:DescribeEndpoint",
"sagemaker:DescribeEndpointConfig",
"sagemaker:DescribeModel"
]
Effect = "Allow"
Resource = [
"arn:aws:sagemaker:${var.aws_region}:${data.aws_caller_identity.current.account_id}:endpoint/${var.project_name}-${var.environment}-*",
"arn:aws:sagemaker:${var.aws_region}:${data.aws_caller_identity.current.account_id}:endpoint-config/${var.project_name}-${var.environment}-*",
"arn:aws:sagemaker:${var.aws_region}:${data.aws_caller_identity.current.account_id}:model/${var.project_name}-${var.environment}-*"
]
},
{
Action = [
"ssm:GetParameter",
"ssm:GetParameters"
]
Effect = "Allow"
Resource = [
"arn:aws:ssm:${var.aws_region}:${data.aws_caller_identity.current.account_id}:parameter${var.db_host_parameter_name}",
"arn:aws:ssm:${var.aws_region}:${data.aws_caller_identity.current.account_id}:parameter${var.db_port_parameter_name}",
"arn:aws:ssm:${var.aws_region}:${data.aws_caller_identity.current.account_id}:parameter${var.db_name_parameter_name}",
"arn:aws:ssm:${var.aws_region}:${data.aws_caller_identity.current.account_id}:parameter${var.db_username_parameter_name}",
"arn:aws:ssm:${var.aws_region}:${data.aws_caller_identity.current.account_id}:parameter${var.db_password_parameter_name}"
]
}
]
})
}
resource "aws_iam_role_policy_attachment" "ml_api_task_policy_attachment" {
role       = aws_iam_role.ml_api_task.name
policy_arn = aws_iam_policy.ml_api_task_policy.arn
}
resource "aws_lb" "ml_api" {
name               = "${var.project_name}-${var.environment}-ml-api-alb"
internal           = true  # Internal ALB since we'll expose via API Gateway
load_balancer_type = "application"
security_groups    = [var.app_security_group_id]
subnets            = var.private_subnet_ids
enable_deletion_protection = var.environment == "prod"
tags = var.tags
}
resource "aws_lb_target_group" "ml_api" {
name        = "${var.project_name}-${var.environment}-ml-api-tg"
port        = 8080
protocol    = "HTTP"
vpc_id      = var.vpc_id
target_type = "ip"
health_check {
enabled             = true
interval            = 30
path                = "/models"
port                = "traffic-port"
protocol            = "HTTP"
healthy_threshold   = 3
unhealthy_threshold = 3
timeout             = 5
matcher             = "200"
}
tags = var.tags
}
resource "aws_lb_listener" "ml_api_http" {
load_balancer_arn = aws_lb.ml_api.arn
port              = 80
protocol          = "HTTP"
default_action {
type             = "forward"
target_group_arn = aws_lb_target_group.ml_api.arn
}
}
resource "aws_ecs_task_definition" "ml_api" {
family                   = "${var.project_name}-${var.environment}-ml-api"
network_mode             = "awsvpc"
requires_compatibilities = ["FARGATE"]
cpu                      = var.task_cpu
memory                   = var.task_memory
execution_role_arn       = aws_iam_role.ml_api_execution.arn
task_role_arn            = aws_iam_role.ml_api_task.arn
container_definitions = jsonencode([
{
name      = "${var.project_name}-${var.environment}-ml-api-container"
image     = "${var.ecr_repository_url}:${var.image_tag}"
essential = true
portMappings = [
{
containerPort = 8080
hostPort      = 8080
protocol      = "tcp"
}
]
environment = [
{ name = "APP_ENV", value = var.environment },
{ name = "PORT", value = "8080" },
{ name = "AWS_REGION", value = var.aws_region },
{ name = "SAGEMAKER_ENDPOINT_PREFIX", value = "${var.project_name}-${var.environment}" },
{ name = "SAGEMAKER_ROLE_ARN", value = var.sagemaker_role_arn },
{ name = "USE_S3", value = "True" },
{ name = "S3_BUCKET", value = var.s3_bucket_name }
]
secrets = [
{
name      = "DB_HOST"
valueFrom = "arn:aws:ssm:${var.aws_region}:${data.aws_caller_identity.current.account_id}:parameter${var.db_host_parameter_name}"
},
{
name      = "DB_PORT"
valueFrom = "arn:aws:ssm:${var.aws_region}:${data.aws_caller_identity.current.account_id}:parameter${var.db_port_parameter_name}"
},
{
name      = "DB_NAME"
valueFrom = "arn:aws:ssm:${var.aws_region}:${data.aws_caller_identity.current.account_id}:parameter${var.db_name_parameter_name}"
},
{
name      = "DB_USER"
valueFrom = "arn:aws:ssm:${var.aws_region}:${data.aws_caller_identity.current.account_id}:parameter${var.db_username_parameter_name}"
},
{
name      = "DB_PASSWORD"
valueFrom = "arn:aws:ssm:${var.aws_region}:${data.aws_caller_identity.current.account_id}:parameter${var.db_password_parameter_name}"
}
]
logConfiguration = {
logDriver = "awslogs"
options = {
"awslogs-group"         = "/ecs/${var.project_name}-${var.environment}-ml-api"
"awslogs-region"        = var.aws_region
"awslogs-stream-prefix" = "ecs"
}
}
}
])
tags = var.tags
}
resource "aws_ecs_service" "ml_api" {
name                               = "${var.project_name}-${var.environment}-ml-api-service"
cluster                            = aws_ecs_cluster.ml_api.id
task_definition                    = aws_ecs_task_definition.ml_api.arn
desired_count                      = var.desired_count
launch_type                        = "FARGATE"
scheduling_strategy                = "REPLICA"
health_check_grace_period_seconds  = 60
force_new_deployment               = true
network_configuration {
subnets          = var.private_subnet_ids
security_groups  = [var.app_security_group_id]
assign_public_ip = false
}
load_balancer {
target_group_arn = aws_lb_target_group.ml_api.arn
container_name   = "${var.project_name}-${var.environment}-ml-api-container"
container_port   = 8080
}
deployment_circuit_breaker {
enable   = true
rollback = true
}
tags = var.tags
}
resource "aws_cloudwatch_log_group" "ml_api_logs" {
name              = "/ecs/${var.project_name}-${var.environment}-ml-api"
retention_in_days = var.logs_retention_days
tags = var.tags
}
resource "aws_appautoscaling_target" "ml_api" {
max_capacity       = var.max_capacity
min_capacity       = var.min_capacity
resource_id        = "service/${aws_ecs_cluster.ml_api.name}/${aws_ecs_service.ml_api.name}"
scalable_dimension = "ecs:service:DesiredCount"
service_namespace  = "ecs"
}
resource "aws_appautoscaling_policy" "ml_api_cpu" {
name               = "${var.project_name}-${var.environment}-ml-api-cpu-policy"
policy_type        = "TargetTrackingScaling"
resource_id        = aws_appautoscaling_target.ml_api.resource_id
scalable_dimension = aws_appautoscaling_target.ml_api.scalable_dimension
service_namespace  = aws_appautoscaling_target.ml_api.service_namespace
target_tracking_scaling_policy_configuration {
predefined_metric_specification {
predefined_metric_type = "ECSServiceAverageCPUUtilization"
}
target_value       = 70.0
scale_in_cooldown  = 300
scale_out_cooldown = 60
}
}
resource "aws_appautoscaling_policy" "ml_api_memory" {
name               = "${var.project_name}-${var.environment}-ml-api-memory-policy"
policy_type        = "TargetTrackingScaling"
resource_id        = aws_appautoscaling_target.ml_api.resource_id
scalable_dimension = aws_appautoscaling_target.ml_api.scalable_dimension
service_namespace  = aws_appautoscaling_target.ml_api.service_namespace
target_tracking_scaling_policy_configuration {
predefined_metric_specification {
predefined_metric_type = "ECSServiceAverageMemoryUtilization"
}
target_value       = 70.0
scale_in_cooldown  = 300
scale_out_cooldown = 60
}
}
resource "aws_cloudwatch_metric_alarm" "ml_api_high_cpu" {
alarm_name          = "${var.project_name}-${var.environment}-ml-api-high-cpu"
comparison_operator = "GreaterThanThreshold"
evaluation_periods  = "2"
metric_name         = "CPUUtilization"
namespace           = "AWS/ECS"
period              = "60"
statistic           = "Average"
threshold           = "85"
alarm_description   = "High CPU utilization for ML API service"
alarm_actions       = [var.sns_topic_arn]
ok_actions          = [var.sns_topic_arn]
dimensions = {
ClusterName = aws_ecs_cluster.ml_api.name
ServiceName = aws_ecs_service.ml_api.name
}
}
resource "aws_cloudwatch_metric_alarm" "ml_api_high_memory" {
alarm_name          = "${var.project_name}-${var.environment}-ml-api-high-memory"
comparison_operator = "GreaterThanThreshold"
evaluation_periods  = "2"
metric_name         = "MemoryUtilization"
namespace           = "AWS/ECS"
period              = "60"
statistic           = "Average"
threshold           = "85"
alarm_description   = "High memory utilization for ML API service"
alarm_actions       = [var.sns_topic_arn]
ok_actions          = [var.sns_topic_arn]
dimensions = {
ClusterName = aws_ecs_cluster.ml_api.name
ServiceName = aws_ecs_service.ml_api.name
}
}

=== hospital-data-chatbot-infrastructure/deploy/terraform/modules/ml_api/variable.tf (text) ===
variable "project_name" {
description = "Name of the project"
type        = string
}
variable "environment" {
description = "Deployment environment (dev-cloud, stage, prod)"
type        = string
}
variable "aws_region" {
description = "AWS region to deploy resources"
type        = string
}
variable "vpc_id" {
description = "ID of the VPC where application will be deployed"
type        = string
}
variable "public_subnet_ids" {
description = "IDs of public subnets for the application load balancer"
type        = list(string)
}
variable "private_subnet_ids" {
description = "IDs of private subnets for the application containers"
type        = list(string)
}
variable "app_security_group_id" {
description = "ID of the security group for the application"
type        = string
}
variable "ecr_repository_url" {
description = "URL of the ECR repository containing the application image"
type        = string
}
variable "s3_bucket_name" {
description = "Name of the S3 bucket for application data"
type        = string
}
variable "image_tag" {
description = "Tag of the Docker image to deploy"
type        = string
default     = "latest"
}
variable "task_cpu" {
description = "CPU units for the ECS task (1024 = 1 vCPU)"
type        = number
default     = 1024
}
variable "task_memory" {
description = "Memory for the ECS task in MB"
type        = number
default     = 2048
}
variable "desired_count" {
description = "Desired count of tasks in the ECS service"
type        = number
default     = 1
}
variable "min_capacity" {
description = "Minimum capacity for Auto Scaling"
type        = number
default     = 1
}
variable "max_capacity" {
description = "Maximum capacity for Auto Scaling"
type        = number
default     = 5
}
variable "logs_retention_days" {
description = "Retention period for CloudWatch logs in days"
type        = number
default     = 30
}
variable "sns_topic_arn" {
description = "ARN of the SNS topic for alarms"
type        = string
}
variable "sagemaker_role_arn" {
description = "ARN of the SageMaker execution role"
type        = string
}
variable "db_host_parameter_name" {
description = "Name of the SSM Parameter Store parameter containing the database host"
type        = string
}
variable "db_port_parameter_name" {
description = "Name of the SSM Parameter Store parameter containing the database port"
type        = string
}
variable "db_name_parameter_name" {
description = "Name of the SSM Parameter Store parameter containing the database name"
type        = string
}
variable "db_username_parameter_name" {
description = "Name of the SSM Parameter Store parameter containing the database username"
type        = string
}
variable "db_password_parameter_name" {
description = "Name of the SSM Parameter Store parameter containing the database password"
type        = string
}
variable "tags" {
description = "Additional tags to apply to resources"
type        = map(string)
default     = {}
}

=== hospital-data-chatbot-infrastructure/deploy/terraform/modules/ml_api/api_gateway.tf (text) ===
resource "aws_api_gateway_rest_api" "ml_api" {
name        = "${var.project_name}-${var.environment}-ml-api"
description = "API Gateway for ML API"
endpoint_configuration {
types = ["REGIONAL"]
}
tags = var.tags
}
resource "aws_api_gateway_resource" "ml_api_proxy" {
rest_api_id = aws_api_gateway_rest_api.ml_api.id
parent_id   = aws_api_gateway_rest_api.ml_api.root_resource_id
path_part   = "{proxy+}"
}
resource "aws_api_gateway_method" "ml_api_proxy" {
rest_api_id   = aws_api_gateway_rest_api.ml_api.id
resource_id   = aws_api_gateway_resource.ml_api_proxy.id
http_method   = "ANY"
authorization = "NONE"
api_key_required = true  # Require API key for security
request_parameters = {
"method.request.path.proxy" = true
}
}
resource "aws_api_gateway_integration" "ml_api_proxy" {
rest_api_id = aws_api_gateway_rest_api.ml_api.id
resource_id = aws_api_gateway_resource.ml_api_proxy.id
http_method = aws_api_gateway_method.ml_api_proxy.http_method
type                    = "HTTP_PROXY"
integration_http_method = "ANY"
uri                     = "http://${aws_lb.ml_api.dns_name}/{proxy}"
connection_type      = "VPC_LINK"
connection_id        = aws_api_gateway_vpc_link.ml_api.id
request_parameters = {
"integration.request.path.proxy" = "method.request.path.proxy"
}
}
resource "aws_api_gateway_method" "ml_api_root" {
rest_api_id   = aws_api_gateway_rest_api.ml_api.id
resource_id   = aws_api_gateway_rest_api.ml_api.root_resource_id
http_method   = "ANY"
authorization = "NONE"
api_key_required = true  # Require API key for security
}
resource "aws_api_gateway_integration" "ml_api_root" {
rest_api_id = aws_api_gateway_rest_api.ml_api.id
resource_id = aws_api_gateway_rest_api.ml_api.root_resource_id
http_method = aws_api_gateway_method.ml_api_root.http_method
type                    = "HTTP_PROXY"
integration_http_method = "ANY"
uri                     = "http://${aws_lb.ml_api.dns_name}/"
connection_type      = "VPC_LINK"
connection_id        = aws_api_gateway_vpc_link.ml_api.id
}
resource "aws_api_gateway_vpc_link" "ml_api" {
name        = "${var.project_name}-${var.environment}-ml-api-vpc-link"
description = "VPC Link for ML API ALB"
target_arns = [aws_lb.ml_api.arn]
}
resource "aws_api_gateway_deployment" "ml_api" {
depends_on = [
aws_api_gateway_integration.ml_api_proxy,
aws_api_gateway_integration.ml_api_root
]
rest_api_id = aws_api_gateway_rest_api.ml_api.id
stage_name  = var.environment
lifecycle {
create_before_destroy = true
}
}
resource "aws_api_gateway_api_key" "ml_api" {
name = "${var.project_name}-${var.environment}-ml-api-key"
}
resource "aws_api_gateway_usage_plan" "ml_api" {
name = "${var.project_name}-${var.environment}-ml-api-usage-plan"
api_stages {
api_id = aws_api_gateway_rest_api.ml_api.id
stage  = aws_api_gateway_deployment.ml_api.stage_name
}
quota_settings {
limit  = 10000
period = "MONTH"
}
throttle_settings {
burst_limit = 100
rate_limit  = 50
}
}
resource "aws_api_gateway_usage_plan_key" "ml_api" {
key_id        = aws_api_gateway_api_key.ml_api.id
key_type      = "API_KEY"
usage_plan_id = aws_api_gateway_usage_plan.ml_api.id
}
resource "aws_cloudwatch_metric_alarm" "api_gateway_5xx" {
alarm_name          = "${var.project_name}-${var.environment}-ml-api-gateway-5xx"
comparison_operator = "GreaterThanThreshold"
evaluation_periods  = "2"
metric_name         = "5XXError"
namespace           = "AWS/ApiGateway"
period              = "60"
statistic           = "Sum"
threshold           = "5"
alarm_description   = "This alarm monitors for 5XX errors in the ML API Gateway"
alarm_actions       = [var.sns_topic_arn]
dimensions = {
ApiName = aws_api_gateway_rest_api.ml_api.name
Stage   = var.environment
}
}
resource "aws_cloudwatch_metric_alarm" "api_gateway_4xx" {
alarm_name          = "${var.project_name}-${var.environment}-ml-api-gateway-4xx"
comparison_operator = "GreaterThanThreshold"
evaluation_periods  = "2"
metric_name         = "4XXError"
namespace           = "AWS/ApiGateway"
period              = "60"
statistic           = "Sum"
threshold           = "20"
alarm_description   = "This alarm monitors for excessive 4XX errors in the ML API Gateway"
alarm_actions       = [var.sns_topic_arn]
dimensions = {
ApiName = aws_api_gateway_rest_api.ml_api.name
Stage   = var.environment
}
}

=== .github/workflows/model-training.yml (yaml) ===
name: ML Model Training and Deployment
on:
workflow_dispatch:
inputs:
model_type:
description: 'Type of model to train'
required: true
default: 'readmission_risk'
type: choice
options:
- readmission_risk
- patient_risk
- diagnosis_cluster
environment:
description: 'Environment to deploy to'
required: true
default: 'dev-cloud'
type: choice
options:
- dev-cloud
- stage
- prod
env:
AWS_REGION: ap-south-1
PROJECT_NAME: hospital-data-chatbot
jobs:
train-model:
name: Train and Deploy ML Model
runs-on: ubuntu-latest
steps:
- uses: actions/checkout@v3
- name: Set up Python 3.9
uses: actions/setup-python@v4
with:
python-version: '3.9'
- name: Install dependencies
run: |
python -m pip install --upgrade pip
pip install boto3 sagemaker pandas scikit-learn
- name: Configure AWS credentials
uses: aws-actions/configure-aws-credentials@v2
with:
aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
aws-region: ${{ env.AWS_REGION }}
- name: Train and deploy model
id: train-deploy
run: |
python scripts/train_deploy_model.py \
- name: Report results
run: |
echo "Model training and deployment completed"
echo "Training job: ${{ steps.train-deploy.outputs.training_job }}"
echo "Endpoint: ${{ steps.train-deploy.outputs.endpoint }}"
echo "Model metrics: ${{ steps.train-deploy.outputs.metrics }}"

=== .github/workflows/hospital-chatbot-ci-cd.yml (yaml) ===
name: Hospital Data Chatbot CI/CD
on:
push:
branches: [main, develop]
paths:
- 'hospital-data-chatbot/**'
- '.github/workflows/hospital-chatbot-ci-cd.yml'
pull_request:
branches: [main]
paths:
- 'hospital-data-chatbot/**'
workflow_dispatch:
inputs:
environment:
description: 'Environment to deploy to'
required: true
default: 'dev'
type: choice
options:
- dev
- staging
- prod
env:
AWS_REGION: ap-south-1
ECR_REPOSITORY: hdc-dev-cloud
WORKING_DIRECTORY: hospital-data-chatbot
permissions:
id-token: write # Required for OIDC authentication
contents: read  # Required to checkout the repository
jobs:
test:
name: Test
runs-on: ubuntu-latest
defaults:
run:
working-directory: ${{ env.WORKING_DIRECTORY }}
services:
postgres:
image: postgres:14
env:
POSTGRES_PASSWORD: postgres
POSTGRES_DB: hospital_data_test
ports:
- 5432:5432
options: >-
steps:
- uses: actions/checkout@v3
- name: Set up Python 3.9
uses: actions/setup-python@v4
with:
python-version: '3.9'
- name: Install uv
run: pip install uv
- name: Setup virtual environment
run: |
uv venv
source .venv/bin/activate
uv pip install -e ".[dev]"
- name: Run linting
run: |
source .venv/bin/activate
flake8 app tests || true
black --check app tests || true
isort --check-only app tests || true
- name: Create test data directory
run: |
mkdir -p data/raw
touch data/raw/hospital_data.xlsx
- name: Run tests
run: |
source .venv/bin/activate
pytest tests/ -v || true
env:
APP_ENV: dev_local
DEBUG: True
DB_HOST: localhost
DB_PORT: 5432
DB_NAME: hospital_data_test
DB_USER: postgres
DB_PASSWORD: postgres
build-and-deploy:
name: Build and Deploy
needs: test
if: github.event_name != 'pull_request'
runs-on: ubuntu-latest
defaults:
run:
working-directory: ${{ env.WORKING_DIRECTORY }}
steps:
- uses: actions/checkout@v3
- name: Determine environment
id: environment
run: |
if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
echo "ENV=${{ github.event.inputs.environment }}" >> $GITHUB_ENV
elif [ "${{ github.ref }}" = "refs/heads/main" ]; then
echo "ENV=prod" >> $GITHUB_ENV
else
echo "ENV=dev" >> $GITHUB_ENV
fi
- name: Configure AWS credentials
uses: aws-actions/configure-aws-credentials@v4
with:
role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/github-actions-hospital-chatbot-${{ env.ENV }}
aws-region: ${{ env.AWS_REGION }}
mask-aws-account-id: true
- name: Login to Amazon ECR
id: login-ecr
uses: aws-actions/amazon-ecr-login@v1
- name: Build and push Docker image
env:
ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
IMAGE_TAG: ${{ github.sha }}
run: |
mkdir -p data/raw
touch data/raw/hospital_data.xlsx
docker build -t $ECR_REGISTRY/${{ env.ECR_REPOSITORY }}-${{ env.ENV }}:$IMAGE_TAG \
.
docker tag $ECR_REGISTRY/${{ env.ECR_REPOSITORY }}-${{ env.ENV }}:$IMAGE_TAG \
$ECR_REGISTRY/${{ env.ECR_REPOSITORY }}-${{ env.ENV }}:latest
docker push $ECR_REGISTRY/${{ env.ECR_REPOSITORY }}-${{ env.ENV }}:$IMAGE_TAG
docker push $ECR_REGISTRY/${{ env.ECR_REPOSITORY }}-${{ env.ENV }}:latest
echo "IMAGE_URI=$ECR_REGISTRY/${{ env.ECR_REPOSITORY }}-${{ env.ENV }}:$IMAGE_TAG" >> $GITHUB_ENV
- name: Trigger ECS deployment
run: |
aws ecs update-service \
echo "ECS deployment triggered successfully"

=== hospital-data-chatbot/Dockerfile (text) ===
FROM python:3.9-slim
WORKDIR /app
RUN pip install uv
ENV APP_ENV=dev
ENV PYTHONUNBUFFERED=1
ENV PORT=8080
ENV DATA_DIR=data
COPY . .
RUN chmod +x docker-entrypoint.sh
RUN uv pip install -e .
EXPOSE 8080
ENTRYPOINT ["/app/docker-entrypoint.sh"]
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8080"]

=== hospital-data-chatbot/.uvproject (text) ===
version = 1
[tool]
python-version = "3.9"
[env]
requirements-dev = ".[dev]"

=== hospital-data-chatbot/pyproject.toml (toml) ===
[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"
[project]
name = "hospital-data-chatbot"
version = "0.1.0"
description = "AI-powered hospital data chatbot using Ollama and Text-to-SQL"
readme = "README.md"
requires-python = ">=3.9"
license = {text = "MIT"}
dependencies = [
"polars>=0.18.0",          # Fast DataFrame library
"numpy>=1.24.0",           # Numerical computing
"fastapi>=0.100.0",        # API framework
"uvicorn>=0.22.0",         # ASGI server
"openpyxl>=3.1.2",         # Excel file handling
"fastexcel>=0.7.0",        # Faster Excel support
"psycopg2-binary>=2.9.5",  # PostgreSQL adapter
"python-dotenv>=1.0.0",    # Environment variable management
"pydantic>=2.0.0",         # Data validation
"rich>=13.0.0",            # Better console output
"requests>=2.31.0",        # HTTP requests for Ollama API
"httpx>=0.24.1",           # Modern HTTP client with async support
"boto3>=1.28.0",           # AWS SDK (if using S3 or Bedrock)
]
[project.optional-dependencies]
dev = [
"pytest>=7.0.0",
"black>=23.0.0",
"isort>=5.0.0",
"flake8>=6.0.0",
"pytest-cov>=4.0.0",       # Coverage reporting
"mypy>=1.0.0",             # Type checking
]
prod = [
"gunicorn>=21.0.0",        # WSGI HTTP Server
]
database = [
"alembic>=1.10.0",         # Database migrations
"sqlalchemy>=2.0.0",       # SQL toolkit and ORM
]
ollama = [
"requests>=2.31.0",        # HTTP requests for Ollama API
"retry>=0.9.2",            # Retry mechanism for API calls
]
[tool.black]
line-length = 88
target-version = ["py39"]
[tool.isort]
profile = "black"
[tool.mypy]
python_version = "3.9"
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
strict_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_return_any = true
warn_unreachable = true
[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = "test_*.py"
python_functions = "test_*"

=== hospital-data-chatbot/Sequence.mermaid (text) ===
sequenceDiagram
participant User
participant OpenWebUI as OpenWebUI_Chat
participant API as Hospital API
participant MCPManager as MCP Context Manager
participant MCPCache as MCP Cache (Redis)
participant MCPDB as PostgreSQL_MCP
participant LLM as LLM_Model
participant HospitalDB as PostgreSQL_Hospital
User->>OpenWebUI: Input query
OpenWebUI->>API: Send query (OpenAI-compatible format)
%% MCP Context Flow
API->>MCPManager: Create/Get session
MCPManager->>MCPDB: Store session & query
API->>MCPManager: Check for similar queries
MCPManager->>MCPCache: Check Redis cache
alt Cache Hit
MCPCache-->>MCPManager: Return cached result
MCPManager-->>API: Return cached response
API-->>OpenWebUI: Send cached results
OpenWebUI-->>User: Display results
else Cache Miss
MCPManager->>MCPDB: Search similar patterns
MCPDB-->>MCPManager: Return patterns/context
API->>MCPManager: Get relevant context
MCPManager->>MCPDB: Retrieve context entries
MCPDB-->>MCPManager: Return context
%% LLM Processing with Context
API->>LLM: Send query + context + patterns
LLM->>LLM: Convert query to SQL
LLM-->>API: Return SQL command
%% Store SQL in MCP
API->>MCPManager: Store generated SQL
MCPManager->>MCPDB: Save SQL pattern
%% Execute SQL
API->>HospitalDB: Execute SQL command
HospitalDB-->>API: Return results
%% Cache Results
API->>MCPManager: Cache query results
MCPManager->>MCPCache: Store in Redis
MCPManager->>MCPDB: Update patterns & stats
%% Format Response
API->>LLM: Format results for user
LLM-->>API: Return formatted response
%% Store Response Context
API->>MCPManager: Store response in context
MCPManager->>MCPDB: Save response entry
API-->>OpenWebUI: Send formatted results
OpenWebUI-->>User: Display results
end

=== hospital-data-chatbot/README.md (markdown) ===
> An AI-powered chatbot for analyzing hospital patient data using AWS Bedrock, Text-to-SQL, and Machine Learning
![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)
![Python: 3.9+](https://img.shields.io/badge/Python-3.9+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-0.100.0+-ff69b4)
![Polars](https://img.shields.io/badge/Polars-0.18.0+-orange)
This application provides an intelligent chatbot interface for hospital staff to query patient and diagnosis data through natural language. It uses AWS Bedrock Large Language Models to interpret queries, converts them to SQL, and leverages machine learning models to provide advanced insights and predictions.
-  **Natural Language to SQL**: Convert plain language questions into precise SQL queries
-  **Machine Learning Insights**: Predictive analytics for patient risk and outcomes
-  **Statistical Analysis**: Accurate calculations on patient metrics and trends
-  **Advanced Data Processing**: Data sanitization and feature engineering pipelines
-  **Automated Data Pipeline**: Scheduled processing for up-to-date insights
-  **PostgreSQL Integration**: Direct querying of hospital database with proper relationships
-  **AWS Integration**: Leverages AWS Bedrock, SageMaker, Lambda, and other services
```mermaid
flowchart TD
subgraph DataSources["Data Sources"]
RDS[("AWS RDS<br>PostgreSQL")]
S3Raw[("AWS S3<br>Raw Data")]
end
subgraph DataProcessing["Data Processing"]
Lambda["AWS Lambda<br>Feature Engineering"]
S3Features[("AWS S3<br>Feature Store")]
end
subgraph MLPipeline["ML Pipeline"]
SageTrain["Amazon SageMaker<br>Training Jobs"]
SageModel["Amazon SageMaker<br>Model Registry"]
SageEndpoint["Amazon SageMaker<br>Endpoints"]
end
subgraph APILayer["API Layer"]
API["Amazon API Gateway"]
EC2["EC2 Instance<br>FastAPI Application"]
ELB["Elastic Load Balancer"]
end
subgraph Monitoring["Monitoring & Management"]
CloudWatch["Amazon CloudWatch"]
CloudTrail["AWS CloudTrail"]
SNS["Amazon SNS<br>Alerts"]
end
%% Connections
RDS --> Lambda
S3Raw --> Lambda
Lambda --> S3Features
S3Features --> SageTrain
SageTrain --> SageModel
SageModel --> SageEndpoint
Lambda --> EC2
SageEndpoint --> EC2
EC2 --> ELB
ELB --> API
SageEndpoint --> CloudWatch
EC2 --> CloudWatch
CloudWatch --> SNS
API --> CloudTrail
%% Style definitions
classDef aws fill:#FF9900,stroke:#232F3E,color:#232F3E,stroke-width:2px
classDef db fill:#3B48CC,stroke:#232F3E,color:white,stroke-width:2px
classDef storage fill:#3B48CC,stroke:#232F3E,color:white,stroke-width:2px
classDef api fill:#CC2264,stroke:#232F3E,color:white,stroke-width:2px
classDef compute fill:#EC7211,stroke:#232F3E,color:white,stroke-width:2px
classDef monitoring fill:#CC2264,stroke:#232F3E,color:white,stroke-width:2px
%% Apply styles
class RDS,S3Raw,S3Features db
class Lambda compute
class SageTrain,SageModel,SageEndpoint aws
class API,ELB api
class EC2 compute
class CloudWatch,CloudTrail,SNS monitoring
```
Our application uses AWS Bedrock to intelligently translate natural language questions into SQL queries:
1. **Query Understanding**: Analyzes intent and context of natural language questions
2. **Schema-Aware Translation**: Generates SQL based on hospital database schema
3. **SQL Validation**: Ensures queries are safe and optimized before execution
4. **Result Formatting**: Presents results in an easy-to-understand natural language format
Example query:
```
"How many patients over 65 were diagnosed with pneumonia last month?"
```
The system provides several ML-powered insights:
1. **Patient Risk Stratification**: Classifies patients by risk level using demographic and clinical factors
2. **Readmission Prediction**: Identifies patients at risk of 30-day readmission
3. **Diagnosis Clustering**: Groups similar diagnoses to uncover patterns
4. **Length of Stay Prediction**: Forecasts expected hospital stay duration
All models are trained using Amazon SageMaker and served through SageMaker endpoints.
```bash
./setup_uv.sh
```
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
python -m app.main
```
1. Install a local PostgreSQL instance or use Docker:
```bash
docker run --name postgres-test -e POSTGRES_PASSWORD=postgres -e POSTGRES_DB=hospital_data_test -p 5432:5432 -d postgres:14
```
2. Create a `.env` file in project root:
```
DEBUG=True
PORT=8080
DATA_DIR=data
DB_HOST=localhost
DB_PORT=5432
DB_NAME=hospital_data_test
DB_USER=postgres
DB_PASSWORD=postgres
USE_S3=False
```
3. Prepare test data:
```bash
mkdir -p data/raw
cp path/to/your/hospital_data.xlsx data/raw/
```
4. Launch the API:
```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8080
```
5. Access the API documentation:
- http://localhost:8080/docs
```
POST /api/db/sql-chat
```
Request body:
```json
{
"query": "How many male patients with diabetes were admitted last year?",
"include_sql": true,
"include_reasoning": false
}
```
```
GET /api/ml/patient-risk?patient_id=P12345
GET /api/ml/readmission-risk/P12345
GET /api/ml/diagnosis-clusters
```
```
GET /api/health
GET /api/data/stats
POST /api/chat
POST /api/import-to-db
```
```bash
docker build -t hospital-chatbot:latest .
aws ecr get-login-password | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com
docker tag hospital-chatbot:latest $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/hospital-chatbot:latest
docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/hospital-chatbot:latest
aws cloudformation deploy \
```
-  **SQL Injection Prevention**: All SQL queries are validated and sanitized
-  **Input Validation**: Comprehensive data validation at all entry points
-  **IAM Role-Based Access**: Fine-grained AWS permissions
-  **Data Encryption**: Hospital data encrypted at rest and in transit
-  **API Key Authentication**: Secure API access with key validation
```
hospital-data-chatbot/

 app/                         # Application code
    api/                     # API endpoints
       routes.py            # Main API routes
       sql_chat_routes.py   # Text-to-SQL endpoints
       ml_routes.py         # Machine learning endpoints
    config/                  # Configuration
    core/                    # Core logic
       data_processor.py    # Data processing and sanitization
       sql_query_engine.py  # Text-to-SQL engine
       llm_connector.py     # AWS Bedrock LLM interface
    ml/                      # ML components
       feature_engineering.py  # Feature extraction
       feature_store.py     # Feature storage and caching
       sagemaker_integration.py # Model training and deployment
       hospital_ml_models.py   # Domain-specific ML models
    models/                  # Data models
    utils/                   # Utilities
        db.py                # Database utilities
        calculation_handler.py # Statistical calculation handling

 data/                        # Data files
    raw/                     # Original data
    processed/               # Processed data

 deploy/                      # Deployment files
    cloudformation.yaml      # AWS CloudFormation template

 tests/                       # Unit tests

 scripts/                     # Utility scripts
    nightly_import.py        # Nightly data import

 Dockerfile                   # Container definition
 pyproject.toml               # Project dependencies
 setup.py                     # Package setup
 README.md                    # This file
```
-  **Model A/B Testing**: Compare different model versions for optimal performance
-  **Model Drift Detection**: Automatically detect when models need retraining
-  **Web Dashboard**: Interactive dashboard for visualizing ML insights
-  **Real-time Monitoring**: Stream processing for immediate alerts
-  **Enhanced Visualizations**: Visual representation of prediction results
-  **Natural Language Explanations**: Human-readable explanations of ML predictions
This project is licensed under the MIT License - see the LICENSE file for details.
- Naman Sharma

=== hospital-data-chatbot/architecture.mermaid (text) ===
graph TB
subgraph "Frontend Layer"
A[Open WebUI] --> B[Nginx/Load Balancer]
end
subgraph "API Gateway Layer"
B --> C[Kong/Traefik API Gateway]
C --> D[Rate Limiter/Auth]
end
subgraph "Application Layer"
D --> E[FastAPI Cluster]
E --> F[Query Router]
F --> G[SQL Engine]
F --> H[Chat Engine]
F --> I[Analytics Engine]
end
subgraph "MCP Context Layer"
G --> J[MCP Context Manager]
H --> J
I --> J
J --> K[MCP PostgreSQL Primary]
K --> L[MCP PostgreSQL Replica 1]
K --> M[MCP PostgreSQL Replica 2]
end
subgraph "Data Layer"
G --> N[Hospital Data PostgreSQL Primary]
N --> O[Read Replica 1]
N --> P[Read Replica 2]
J --> Q[Redis Context Cache]
J --> R[Vector DB - Pinecone/Weaviate]
end
subgraph "LLM Layer"
G --> S[Ollama Cluster/AWS Bedrock]
H --> S
I --> S
end
subgraph "Observability"
T[Prometheus] --> E
T --> K
T --> N
U[Grafana] --> T
V[ELK Stack] --> E
end

=== hospital-data-chatbot/setup.py (python) ===
from setuptools import setup, find_packages
setup(
 name="hospital-data-chatbot",
 version="0.1.0",
 packages=find_packages(),
 include_package_data=True,
 python_requires=">=3.9",
)

=== hospital-data-chatbot/docker-entrypoint.sh (bash) ===
set -e
mkdir -p $DATA_DIR/raw
mkdir -p $DATA_DIR/processed
exec "$@"

=== hospital-data-chatbot/setup_uv.sh (bash) ===
if ! command -v uv &> /dev/null; then
echo "Installing uv package manager..."
curl -LsSf https://astral.sh/uv/install.sh | sh
fi
uv venv
source .venv/bin/activate
uv pip install -e .
uv pip install -e ".[dev]"
echo "Setup complete! Activate the environment with: source .venv/bin/activate"

=== hospital-data-chatbot/app/__init__.py (python) ===


=== hospital-data-chatbot/app/main.py (python) ===
from fastapi import FastAPI
from app.api.routes import router as api_router
from app.core.data_processor import DataProcessor
from app.core.sql_query_engine import SQLQueryEngine
from app.utils.logging import setup_logging
from fastapi.middleware.cors import CORSMiddleware
from app.config.settings import AppConfig
import os
def create_app():
    """Create and configure the FastAPI application."""
    app = FastAPI(
        title="Hospital Data Chatbot",
        description="AI-powered analysis of hospital patient data",
        version="0.1.0"
    )

    # Set up logging
    logger = setup_logging(log_to_file=True)
    logger.info(f"Starting application in {AppConfig.get_environment_name()} environment")

    # Check if data file exists before attempting to load
    data_file_path = os.path.join(AppConfig.DATA_DIR, 'raw', 'hospital_data.xlsx')
    if os.path.exists(data_file_path):
        # Initialize data processor and load data
        data_processor = DataProcessor(auto_ingest_db=not AppConfig.is_production())
        app.state.data_processor = data_processor
        logger.info("Data processor initialized and data loaded")
    else:
        # Initialize data processor without loading data
        logger.warning(f"Data file not found at {data_file_path}, initializing without data")
        data_processor = DataProcessor(auto_load=False, auto_ingest_db=False)
        app.state.data_processor = data_processor

    # Initialize SQL query engine
    sql_query_engine = SQLQueryEngine()
    app.state.sql_query_engine = sql_query_engine
    logger.info("SQL Query Engine initialized")

    # Include API routes
    app.include_router(api_router, prefix="/api")

    # Middleware for CORS
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"] if not AppConfig.is_production() else ["https://your-production-domain.com"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    # Apply environment-specific configurations
    if AppConfig.is_development():
        logger.debug("Development-specific configuration applied")

        # Add development routes if in development environment
        from app.api.dev_routes import router as dev_router
        app.include_router(dev_router, prefix="/dev")
        logger.debug("Development routes added")
    else:
        # Production & staging settings
        logger.info("Production/Staging configuration applied")

    logger.info("FastAPI application configured and ready")
    return app

app = create_app()

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("app.main:app", host="0.0.0.0", port=8080, reload=True)

=== hospital-data-chatbot/app/core/query_engine.py (python) ===
import polars as pl
from app.core.llm_connector import BedrockLLM
class QueryEngine:
    """Processes natural language queries on hospital data."""

    def __init__(self, patient_data, diagnosis_data):
        self.patient_data = patient_data
        self.diagnosis_data = diagnosis_data
        self.llm = BedrockLLM()

    def process_query(self, query):
        """
  Process a user query and always route to the LLM.
  For calculation-related queries, provide context with calculation hints.
        """
        # Prepare context with dataset information
        context = self._prepare_context(query)

        # Add calculation capabilities information to the context
        calculation_info = self._get_calculation_capabilities()
        context += calculation_info

        # Send to LLM with enhanced context
        llm_response = self.llm.query(query, context)

        return llm_response

    def _get_calculation_capabilities(self):
        """Provide information about calculation capabilities for the LLM."""
        return """
        CALCULATION CAPABILITIES:
        For any calculations, please use the following format to trigger our calculation API:

        [CALCULATE: dataset.operation(column_name)]

        Available datasets:
        - patient_details: Contains patient records
        - diagnosis_details: Contains diagnosis information related to patients

        Available operations:
        - mean(column_name): Calculate the average of values in the column
        - count(): Count the total number of records
        - min(column_name): Find the minimum value
        - max(column_name): Find the maximum value
        - sum(column_name): Sum all values in the column
        - median(column_name): Find the median value
        - std(column_name): Calculate standard deviation
        - count_unique(column_name): Count unique values
        - count_by_patient(column_name): Count diagnoses grouped by patient

        Example usage:
        - To calculate average stay duration: [CALCULATE: patient_details.mean(stay_duration)]
        - To count patients: [CALCULATE: patient_details.count()]
        - To count diagnoses per patient: [CALCULATE: diagnosis_details.count_by_patient(registry_id)]

        Please use these calculations when answering queries about statistics or numeric analysis.
        """
 def _prepare_context(self, query):
        """Prepare relevant data context based on the query."""
        # Create initial context with data overview
        context = "Hospital Data Overview:\n"
        context += f"1. Patient Details: {self.patient_data.height} patients with {self.patient_data.width} attributes\n"
        context += f"2. Diagnosis Details: {self.diagnosis_data.height} diagnoses with {self.diagnosis_data.width} attributes\n\n"

        # Explain the relationship between tables
        context += "Relationship: Each patient (identified by registry_id) can have multiple diagnoses.\n\n"

        # Add specific details based on whether the query seems patient-focused or diagnosis-focused
        if any(keyword in query.lower() for keyword in ["patient", "person", "individual", "admission", "discharge"]):
            # Patient-focused query
            context += self._get_patient_context(query)
            context += "\n" + self._get_brief_diagnosis_context()
        elif any(keyword in query.lower() for keyword in ["diagnosis", "condition", "disease", "treatment"]):
            # Diagnosis-focused query
            context += self._get_diagnosis_context(query)
            context += "\n" + self._get_brief_patient_context()
        else:
            # General query - provide both contexts
            context += self._get_patient_context(query, include_sample=True)
            context += "\n" + self._get_diagnosis_context(query, include_sample=True)

        return context

    def _get_patient_context(self, query, include_sample=True):
        """Get patient data context."""
  context="PATIENT DETAILS:\n"
  relevant_columns=self._identify_relevant_columns(query, self.patient_data)
  context+="Available patient attributes:\n"
  for col in relevant_columns:
   if col in self.patient_data.columns:
    dtype=self.patient_data.schema[col]
    context+=f"-{col} (Type: {dtype})\n"
  if include_sample:
   sample_size=min(5, self.patient_data.height)
   sample_df=self.patient_data.select(relevant_columns).slice(0, sample_size)
   sample_data=sample_df.to_string()
   context+=f"\nSample patient data (first {sample_size} records):\n{sample_data}\n"
  return context
 def _get_diagnosis_context(self, query, include_sample=True):
        """Get diagnosis data context."""
        context = "DIAGNOSIS DETAILS:\n"

        # Add column information
        relevant_columns = self._identify_relevant_columns(query, self.diagnosis_data)
        context += "Available diagnosis attributes:\n"
        for col in relevant_columns:
            if col in self.diagnosis_data.columns:
                dtype = self.diagnosis_data.schema[col]
                context += f"- {col} (Type: {dtype})\n"

        # Add sample data if requested
        if include_sample:
            sample_size = min(5, self.diagnosis_data.height)
            sample_df = self.diagnosis_data.select(relevant_columns).slice(0, sample_size)
            sample_data = sample_df.to_string()
            context += f"\nSample diagnosis data (first {sample_size} records):\n{sample_data}\n"

        return context

    def _get_brief_patient_context(self):
        """Get brief context about patient data."""
  key_columns=["registry_id", "age", "gender"]
  available_columns=[col for col in key_columns if col in self.patient_data.columns]
  return f"Patient data includes {', '.join(available_columns)} and other attributes."
 def _get_brief_diagnosis_context(self):
        """Get brief context about diagnosis data."""
        key_columns = ["registry_id", "diagnosis", "diagnosis_date"]
        available_columns = [col for col in key_columns if col in self.diagnosis_data.columns]

        return f"Diagnosis data includes {', '.join(available_columns)} and other attributes."

    def _identify_relevant_columns(self, query, df):
        """Identify columns that might be relevant to the query."""
  query_terms=query.lower().split()
  relevant_columns=[]
  for col in df.columns:
   if any(term in col.lower() for term in query_terms):
    relevant_columns.append(col)
  if "registry_id" in df.columns and "registry_id" not in relevant_columns:
   relevant_columns.insert(0, "registry_id")
  if len(relevant_columns)<3:
   if "age" in df.columns and "age" not in relevant_columns:
    relevant_columns.append("age")
   if "gender" in df.columns and "gender" not in relevant_columns:
    relevant_columns.append("gender")
   if "diagnosis" in df.columns and "diagnosis" not in relevant_columns:
    relevant_columns.append("diagnosis")
  if len(relevant_columns)<3:
   for col in df.columns[:5]:
    if col not in relevant_columns:
     relevant_columns.append(col)
  return relevant_columns[:8]

=== hospital-data-chatbot/app/core/data_processor.py (python) ===
import os
import re
import polars as pl
from pathlib import Path
from datetime import datetime
import concurrent.futures
from typing import Dict, Tuple, Optional, List, Any
from app.utils.aws import upload_to_s3
from app.utils.db import get_db_connection, create_tables, insert_data
from app.config.settings import AppConfig
from app.utils.logging import get_logger
class DataProcessor:
    """
    Enhanced data processor that handles loading, processing, and storing hospital data.
    Features:
    - Robust data loading from Excel with better error handling
    - Data sanitization for removing special characters and unwanted spaces
    - Efficient data type conversion and schema management
    - Parallel processing capabilities
    - Direct database ingestion
    - Data validation and cleaning
    """
 logger=get_logger(__name__)
 def __init__(self, auto_load: bool=True, auto_ingest_db: bool=False, recreate_db_schema: bool=False):
        """
        Initialize the data processor.

        Args:
            auto_load: Automatically load data during initialization
            auto_ingest_db: Automatically ingest data into database after loading
            recreate_db_schema: Drop and recreate database schema (development mode only)
        """
  self.patient_data=None
  self.diagnosis_data=None
  self.data_path=Path(AppConfig.DATA_DIR)/'raw'/'hospital_data.xlsx'
  self.recreate_db_schema=recreate_db_schema
  if self.recreate_db_schema and not AppConfig.is_development():
   self.logger.warning("Schema recreation requested but not allowed in non-development environment")
   self.recreate_db_schema=False
  if self.recreate_db_schema:
   self.logger.warning("Database schema will be dropped and recreated! Use this only in development.")
  self.stats={
   "patient_records": 0,
   "diagnosis_records": 0,
   "load_timestamp": None,
   "processing_time_sec": 0,
   "data_quality": {},
   "environment": AppConfig.get_environment_name(),
   "sanitization_stats": {}  # New field to track sanitization changes
  }
  self.logger.info(f"DataProcessor initialized in {AppConfig.get_environment_name()} environment")
  if AppConfig.is_production():
   self.logger.info("Production environment detected: Auto-ingest disabled for safety")
   auto_ingest_db=False
   self.recreate_db_schema=False
  if auto_load:
   self.load_data()
  if auto_ingest_db and self.patient_data is not None and self.diagnosis_data is not None:
   self.ingest_to_database()
 def load_data(self)->Tuple[pl.DataFrame, pl.DataFrame]:
        """
        Load hospital data from source Excel file with improved error handling.

        Returns:
            Tuple containing (patient_data, diagnosis_data)
        """
  start_time=datetime.now()
  if not os.path.exists(self.data_path):
   self.logger.error(f"Data file not found: {self.data_path}")
   raise FileNotFoundError(f"Hospital data file not found at {self.data_path}")
  self.logger.info(f"Loading data from {self.data_path}")
  self.patient_data=None
  self.diagnosis_data=None
  try:
   self._verify_excel_file()
   try:
    self.logger.info("Loading patient and diagnosis data")
    self.patient_data=self._load_patient_data()
    self.diagnosis_data=self._load_diagnosis_data()
   except Exception as e:
    self.logger.error(f"Error during data loading: {str(e)}", exc_info=True)
    raise ValueError(f"Failed to load data: {str(e)}")
   if self.patient_data is None:
    raise ValueError("Failed to load patient data")
   if self.diagnosis_data is None:
    raise ValueError("Failed to load diagnosis data")
   self.logger.info("Starting data sanitization process")
   try:
    patient_sanitize_stats, sanitized_patient_data=self._sanitize_dataframe(self.patient_data)
    self.patient_data=sanitized_patient_data
   except Exception as e:
    self.logger.warning(f"Patient data sanitization might have failed: {str(e)}")
    patient_sanitize_stats={"error": str(e)}
   try:
    diagnosis_sanitize_stats, sanitized_diagnosis_data=self._sanitize_dataframe(self.diagnosis_data)
    self.diagnosis_data=sanitized_diagnosis_data
   except Exception as e:
    self.logger.warning(f"Diagnosis data sanitization might have failed: {str(e)}")
    diagnosis_sanitize_stats={"error": str(e)}
   self.stats["sanitization_stats"]={
    "patient_data": patient_sanitize_stats,
    "diagnosis_data": diagnosis_sanitize_stats
   }
   self.stats["patient_records"]=self.patient_data.height
   self.stats["diagnosis_records"]=self.diagnosis_data.height
   self.stats["load_timestamp"]=datetime.now()
   self.stats["processing_time_sec"]=(datetime.now()-start_time).total_seconds()
   self.logger.info(f"Successfully loaded and sanitized {self.patient_data.height} patient records and "
       f"{self.diagnosis_data.height} diagnosis records")
   self._validate_data_integrity()
   return self.patient_data, self.diagnosis_data
  except Exception as e:
   self.logger.error(f"Failed to load data: {str(e)}", exc_info=True)
   raise ValueError(f"Failed to load data: {str(e)}")
 def _verify_excel_file(self):
        """
        Verify that the Excel file exists and contains the required sheets.
        Raises appropriate exceptions if verification fails.
        """
  import openpyxl
  try:
   self.logger.info(f"Verifying Excel file at {self.data_path}")
   workbook=openpyxl.load_workbook(self.data_path, read_only=True)
   sheet_names=workbook.sheetnames
   self.logger.debug(f"Excel file contains sheets: {sheet_names}")
   if "Patient Details" not in sheet_names:
    raise ValueError("Excel file missing required sheet 'Patient Details'")
   if "Diagnosis Details" not in sheet_names:
    raise ValueError("Excel file missing required sheet 'Diagnosis Details'")
   workbook.close()
   self.logger.info("Excel file verification successful")
  except Exception as e:
   self.logger.error(f"Excel file verification failed: {str(e)}", exc_info=True)
   raise ValueError(f"Excel file verification failed: {str(e)}")
 def _load_patient_data(self)->pl.DataFrame:
        """
        Load patient data from Excel with enhanced error handling.

        Returns:
            Patient data as a Polars DataFrame with standardized column names
        """
  try:
   self.logger.info(f"Loading patient data from {self.data_path}, sheet 'Patient Details'")
   try:
    df=pl.read_excel(self.data_path, sheet_name="Patient Details")
   except Exception as polars_error:
    self.logger.warning(f"Polars read_excel failed: {str(polars_error)}. Trying pandas fallback.")
    import pandas as pd
    pandas_df=pd.read_excel(self.data_path, sheet_name="Patient Details")
    df=pl.from_pandas(pandas_df)
    self.logger.info("Successfully loaded with pandas fallback")
   if df is None or df.height==0 or df.width==0:
    raise ValueError(f"No data found in Patient Details sheet (rows: {df.height if df is not None else 'None'}, cols: {df.width if df is not None else 'None'})")
   df=self._convert_column_names_to_snake_case(df)
   self.logger.debug(f"Loaded patient data with {df.height} rows and {df.width} columns")
   return df
  except Exception as e:
   self.logger.error(f"Error loading patient data: {str(e)}", exc_info=True)
   raise
 def _load_diagnosis_data(self)->pl.DataFrame:
        """
        Load diagnosis data from Excel with enhanced error handling.

        Returns:
            Diagnosis data as a Polars DataFrame with standardized column names
        """
  try:
   self.logger.info(f"Loading diagnosis data from {self.data_path}, sheet 'Diagnosis Details'")
   try:
    df=pl.read_excel(self.data_path, sheet_name="Diagnosis Details")
   except Exception as polars_error:
    self.logger.warning(f"Polars read_excel failed: {str(polars_error)}. Trying pandas fallback.")
    import pandas as pd
    pandas_df=pd.read_excel(self.data_path, sheet_name="Diagnosis Details")
    df=pl.from_pandas(pandas_df)
    self.logger.info("Successfully loaded with pandas fallback")
   if df is None or df.height==0 or df.width==0:
    raise ValueError(f"No data found in Diagnosis Details sheet (rows: {df.height if df is not None else 'None'}, cols: {df.width if df is not None else 'None'})")
   df=self._convert_column_names_to_snake_case(df)
   self.logger.debug(f"Loaded diagnosis data with {df.height} rows and {df.width} columns")
   return df
  except Exception as e:
   self.logger.error(f"Error loading diagnosis data: {str(e)}", exc_info=True)
   raise
 def _sanitize_dataframe(self, df: pl.DataFrame)->Dict[str, Any]:
        """
        Sanitize DataFrame by removing special characters and unwanted spaces from string columns.

        Args:
            df: DataFrame to sanitize

        Returns:
            Dictionary with sanitization statistics
        """
  if df is None:
   return {"error": "No data to sanitize"}
  sanitize_stats={
   "original_rows": df.height,
   "columns_sanitized": [],
   "cells_modified": 0,
   "sanitized_rows": 0
  }
  self.logger.info(f"Sanitizing DataFrame with {df.height} rows and {df.width} columns")
  try:
   string_columns=[col for col in df.columns if df.schema[col]==pl.Utf8]
   sanitize_stats["total_string_columns"]=len(string_columns)
   if not string_columns:
    self.logger.info("No string columns to sanitize")
    return sanitize_stats
   modified_rows=set()
   new_df_columns={}
   for col in df.columns:
    if col in string_columns:
     original_values=df[col].to_list()
     sanitized_values=[]
     was_modified=False
     modified_cells_count=0
     for row_idx, value in enumerate(original_values):
      sanitized_value=self._sanitize_string_value(col, value)
      sanitized_values.append(sanitized_value)
      if value!=sanitized_value:
       modified_cells_count+=1
       modified_rows.add(row_idx)
       was_modified=True
     if was_modified:
      sanitize_stats["columns_sanitized"].append(col)
      sanitize_stats["cells_modified"]+=modified_cells_count
      self.logger.debug(f"Sanitized column '{col}': {modified_cells_count} cells modified")
      examples=[]
      for i, (orig, sanitized) in enumerate(zip(original_values, sanitized_values)):
       if orig!=sanitized and len(examples)<5:
        examples.append(f"Row {i}: '{orig}'->'{sanitized}'")
      if examples:
       self.logger.debug("Examples of modifications: "+", ".join(examples))
     new_df_columns[col]=pl.Series(name=col, values=sanitized_values)
    else:
     new_df_columns[col]=df[col]
   sanitized_df=pl.DataFrame(new_df_columns)
   sanitize_stats["sanitized_rows"]=len(modified_rows)
   self.logger.info(f"Sanitization complete: {len(sanitize_stats['columns_sanitized'])}/{sanitize_stats['total_string_columns']} columns modified, "
       f"{sanitize_stats['cells_modified']} cells, {sanitize_stats['sanitized_rows']} rows")
   return sanitize_stats, sanitized_df
  except Exception as e:
   self.logger.error(f"Error during DataFrame sanitization: {str(e)}", exc_info=True)
   return {"error": str(e), "exception": str(e)}, df
 def _sanitize_string_value(self, column_name, value):
        """
        Sanitize a string value based on the column type.

        Args:
            column_name: Name of the column (used to determine sanitization rules)
            value: The string value to sanitize

        Returns:
            Sanitized string value
        """
  if value is None or not isinstance(value, str):
   return value
  original=value
  value=value.strip()
  value=re.sub(r'\s+', ' ', value)
  value=re.sub(r'[\x00-\x1F\x7F]', '', value)
  if column_name.lower() in ('registry_id', 'patient_id', 'id'):
   value=re.sub(r'[^\w\-\.]', '', value)
  elif 'name' in column_name.lower():
   value=re.sub(r'[^\w\s\-\']', '', value)
  elif 'diagnosis' in column_name.lower():
   value=re.sub(r'[^\w\s\-\.,:/()]', '', value)
  else:
   value=re.sub(r'[^\w\s\-\.,:/()]', '', value)
  return value
 def _convert_column_names_to_snake_case(self, df):
        """
        Convert column names to snake_case and validate no duplicates are created.
        Handles decimal numbers in column names and removes leading numbers.

        Args:
            df: Polars DataFrame with original column names

        Returns:
            DataFrame with sanitized snake_case column names
        """
  column_mapping={}
  for col in df.columns:
   clean_col=''.join(c if c.isalnum() or c.isspace() or c=='.' else ' ' for c in col)
   snake_col=clean_col.replace(' ', '_').replace('-', '_')
   snake_col=re.sub(r'(?<=[a-z0-9])(?=[A-Z])', '_', snake_col)
   snake_col=snake_col.lower().replace('__', '_').strip('_')
   snake_col=re.sub(r'^(\d+(\.\d+)+)[\._]*', '', snake_col)
   snake_col=re.sub(r'^[0-9]+[\._]*', '', snake_col)
   if not snake_col or snake_col.strip('_')=='':
    snake_col=f"column_{df.columns.index(col)}"
   column_mapping[col]=snake_col
  snake_case_names=list(column_mapping.values())
  duplicate_names=set([name for name in snake_case_names if snake_case_names.count(name)>1])
  if duplicate_names:
   self.logger.warning(f"Found duplicate column names after snake_case conversion: {duplicate_names}")
   processed_names={}
   for original_col, snake_col in list(column_mapping.items()):
    if snake_col in duplicate_names:
     if snake_col in processed_names:
      processed_names[snake_col]+=1
      unique_name=f"{snake_col}_{processed_names[snake_col]}"
      column_mapping[original_col]=unique_name
      self.logger.info(f"Renamed duplicate column '{original_col}' to '{unique_name}'")
     else:
      processed_names[snake_col]=1
  if len(set(column_mapping.values()))!=len(column_mapping):
   remaining_duplicates=[name for name, count in
        {name: list(column_mapping.values()).count(name) for name in set(column_mapping.values())}.items()
        if count>1]
   raise ValueError(f"Failed to resolve duplicate column names: {remaining_duplicates}. Review your data schema.")
  for original, converted in column_mapping.items():
   if original!=converted:
    self.logger.debug(f"Column renamed: '{original}'->'{converted}'")
  return df.rename(column_mapping)
 def _preprocess_patient_data(self, df):
        """
        Minimal preprocessing for patient data, keeping data as-is.

        Args:
            df: Patient data DataFrame

        Returns:
            Original DataFrame with minimal changes
        """
  df=df.fill_null(None)
  self.logger.info("Minimal patient data preprocessing complete-keeping data as-is")
  return df
 def _preprocess_diagnosis_data(self, df):
        """
        Minimal preprocessing for diagnosis data, keeping data as-is.

        Args:
            df: Diagnosis data DataFrame

        Returns:
            Original DataFrame with minimal changes
        """
  df=df.fill_null(None)
  self.logger.info("Minimal diagnosis data preprocessing complete-keeping data as-is")
  return df
 def _validate_data_integrity(self)->Dict[str, Any]:
        """
        Perform data validation checks to ensure data integrity.
        All comparisons are performed as strings to avoid type mismatches.

        Returns:
            Dictionary with validation results
        """
  validation_results={}
  if self.patient_data is None or self.diagnosis_data is None:
   self.logger.error("Cannot validate data integrity-data not loaded")
   return {"error": "Data not loaded"}
  if 'registry_id' in self.patient_data.columns:
   missing_ids=self.patient_data.filter(
    pl.col('registry_id').is_null() |
    (pl.col('registry_id').cast(pl.Utf8)=="") |
    (pl.col('registry_id').cast(pl.Utf8)=="null") |
    (pl.col('registry_id').cast(pl.Utf8)=="nan")
   ).height
   validation_results['missing_patient_ids']=missing_ids
   if missing_ids>0:
    self.logger.warning(f"Found {missing_ids} patients without registry_id")
  if ('registry_id' in self.patient_data.columns and
   'registry_id' in self.diagnosis_data.columns):
   try:
    patient_id_series=self.patient_data.select(
     pl.col('registry_id').cast(pl.Utf8).alias('registry_id_str')
    ).to_series()
    patient_ids=set()
    for id_val in patient_id_series:
     if id_val is not None and str(id_val).strip() not in ("", "null", "nan"):
      patient_ids.add(str(id_val).strip())
    self.logger.debug(f"Found {len(patient_ids)} unique patient IDs for validation")
    orphaned_diagnoses=self.diagnosis_data.filter(
     ~pl.col('registry_id').cast(pl.Utf8).is_in(list(patient_ids))
    ).height
    validation_results['orphaned_diagnoses']=orphaned_diagnoses
    if orphaned_diagnoses>0:
     self.logger.warning(f"Found {orphaned_diagnoses} diagnoses without a matching patient")
     sample_orphaned=self.diagnosis_data.filter(
      ~pl.col('registry_id').cast(pl.Utf8).is_in(list(patient_ids))
     ).head(5)
     for i, row in enumerate(sample_orphaned.to_dicts()):
      self.logger.debug(f"Orphaned diagnosis #{i+1}: registry_id={row['registry_id']} "
          f"(type: {type(row['registry_id']).__name__})")
   except Exception as e:
    self.logger.error(f"Error validating orphaned diagnoses: {str(e)}", exc_info=True)
    validation_results['orphaned_diagnoses_error']=str(e)
  if 'registry_id' in self.patient_data.columns:
   try:
    duplicate_ids=(
     self.patient_data.with_column(
      pl.col('registry_id').cast(pl.Utf8).alias('registry_id_str')
     )
     .group_by('registry_id_str')
     .agg(pl.count().alias('count'))
     .filter(pl.col('count')>1)
    )
    duplicate_count=duplicate_ids.height
    validation_results['duplicate_patient_ids']=duplicate_count
    if duplicate_count>0:
     self.logger.warning(f"Found {duplicate_count} duplicate patient registry_ids")
     for row in duplicate_ids.to_dicts():
      self.logger.debug(f"Duplicate patient ID: {row['registry_id_str']} appears {row['count']} times")
   except Exception as e:
    self.logger.error(f"Error checking for duplicate patient IDs: {str(e)}", exc_info=True)
    validation_results['duplicate_check_error']=str(e)
  validation_results['type_validation']=self._validate_data_types()
  self.stats['data_quality']=validation_results
  return validation_results
 def _validate_data_types(self)->Dict[str, Any]:
        """
        Validate data types in both patient and diagnosis data.

        Returns:
            Dictionary with type validation results
        """
  validation_results={
   'patient_data': {},
   'diagnosis_data': {}
  }
  if self.patient_data is not None:
   patient_issues={}
   if 'age' in self.patient_data.columns:
    try:
     non_numeric_ages=self.patient_data.filter(
      pl.col('age').is_not_null() &
      ~pl.col('age').cast(pl.Utf8).str.strip().cast(pl.Float64, strict=False).is_not_null()
     ).height
     if non_numeric_ages>0:
      patient_issues['non_numeric_ages']=non_numeric_ages
      self.logger.warning(f"Found {non_numeric_ages} patient records with non-numeric age values")
    except Exception as e:
     self.logger.error(f"Error validating age data types: {str(e)}")
     patient_issues['age_validation_error']=str(e)
   if 'gender' in self.patient_data.columns:
    try:
     gender_values=self.patient_data.select(
      pl.col('gender').cast(pl.Utf8).alias('gender_str')
     ).unique().to_series().to_list()
     gender_values=[g for g in gender_values if g is not None]
     standard_genders={'male', 'female', 'm', 'f', 'man', 'woman'}
     non_standard=[g for g in gender_values if g.lower() not in standard_genders]
     if non_standard:
      patient_issues['non_standard_genders']=non_standard
      self.logger.info(f"Found non-standard gender values: {non_standard}")
    except Exception as e:
     self.logger.error(f"Error validating gender values: {str(e)}")
     patient_issues['gender_validation_error']=str(e)
   validation_results['patient_data']=patient_issues
  if self.diagnosis_data is not None:
   diagnosis_issues={}
   date_columns=[col for col in self.diagnosis_data.columns if 'date' in col.lower()]
   for date_col in date_columns:
    try:
     non_date_count=self.diagnosis_data.filter(
      pl.col(date_col).is_not_null() &
      ~pl.col(date_col).is_datelike()
     ).height
     if non_date_count>0:
      diagnosis_issues[f'non_date_{date_col}']=non_date_count
      self.logger.warning(f"Found {non_date_count} records with non-date values in {date_col}")
    except Exception as e:
     self.logger.error(f"Error validating date column {date_col}: {str(e)}")
     diagnosis_issues[f'{date_col}_validation_error']=str(e)
   validation_results['diagnosis_data']=diagnosis_issues
  return validation_results
 def ingest_to_database(self)->Dict[str, Any]:
        """
        Ingest processed data into the database with enhanced validation.

        Returns:
            Dictionary with ingestion results including validation statistics
        """
  if self.patient_data is None or self.diagnosis_data is None:
   raise ValueError("No data available for ingestion-load data first")
  try:
   self.logger.info("Starting database ingestion process with validation")
   self.logger.debug(f"Connection details: host={AppConfig.DB_HOST}, port={AppConfig.DB_PORT}, db={AppConfig.DB_NAME}")
   self.logger.debug(f"Patient data shape: {self.patient_data.shape}, columns: {self.patient_data.columns}")
   self.logger.debug(f"Diagnosis data shape: {self.diagnosis_data.shape}, columns: {self.diagnosis_data.columns}")
   conn=get_db_connection()
   try:
    self.logger.info("Creating database tables if they don't exist")
    create_tables(conn, self.patient_data, self.diagnosis_data, drop_if_exists=getattr(self, 'recreate_db_schema', False))
    self.logger.info(f"Validating and inserting {self.patient_data.height} patient records")
    patient_stats=insert_data(conn, "patient_details", self.patient_data)
    self.logger.info(f"Validating and inserting {self.diagnosis_data.height} diagnosis records")
    diagnosis_stats=insert_data(conn, "diagnosis_details", self.diagnosis_data)
    conn.commit()
    self.logger.info("Transaction committed successfully")
    success_rate_patients=(patient_stats["valid_records"]/patient_stats["total_records"])*100 if patient_stats["total_records"]>0 else 0
    success_rate_diagnosis=(diagnosis_stats["valid_records"]/diagnosis_stats["total_records"])*100 if diagnosis_stats["total_records"]>0 else 0
    self.logger.info(f"Patient data: {patient_stats['valid_records']} inserted, {patient_stats['rejected_records']} rejected ({success_rate_patients:.1f}% success)")
    self.logger.info(f"Diagnosis data: {diagnosis_stats['valid_records']} inserted, {diagnosis_stats['rejected_records']} rejected ({success_rate_diagnosis:.1f}% success)")
    if patient_stats["rejected_records"]>0:
     self.logger.warning("Main reasons for patient data rejection:")
     for reason, count in sorted(patient_stats.get("error_reasons", {}).items(), key=lambda x: x[1], reverse=True)[:3]:
      self.logger.warning(f"-{reason}: {count} records")
    if diagnosis_stats["rejected_records"]>0:
     self.logger.warning("Main reasons for diagnosis data rejection:")
     for reason, count in sorted(diagnosis_stats.get("error_reasons", {}).items(), key=lambda x: x[1], reverse=True)[:3]:
      self.logger.warning(f"-{reason}: {count} records")
    result={
     "status": "success",
     "patient_records": {
      "total": patient_stats["total_records"],
      "inserted": patient_stats["valid_records"],
      "rejected": patient_stats["rejected_records"],
      "success_rate": f"{success_rate_patients:.1f}%"
     },
     "diagnosis_records": {
      "total": diagnosis_stats["total_records"],
      "inserted": diagnosis_stats["valid_records"],
      "rejected": diagnosis_stats["rejected_records"],
      "success_rate": f"{success_rate_diagnosis:.1f}%"
     },
     "validation_details": {
      "patient_errors": patient_stats.get("error_reasons", {}),
      "diagnosis_errors": diagnosis_stats.get("error_reasons", {})
     },
     "timestamp": datetime.now().isoformat(),
     "schema_recreated": getattr(self, 'recreate_db_schema', False)
    }
    self.stats["db_ingestion"]=result
    self.logger.info(f"Database ingestion complete with validation")
    return result
   except Exception as e:
    conn.rollback()
    self.logger.error(f"Database ingestion failed: {str(e)}", exc_info=True)
    raise
   finally:
    conn.close()
    self.logger.debug("Database connection closed")
  except Exception as e:
   error_msg=f"Failed to ingest data to database: {str(e)}"
   self.logger.error(error_msg, exc_info=True)
   return {
    "status": "error",
    "message": error_msg,
    "timestamp": datetime.now().isoformat()
   }
 def save_processed_data(self)->Dict[str, str]:
        """
        Save processed data to files or S3.

        Returns:
            Dictionary with file paths
        """
  if self.patient_data is None or self.diagnosis_data is None:
   raise ValueError("No processed data available to save")
  try:
   if not AppConfig.USE_S3:
    os.makedirs(os.path.join(AppConfig.DATA_DIR, 'processed'), exist_ok=True)
   if AppConfig.USE_S3:
    self.logger.info(f"Saving processed data to S3 bucket {AppConfig.S3_BUCKET}")
    patient_data_path=upload_to_s3(
     self.patient_data,
     AppConfig.S3_BUCKET,
     'processed/patient_data.csv'
    )
    diagnosis_data_path=upload_to_s3(
     self.diagnosis_data,
     AppConfig.S3_BUCKET,
     'processed/diagnosis_data.csv'
    )
   else:
    patient_data_path=os.path.join(AppConfig.DATA_DIR, 'processed', 'patient_data.csv')
    diagnosis_data_path=os.path.join(AppConfig.DATA_DIR, 'processed', 'diagnosis_data.csv')
    self.logger.info(f"Saving processed data to local files: {patient_data_path}, {diagnosis_data_path}")
    self.patient_data.write_csv(patient_data_path)
    self.diagnosis_data.write_csv(diagnosis_data_path)
   self.logger.info("Data saved successfully")
   return {
    'patient_data': patient_data_path,
    'diagnosis_data': diagnosis_data_path
   }
  except Exception as e:
   self.logger.error(f"Failed to save processed data: {str(e)}", exc_info=True)
   raise
 def get_data_stats(self)->Dict[str, Any]:
        """
        Get comprehensive statistics about the loaded datasets.

        Returns:
            Dictionary with dataset statistics
        """
  if self.patient_data is None or self.diagnosis_data is None:
   return {"error": "No data loaded"}
  try:
   patient_stats={
    "record_count": self.patient_data.height,
    "column_count": self.patient_data.width,
    "columns": self.patient_data.columns,
   }
   if 'age' in self.patient_data.columns:
    try:
     age_stats=self.patient_data.select([
      pl.col('age').mean().alias('avg_age'),
      pl.col('age').min().alias('min_age'),
      pl.col('age').max().alias('max_age'),
      pl.col('age').median().alias('median_age')
     ]).to_dicts()[0]
     patient_stats['age_stats']=age_stats
    except:
     pass
   if 'stay_duration' in self.patient_data.columns:
    try:
     stay_stats=self.patient_data.select([
      pl.col('stay_duration').mean().alias('avg_stay'),
      pl.col('stay_duration').min().alias('min_stay'),
      pl.col('stay_duration').max().alias('max_stay'),
      pl.col('stay_duration').median().alias('median_stay')
     ]).to_dicts()[0]
     patient_stats['stay_stats']=stay_stats
    except:
     pass
   if 'gender' in self.patient_data.columns:
    try:
     gender_counts=(
      self.patient_data
      .group_by('gender')
      .agg(pl.count().alias('count'))
      .sort('count', descending=True)
      .to_dicts()
     )
     patient_stats['gender_distribution']=gender_counts
    except:
     pass
   diagnosis_stats={
    "record_count": self.diagnosis_data.height,
    "column_count": self.diagnosis_data.width,
    "columns": self.diagnosis_data.columns,
   }
   if 'diagnosis' in self.diagnosis_data.columns:
    try:
     top_diagnoses=(
      self.diagnosis_data
      .group_by('diagnosis')
      .agg(pl.count().alias('count'))
      .sort('count', descending=True)
      .head(10)
      .to_dicts()
     )
     diagnosis_stats['top_diagnoses']=top_diagnoses
    except:
     pass
   if 'registry_id' in self.diagnosis_data.columns:
    try:
     diagnoses_per_patient=(
      self.diagnosis_data
      .group_by('registry_id')
      .agg(pl.count().alias('diagnosis_count'))
     )
     diag_stats=diagnoses_per_patient.select([
      pl.col('diagnosis_count').mean().alias('avg_diagnoses_per_patient'),
      pl.col('diagnosis_count').min().alias('min_diagnoses_per_patient'),
      pl.col('diagnosis_count').max().alias('max_diagnoses_per_patient'),
      pl.col('diagnosis_count').median().alias('median_diagnoses_per_patient')
     ]).to_dicts()[0]
     diagnosis_stats['diagnoses_per_patient']=diag_stats
    except:
     pass
   sanitization_stats=self.stats.get("sanitization_stats", {})
   stats={
    "patient_data": patient_stats,
    "diagnosis_data": diagnosis_stats,
    "processing_stats": {
     "load_timestamp": self.stats.get("load_timestamp"),
     "processing_time_sec": self.stats.get("processing_time_sec"),
    },
    "data_quality": self.stats.get("data_quality", {}),
    "sanitization": {
     "patient_data": sanitization_stats.get("patient_data", {}),
     "diagnosis_data": sanitization_stats.get("diagnosis_data", {}),
     "summary": {
      "total_columns_sanitized": len(sanitization_stats.get("patient_data", {}).get("columns_sanitized", []))+
             len(sanitization_stats.get("diagnosis_data", {}).get("columns_sanitized", [])),
      "total_cells_modified": sanitization_stats.get("patient_data", {}).get("cells_modified", 0)+
              sanitization_stats.get("diagnosis_data", {}).get("cells_modified", 0),
      "total_rows_affected": sanitization_stats.get("patient_data", {}).get("sanitized_rows", 0)+
             sanitization_stats.get("diagnosis_data", {}).get("sanitized_rows", 0)
     }
    }
   }
   return stats
  except Exception as e:
   self.logger.error(f"Error generating data statistics: {str(e)}", exc_info=True)
   return {
    "error": f"Failed to generate statistics: {str(e)}",
    "patient_record_count": self.patient_data.height if self.patient_data is not None else 0,
    "diagnosis_record_count": self.diagnosis_data.height if self.diagnosis_data is not None else 0
   }

=== hospital-data-chatbot/app/core/db_importer.py (python) ===
from app.utils.db import get_db_connection, create_tables, insert_patient_data, insert_metadata
class DbImporter:
    """Handles importing data to Aurora PostgreSQL."""

    def __init__(self):
        self.conn = get_db_connection()

    def setup_database(self):
        """Set up database tables."""
  create_tables(self.conn)
 def import_data(self, patient_data, metadata):
        """Import both patient data and metadata into the database."""
        # Create tables if they don't exist
        self.setup_database()

        # Import data
        patient_count = insert_patient_data(self.conn, patient_data)
        metadata_count = insert_metadata(self.conn, metadata)

        return {
            'patient_records': patient_count,
            'metadata_records': metadata_count
        }

    def close(self):
        """Close the database connection."""
  if self.conn:
   self.conn.close()

=== hospital-data-chatbot/app/core/ollama_connector.py (python) ===
import requests
import json
import re
from app.config.settings import AppConfig
from app.utils.logging import get_logger
class OllamaLLM:
    """Interface for Ollama LLM services."""

    def __init__(self):
        self.logger = get_logger(__name__)
        self.model = AppConfig.OLLAMA_MODEL
        self.base_url = AppConfig.OLLAMA_HOST
        self.logger.info(f"Initialized Ollama connector with model: {self.model} at {self.base_url}")

    def query(self, user_query, context, max_tokens=1000):
        """
  Send a query to the LLM with context information.
  Args:
   user_query: User's question
   context: Context information (schema, etc.)
   max_tokens: Maximum tokens to generate
  Returns:
   LLM response text
        """
        prompt = self._format_prompt(user_query, context)

        try:
            # Format the request for Ollama API
            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": False,  # Disable streaming to get a single response
                "options": {
                    "temperature": 0.1,
                    "top_p": 0.95,
                    "num_predict": max_tokens
                }
            }

            self.logger.debug(f"Sending request to Ollama")

            # Send request to Ollama
            response = requests.post(
                f"{self.base_url}/api/generate",
                json=payload,
                headers={"Content-Type": "application/json"}
            )

            # Check if request was successful
            response.raise_for_status()

            # Get the raw response for debugging
            raw_response = response.text
            self.logger.debug(f"Raw response from Ollama: {raw_response[:100]}...")

            # Parse the JSON response
            try:
                result = json.loads(raw_response)
                full_response = result.get("response", "")

                # Extract SQL and reasoning using regex
                reasoning_match = re.search(r'REASONING:\s*(.*?)(?=\s*SQL:|$)', full_response, re.DOTALL)
                sql_match = re.search(r'SQL:\s*(.*?)(?=$)', full_response, re.DOTALL)

                reasoning = reasoning_match.group(1).strip() if reasoning_match else ""
                sql = sql_match.group(1).strip() if sql_match else ""

                self.logger.debug(f"Extracted SQL: {sql[:100]}...")

                # Return the full response for now - the SQL query engine will parse out the SQL portion
                return full_response

            except json.JSONDecodeError as e:
                self.logger.error(f"JSON parsing error: {str(e)}")
                # If JSON parsing fails, just return the raw response
                return raw_response

        except Exception as e:
            self.logger.error(f"Error querying Ollama: {str(e)}")
            return f"Sorry, I encountered an error processing your request: {str(e)}"

    def _format_prompt(self, query, context):
        """Format the prompt for the text-to-SQL task."""
        return f"""
        You are an SQL developer specialized in helping to translate natural language queries about hospital patient data into SQL queries.

        {context}

        User Query: {query}

        Your task:
        1. Analyze the request and determine what SQL is needed
        2. Write a PostgreSQL query that answers the user's question
        3. Explain your reasoning step by step
        4. Ensure the query is secure and efficient

        Format your response as follows:

        REASONING:
        [Step by step explanation of how you're interpreting the query and constructing the SQL]

        SQL:
        [Your PostgreSQL query - only include the actual SQL code here]
        """

=== hospital-data-chatbot/app/core/llm_connector.py (python) ===
import boto3
import json
from app.config.settings import AppConfig
class BedrockLLM:
    """Interface for AWS Bedrock LLM services."""

    def __init__(self):
        self.model_id = AppConfig.BEDROCK_MODEL_ID
        self.client = boto3.client('bedrock-runtime')

    def query(self, user_query, context, max_tokens=1000):
        """
  Send a query to the LLM with context information.
        """
        prompt = self._format_prompt(user_query, context)

        # For Claude models in Bedrock
        body = json.dumps({
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": max_tokens,
            "messages": [
                {"role": "user", "content": prompt}
            ]
        })

        try:
            response = self.client.invoke_model(
                modelId=self.model_id,
                body=body
            )

            response_body = json.loads(response.get('body').read())
            return response_body['content'][0]['text']

        except Exception as e:
            print(f"Error querying LLM: {str(e)}")
            return f"Sorry, I encountered an error processing your request: {str(e)}"

    def _format_prompt(self, query, context):
        """Format the prompt for the LLM."""
        return f"""
        You are a helpful assistant specialized in analyzing hospital patient data.

        Context information about the hospital patient records:
        {context}

        User query: {query}

        Please analyze the data and provide a clear, concise, and accurate answer.
        If calculations are involved, show your reasoning step by step.
        If the data doesn't contain information to answer the query, please state that clearly.
        """

=== hospital-data-chatbot/app/core/__init__.py (python) ===


=== hospital-data-chatbot/app/core/sql_query_engine.py (python) ===
import re
from typing import Dict, List, Any, Tuple, Optional
import psycopg2
import psycopg2.extras
import polars as pl
from io import StringIO
from app.core.llm_connector import BedrockLLM
from app.utils.db import get_db_connection
from app.utils.logging import get_logger
from app.config.settings import AppConfig
import logging
logger=logging.getLogger(__name__)
class SQLQueryEngine:
    """
    Processes natural language queries by converting them to SQL using LLM,
    executing the SQL against the database, and formatting the results.

    Flow:
    User query -> LLM generates SQL -> SQL executed on DB -> Results returned to LLM -> Response formatted
    """
 def __init__(self):
        """Initialize the SQL Query Engine with LLM connector."""
        self.logger = get_logger(__name__)

        # Choose LLM implementation based on config
        if AppConfig.USE_OLLAMA:
            from app.core.ollama_connector import OllamaLLM
            self.llm = OllamaLLM()
            self.logger.info("Using Ollama for LLM processing")
        else:
            from app.core.llm_connector import BedrockLLM
            self.llm = BedrockLLM()
            self.logger.info("Using AWS Bedrock for LLM processing")

        self.db_schema = None

        # Load DB schema on initialization
        self._load_db_schema()

    def process_query(self, user_query: str) -> Dict[str, Any]:
        """
  Process a natural language query by:
  1. Converting it to SQL using LLM
  2. Validating the SQL for safety
  3. Executing the SQL against the database
  4. Formatting the results with LLM to include both SQL and results
  Args:
   user_query: Natural language query from the user
  Returns:
   Dictionary containing the response and metadata
        """
        self.logger.info(f"Processing natural language query: {user_query}")

        try:
            # Step 1: Convert natural language to SQL
            generated_sql, reasoning = self._generate_sql_from_query(user_query)

            if not generated_sql:
                return {
                    "response": "I couldn't generate a valid SQL query for your question. Could you rephrase it?",
                    "success": False,
                    "sql_generated": False
                }

            # Step 2: Validate SQL for safety
            is_safe, safety_message = self._validate_sql_safety(generated_sql)
            if not is_safe:
                self.logger.warning(f"Unsafe SQL query generated: {generated_sql}")
                return {
                    "response": f"I couldn't create a safe query for your question. {safety_message}",
                    "success": False,
                    "sql_generated": True,
                    "sql": generated_sql,
                    "reasoning": reasoning
                }

            # Step 3: Execute SQL query
            result_data, column_names, error = self._execute_sql_query(generated_sql)

            if error:
                self.logger.error(f"SQL execution error: {error}")

                # Try to get LLM to fix the query
                fixed_sql, fix_reasoning = self._fix_sql_query(generated_sql, error, user_query)

                if fixed_sql and fixed_sql != generated_sql:
                    self.logger.info(f"Attempting to execute fixed SQL: {fixed_sql}")
                    result_data, column_names, error = self._execute_sql_query(fixed_sql)

                    if not error:
                        generated_sql = fixed_sql

                # If we still have an error after attempting to fix
                if error:
                    return {
                        "response": f"I generated a SQL query but encountered an error when executing it: {error}",
                        "success": False,
                        "sql_generated": True,
                        "sql": generated_sql,
                        "error": error
                    }

            # Step 4: Format the results using LLM, including SQL in the response
            if result_data:
                formatted_response = self._format_sql_results_with_query(
                    user_query, generated_sql, result_data, column_names
                )
            else:
                formatted_response = f"The query executed successfully but didn't return any results.\n\nSQL Query Used:\n```sql\n{generated_sql}\n```"

            # Prepare success response
            return {
                "response": formatted_response,
                "success": True,
                "sql_generated": True,
                "sql": generated_sql,
                "row_count": len(result_data) if result_data else 0,
                "column_names": column_names,
                "data": result_data
            }

        except Exception as e:
            self.logger.error(f"Error in SQL query engine: {str(e)}", exc_info=True)
            return {
                "response": f"I encountered an error while processing your query: {str(e)}",
                "success": False,
                "error": str(e)
            }

    def _load_db_schema(self) -> None:
        """
  Load database schema information to help with SQL generation.
        """
        try:
            self.logger.info("Loading database schema information")
            conn = get_db_connection()

            try:
                with conn.cursor() as cursor:
                    # Get list of tables
                    cursor.execute("""
      SELECT table_name
      FROM information_schema.tables
      WHERE table_schema='public'
      AND table_type='BASE TABLE'
                    """)
                    tables = [row[0] for row in cursor.fetchall()]

                    # Get table structures
                    schema_info = {}
                    for table in tables:
                        cursor.execute(f"""
       SELECT column_name, data_type, is_nullable
       FROM information_schema.columns
       WHERE table_schema='public' AND table_name=%s
       ORDER BY ordinal_position
                        """, (table,))

                        columns = []
                        for row in cursor.fetchall():
                            columns.append({
                                "name": row[0],
                                "type": row[1],
                                "nullable": row[2]
                            })

                        schema_info[table] = columns

                    # Get foreign key relationships
                    cursor.execute("""
      SELECT
       tc.table_name AS table_name,
       kcu.column_name AS column_name,
       ccu.table_name AS foreign_table_name,
       ccu.column_name AS foreign_column_name
      FROM
       information_schema.table_constraints AS tc
       JOIN information_schema.key_column_usage AS kcu
         ON tc.constraint_name=kcu.constraint_name
         AND tc.table_schema=kcu.table_schema
       JOIN information_schema.constraint_column_usage AS ccu
         ON ccu.constraint_name=tc.constraint_name
         AND ccu.table_schema=tc.table_schema
      WHERE tc.constraint_type='FOREIGN KEY'
      AND tc.table_schema='public'
                    """)

                    relationships = []
                    for row in cursor.fetchall():
                        relationships.append({
                            "table": row[0],
                            "column": row[1],
                            "foreign_table": row[2],
                            "foreign_column": row[3]
                        })

                    # Store the complete schema information
                    self.db_schema = {
                        "tables": schema_info,
                        "relationships": relationships
                    }

                    self.logger.info(f"Loaded schema information for {len(tables)} tables")

            finally:
                conn.close()

        except Exception as e:
            self.logger.error(f"Error loading database schema: {str(e)}", exc_info=True)
            self.db_schema = None

    def _generate_sql_prompt(self, user_query: str) -> str:
        """
  Generate a prompt for the LLM to convert natural language to SQL.
  Args:
   user_query: The natural language query from the user
  Returns:
   Formatted prompt for LLM
        """
        # Build schema information string
        schema_info = ""
        if self.db_schema:
            schema_info = "Database Schema:\n"

            # Add table information
            for table_name, columns in self.db_schema["tables"].items():
                schema_info += f"\nTABLE: {table_name}\n"
                schema_info += "Columns:\n"

                for col in columns:
                    nullable = "NULL" if col["nullable"] == "YES" else "NOT NULL"
                    schema_info += f"- {col['name']} ({col['type']}, {nullable})\n"

            # Add relationships
            if self.db_schema["relationships"]:
                schema_info += "\nRelationships:\n"
                for rel in self.db_schema["relationships"]:
                    schema_info += f"- {rel['table']}.{rel['column']} references {rel['foreign_table']}.{rel['foreign_column']}\n"

        # Build the prompt with more guidance on data types
        prompt = f"""
  You are an expert SQL developer helping to translate natural language queries about hospital patient data into SQL queries.
  {schema_info}
  IMPORTANT DATA TYPE HANDLING:
  -All columns are stored as TEXT in the database, even if they contain numeric values
  -When comparing numeric values, always use CAST or :: to convert text to numbers
  -For reliable conversion, use: CAST(NULLIF(column_name, '') AS NUMERIC)
  -This approach handles NULL values and empty strings properly
  The most important tables are:
  1. patient_details-Contains patient records with columns like registry_id, age, gender
  2. diagnosis_details-Contains diagnosis information related to patients, linked by registry_id
  User Query: {user_query}
  Your task:
  1. Analyze the request and determine what SQL is needed
  2. Write a PostgreSQL query that answers the user's question
  3. Ensure proper type conversion for any numeric comparisons
  4. Explain your reasoning step by step
  5. Ensure the query is secure and efficient
  Format your response as follows:
  REASONING:
  [Step by step explanation of how you're interpreting the query and constructing the SQL]
  SQL:
  [Your PostgreSQL query-only include the actual SQL code here]
        """

        return prompt

    def _generate_sql_from_query(self, user_query: str) -> Tuple[str, str]:
        """
  Use LLM to generate SQL from natural language query without extensive reasoning.
  Args:
   user_query: The natural language query from the user
  Returns:
   Tuple of (generated_sql, empty_reasoning)
        """
        prompt = f"""
  You are an expert SQL developer helping to translate natural language queries about hospital patient data into SQL queries.
  The database contains patient data with tables including:
  1. patient_details-Contains patient records with columns like registry_id, age, gender
  2. diagnosis_details-Contains diagnosis information related to patients, linked by registry_id
  IMPORTANT: When working with numeric columns (like age), use CAST(NULLIF(age, '') AS NUMERIC) for comparisons.
  User Query: {user_query}
  Please respond ONLY with the PostgreSQL query that would answer this question.
  No explanations or reasoning-just give me the SQL query.
        """

        try:
            # Get response from LLM
            response = self.llm.query(prompt, "")

            # Extract the SQL from the response
            # Look for code blocks first
            sql_match = re.search(r'```(?:sql)?\s*(.*?)\s*```', response, re.DOTALL)
            if sql_match:
                generated_sql = sql_match.group(1).strip()
            else:
                # Otherwise try to find a SELECT statement
                select_match = re.search(r'(SELECT\s+.*?;)', response, re.DOTALL, re.IGNORECASE)
                if select_match:
                    generated_sql = select_match.group(1).strip()
                else:
                    # Use the whole response if none of the above patterns match
                    generated_sql = response.strip()

            self.logger.info("SQL query generated successfully")
            self.logger.debug(f"Generated SQL: {generated_sql}")

            # Return SQL with empty reasoning
            return generated_sql, ""

        except Exception as e:
            self.logger.error(f"Error generating SQL: {str(e)}", exc_info=True)
            return "", f"Error generating SQL: {str(e)}"

    def _validate_sql_safety(self, sql_query: str) -> Tuple[bool, str]:
        """
  Validate that the SQL query is safe to execute.
  Attempt to fix common type conversion issues.
  Args:
   sql_query: The SQL query to validate
  Returns:
   Tuple of (is_safe, message)
        """
        # Keep the original query
        fixed_query = sql_query

        # Block dangerous operations
        dangerous_patterns = [
            (r'\bDROP\b', "DROP operations are not allowed"),
            (r'\bTRUNCATE\b', "TRUNCATE operations are not allowed"),
            (r'\bDELETE\b', "DELETE operations are not allowed"),
            (r'\bUPDATE\b', "UPDATE operations are not allowed"),
            (r'\bINSERT\b', "INSERT operations are not allowed"),
            (r'\bALTER\b', "ALTER operations are not allowed"),
            (r'\bCREATE\b', "CREATE operations are not allowed"),
            (r'\bGRANT\b', "GRANT operations are not allowed"),
            (r'\bREVOKE\b', "REVOKE operations are not allowed"),
            (r';.*?;', "Multiple SQL statements are not allowed"),
        ]

        # Check for dangerous patterns
        for pattern, message in dangerous_patterns:
            if re.search(pattern, sql_query, re.IGNORECASE):
                return False, message

        # Only allow SELECT statements
        if not re.match(r'^\s*SELECT', sql_query, re.IGNORECASE):
            return False, "Only SELECT statements are allowed"

        # Fix common type conversion issues directly on the input query
        # Pattern for simple comparisons like: WHERE age > 65
        numeric_comparison = re.compile(r'WHERE\s+(\w+)\s*([><=!]+)\s*(\d+(?:\.\d+)?)', re.IGNORECASE)
        sql_query = numeric_comparison.sub(
            r'WHERE CAST(NULLIF(\1, \'\') AS NUMERIC) \2 \3',
            sql_query
        )

        # Pattern for numeric IN clauses: WHERE age IN (65, 70, 75)
        in_comparison = re.compile(r'WHERE\s+(\w+)\s+IN\s*\(([\d\s,\.]+)\)', re.IGNORECASE)
        sql_query = in_comparison.sub(
            r'WHERE CAST(NULLIF(\1, \'\') AS NUMERIC) IN (\2)',
            sql_query
        )

        # Pattern for BETWEEN clauses: WHERE age BETWEEN 65 AND 75
        between_comparison = re.compile(
            r'WHERE\s+(\w+)\s+BETWEEN\s+(\d+(?:\.\d+)?)\s+AND\s+(\d+(?:\.\d+)?)',
            re.IGNORECASE
        )
        sql_query = between_comparison.sub(
            r'WHERE CAST(NULLIF(\1, \'\') AS NUMERIC) BETWEEN \2 AND \3',
            sql_query
        )

        # If the query was modified, log it
        if sql_query != fixed_query:
            self.logger.info("SQL query automatically modified for better type handling")
            self.logger.debug(f"Original: {fixed_query}\nModified: {sql_query}")

        return True, "SQL query is safe"

    def _execute_sql_query(self, sql_query: str) -> Tuple[List[Dict[str, Any]], List[str], Optional[str]]:
        """
  Execute SQL query and return results.
  Args:
   sql_query: The SQL query to execute
  Returns:
   Tuple of (result_data, column_names, error_message)
        """
        try:
            self.logger.info("Executing SQL query")
            conn = get_db_connection()

            try:
                with conn.cursor(cursor_factory=psycopg2.extras.DictCursor) as cursor:
                    cursor.execute(sql_query)
                    results = cursor.fetchall()

                    # Get column names
                    column_names = [desc[0] for desc in cursor.description] if cursor.description else []

                    # Convert to list of dicts for easier processing
                    result_data = []
                    for row in results:
                        # Convert to dict while handling potential binary/non-serializable data
                        row_dict = {}
                        for i, col_name in enumerate(column_names):
                            value = row[i]
                            # Convert non-serializable types to strings
                            if isinstance(value, (bytes, bytearray)):
                                value = f"<binary data of length {len(value)}>"
                            row_dict[col_name] = value
                        result_data.append(row_dict)

                    self.logger.info(f"SQL query executed successfully, returned {len(result_data)} rows")
                    return result_data, column_names, None

            finally:
                conn.close()

        except Exception as e:
            self.logger.error(f"Error executing SQL query: {str(e)}", exc_info=True)
            return [], [], str(e)

    def _format_sql_results(self, user_query: str, sql_query: str,
                        result_data: List[Dict[str, Any]], column_names: List[str]) -> str:
        """
  Use LLM to format SQL results into natural language response.
  Args:
   user_query: Original user query
   sql_query: The SQL query that was executed
   result_data: The query results
   column_names: Names of the columns in the results
  Returns:
   Formatted natural language response
        """
        # Limit result size to avoid LLM token limits
        max_rows = 25
        truncated = len(result_data) > max_rows

        if truncated:
            result_sample = result_data[:max_rows]
            truncation_message = f"\n\nNote: Results limited to {max_rows} rows out of {len(result_data)} total."
        else:
            result_sample = result_data
            truncation_message = ""

        # Format results as a readable table for the prompt
        result_table = "Results:\n"

        # Convert to DataFrame for easier formatting
        # Create a Polars DataFrame from the result sample
        if result_sample:
            df = pl.DataFrame(result_sample)
            if not df.is_empty():
                # Format as a readable table using Polars - Fixed line below
                # Polars uses str() or .write_csv() to a StringIO object, not to_string()
                result_table += str(df)  # This is the correct way to convert Polars DataFrame to string
                result_table += "\n\n"
                result_table += f"Total rows returned: {len(result_data)}{truncation_message}"
            else:
                result_table = "No results returned."
        else:
            result_table = "No results returned."

        # Create the prompt for result formatting
        prompt = f"""
  You are an expert data analyst helping interpret SQL query results for hospital data.
  Original User Question: {user_query}
  SQL Query Used: {sql_query}
  {result_table}
  Total rows returned: {len(result_data)}{truncation_message if truncated else ""}
  Please provide a clear, comprehensive answer to the user's original question based on these results.
  Format your response in a conversational style, but include specific figures from the data.
  If helpful, include brief statistical summaries or relevant patterns in the data.
        """

        try:
            # Get formatted response from LLM
            response = self.llm.query(prompt, "")

            self.logger.info("Results formatted successfully")
            return response

        except Exception as e:
            self.logger.error(f"Error formatting results: {str(e)}", exc_info=True)

            # Fallback to basic formatting if LLM fails
            basic_response = f"Here are the results of your query:\n\n"
            if result_data:
                basic_response += f"Found {len(result_data)} records."
                if truncated:
                    basic_response += f" Showing first {max_rows}."

                # Format a simple table using Polars if available
                try:
                    if result_sample:
                        df = pl.DataFrame(result_sample)
                        if not df.is_empty():
                            # Fixed line below - use str() instead of to_string()
                            basic_response += "\n\n" + str(df)
                        else:
                            # Manual formatting as fallback
                            basic_response += self._format_results_manually(result_sample, column_names)
                    else:
                        basic_response += "\n\nNo results to display."
                except Exception:
                    # Ultimate fallback: manual formatting
                    basic_response += self._format_results_manually(result_sample, column_names)
            else:
                basic_response += "No results found for your query."

            return basic_response

    def _format_results_manually(self, result_sample: List[Dict[str, Any]], column_names: List[str]) -> str:
        """
  Manually format results as a simple table when other methods fail.
  Args:
   result_sample: List of result dictionaries
   column_names: Names of the columns
  Returns:
   Formatted table as string
        """
        output = "\n\n"

        # Add header row
        for col in column_names:
            output += f"{col}\t"
        output += "\n"

        # Add separator
        output += "-" * (sum(len(col) + 8 for col in column_names)) + "\n"

        # Add data rows
        for row in result_sample:
            for col in column_names:
                value = row.get(col, "N/A")
                # Truncate long values
                if isinstance(value, str) and len(value) > 30:
                    value = value[:27] + "..."
                output += f"{value}\t"
            output += "\n"

        return output

    def _fix_sql_query(self, sql_query: str, error_message: str, user_query: str) -> Tuple[str, str]:
        """
  Use LLM to fix a failed SQL query based on the error message.
  Args:
   sql_query: The SQL query that failed
   error_message: The error message returned by the database
   user_query: The original user query
  Returns:
   Tuple of (fixed_sql_query, reasoning)
        """
        # Create prompt for fixing SQL with more guidance on type handling
        prompt = f"""
  You are an expert SQL developer fixing a PostgreSQL query that failed.
  Original User Query: {user_query}
  Failed SQL Query:
  ```sql
  {sql_query}
  ```
  Error Message: {error_message}
  Important notes for fixing this query:
  1. The 'age' column is stored as TEXT in the database, not as a number
  2. For numeric comparisons with text columns, use CAST or :: syntax
  3. Some values may have decimal points, so cast to FLOAT or NUMERIC first, then to INTEGER if needed
  4. For example, use: WHERE CAST(NULLIF(age, '') AS NUMERIC)>65
  5. The NULLIF function handles empty strings that can't be converted to numbers
  Please fix the SQL query to resolve the error. Only return the corrected SQL query and your reasoning.
  Format your response as follows:
  REASONING:
  [Explain what was wrong with the original query and how you fixed it]
  SQL:
  [Your fixed PostgreSQL query-only include the actual SQL code here]
        """

        try:
            # Get fixed SQL from LLM
            response = self.llm.query(prompt, "")

            # Extract SQL and reasoning from the response
            sql_match = re.search(r'SQL:\s*(.*?)(?=$|\n\n)', response, re.DOTALL)
            reasoning_match = re.search(r'REASONING:\s*(.*?)(?=$|\n\nSQL:)', response, re.DOTALL)

            fixed_sql = sql_match.group(1).strip() if sql_match else ""
            reasoning = reasoning_match.group(1).strip() if reasoning_match else ""

            # Clean up SQL (remove markdown code block markers if present)
            fixed_sql = re.sub(r'^```sql\s*|\s*```$', '', fixed_sql)

            self.logger.info("SQL query fixed successfully")
            self.logger.debug(f"Fixed SQL: {fixed_sql}")

            return fixed_sql, reasoning

        except Exception as e:
            self.logger.error(f"Error fixing SQL: {str(e)}")
            return "", f"Error fixing SQL: {str(e)}"

    def _format_sql_results_with_query(self, user_query: str, sql_query: str,
                                result_data: List[Dict[str, Any]], column_names: List[str]) -> str:
        """
  Format SQL results into a response that includes both the SQL query and results.
  Args:
   user_query: Original user query
   sql_query: The SQL query that was executed
   result_data: The query results
   column_names: Names of the columns in the results
  Returns:
   Formatted natural language response with SQL query included
        """
        # Limit result size to avoid LLM token limits
        max_rows = 25
        truncated = len(result_data) > max_rows

        if truncated:
            result_sample = result_data[:max_rows]
            truncation_message = f"\n\nNote: Results limited to {max_rows} rows out of {len(result_data)} total."
        else:
            result_sample = result_data
            truncation_message = ""

        # Format results as a readable table
        result_table = "Results:\n"

        # Format results using Polars
        if result_sample:
            try:
                df = pl.DataFrame(result_sample)
                if not df.is_empty():
                    # Use str() for Polars DataFrames
                    result_table += str(df)
                    result_table += f"\n\nTotal rows returned: {len(result_data)}{truncation_message}"
                else:
                    result_table = "No results returned."
            except Exception as e:
                self.logger.error(f"Error formatting results with Polars: {str(e)}", exc_info=True)
                # Fallback to manual formatting
                result_table += self._format_results_manually(result_sample, column_names)
        else:
            result_table = "No results returned."

        # Create the prompt for result formatting - explicitly request to include SQL
        prompt = f"""
  You are an expert SQL analyst helping interpret database query results.
  Original User Question: {user_query}
  SQL Query Used:
  ```sql
  {sql_query}
  ```
  {result_table}
  Please format your response as follows:
  1. First provide a clear, direct answer to the user's question based on the results
  2. Then include the SQL query that was used
  3. Finally provide a brief explanation of the results
  Keep your tone conversational and expressive. Make sure you explicitly include the SQL query code block in your response.
        """

        try:
            # Get formatted response from LLM
            response = self.llm.query(prompt, "")

            # If the response doesn't include the SQL query, add it
            if "```sql" not in response:
                response = f"{response}\n\nSQL Query Used:\n```sql\n{sql_query}\n```"

            self.logger.info("Results formatted successfully with SQL query included")
            return response

        except Exception as e:
            self.logger.error(f"Error formatting results: {str(e)}", exc_info=True)

            # Fallback to basic formatting if LLM fails, ensuring SQL is included
            basic_response = f"Here are the results of your query:\n\n"

            if result_data:
                basic_response += f"Found {len(result_data)} records."
                if truncated:
                    basic_response += f" Showing first {max_rows}."

                # Format a simple table
                try:
                    if result_sample:
                        df = pl.DataFrame(result_sample)
                        if not df.is_empty():
                            basic_response += "\n\n" + str(df)
                        else:
                            # Manual formatting as fallback
                            basic_response += self._format_results_manually(result_sample, column_names)
                    else:
                        basic_response += "\n\nNo results to display."
                except Exception:
                    # Ultimate fallback: manual formatting
                    basic_response += self._format_results_manually(result_sample, column_names)
            else:
                basic_response += "No results found for your query."

            # Always include the SQL query in the fallback response
            basic_response += f"\n\nSQL Query Used:\n```sql\n{sql_query}\n```"

            return basic_response

=== hospital-data-chatbot/app/config/__init__.py (python) ===


=== hospital-data-chatbot/app/config/settings.py (python) ===
import os
from typing import Dict, Any
import boto3
class AppConfig:
    """Application configuration settings."""

    # Environment selection
    ENV = os.getenv('APP_ENV', 'dev_local')  # Options: dev_local, dev-cloud, stage, prod

    # Basic settings without defaults that would override env-specific configs
    DEBUG = os.getenv('DEBUG') == 'True' if os.getenv('DEBUG') else None
    PORT = os.getenv('PORT')
    DATA_DIR = os.getenv('DATA_DIR')

    # app/config/settings.py (additions)

    # Ollama settings
    USE_OLLAMA = os.getenv('USE_OLLAMA', 'True').lower() == 'true'
    OLLAMA_MODEL = os.getenv('OLLAMA_MODEL', 'qwen2.5-coder:14b')
    OLLAMA_HOST = os.getenv('OLLAMA_HOST', 'http://localhost:11434')

    @classmethod
    def get_ssm_parameter(cls, parameter_name, default=None):
        """Fetch a parameter from AWS Systems Manager Parameter Store."""
  try:
   if cls.is_development() and not cls.USE_S3:
    return default
   ssm=boto3.client('ssm', region_name=cls.AWS_REGION)
   response=ssm.get_parameter(
    Name=parameter_name,
    WithDecryption=True
   )
   return response['Parameter']['Value']
  except Exception as e:
   print(f"Error retrieving parameter {parameter_name}: {str(e)}")
   return default
 _env_configs: Dict[str, Dict[str, Any]]={
  'dev_local': {
   'DEBUG': True,
   'PORT': '8080',
   'DATA_DIR': 'data',
   'DB_HOST': 'localhost',
   'DB_PORT': 5432,
   'DB_NAME': 'hospital_data_test',
   'DB_USER': 'postgres',
   'DB_PASSWORD': 'postgres',
   'USE_S3': False,
   'AWS_REGION': 'ap-south-1',
   'LOG_LEVEL': 'DEBUG',
   'API_KEY_REQUIRED': False
  },
  'dev-cloud': {
   'DEBUG': True,
   'PORT': '8080',
   'DATA_DIR': 'data',
   'DB_HOST': 'dev-aurora-cluster.cluster-xyz.ap-south-1.rds.amazonaws.com',
   'DB_PORT': 5432,
   'DB_NAME': 'hospital_data_dev',
   'DB_USER': 'dev_user',
   'DB_PASSWORD': os.getenv('DEV_DB_PASSWORD', 'dev_password'),
   'USE_S3': True,
   'S3_BUCKET': 'hospital-data-chatbot-dev',
   'AWS_REGION': 'ap-south-1',
   'LOG_LEVEL': 'DEBUG',
   'API_KEY_REQUIRED': True
  },
  'stage': {
   'DEBUG': False,
   'PORT': '8080',
   'DATA_DIR': 'data',
   'DB_HOST': 'stage-aurora-cluster.cluster-xyz.ap-south-1.rds.amazonaws.com',
   'DB_PORT': 5432,
   'DB_NAME': 'hospital_data_stage',
   'DB_USER': 'stage_user',
   'DB_PASSWORD': os.getenv('STAGE_DB_PASSWORD', 'stage_password'),
   'USE_S3': True,
   'S3_BUCKET': 'hospital-data-chatbot-stage',
   'AWS_REGION': 'ap-south-1',
   'LOG_LEVEL': 'INFO',
   'API_KEY_REQUIRED': True
  },
  'prod': {
   'DEBUG': False,
   'PORT': '8080',
   'DATA_DIR': 'data',
   'DB_HOST': 'prod-aurora-cluster.cluster-xyz.ap-south-1.rds.amazonaws.com',
   'DB_PORT': 5432,
   'DB_NAME': 'hospital_data_prod',
   'DB_USER': 'prod_user',
   'DB_PASSWORD': os.getenv('PROD_DB_PASSWORD', 'change_this_in_prod'),
   'USE_S3': True,
   'S3_BUCKET': 'hospital-data-chatbot-prod',
   'AWS_REGION': 'ap-south-1',
   'LOG_LEVEL': 'WARNING',
   'API_KEY_REQUIRED': True
  }
 }
 env_config=_env_configs.get(ENV, _env_configs['dev_local'])
 for key, value in env_config.items():
  locals()[key]=value
 for key in env_config.keys():
  env_value=os.getenv(key)
  if env_value is not None:
   if key=='DEBUG' or key=='USE_S3' or key=='API_KEY_REQUIRED':
    locals()[key]=env_value.lower()=='true'
   elif key=='DB_PORT':
    locals()[key]=int(env_value)
   else:
    locals()[key]=env_value
 BEDROCK_MODEL_ID=os.getenv(
  'BEDROCK_MODEL_ID',
  'anthropic.claude-3-sonnet-20240229-v1:0'
 )
 API_KEY=os.getenv('API_KEY', 'default_dev_key')  # Change in production!
 @classmethod
 def get_environment_name(cls)->str:
        """Get a human-readable name for the current environment."""
        env_names = {
            'dev_local': 'Development (Local)',
            'dev-cloud': 'Development (Cloud)',
            'stage': 'Staging',
            'prod': 'Production'
        }
        return env_names.get(cls.ENV, cls.ENV)

    @classmethod
    def is_development(cls) -> bool:
        """Check if the current environment is a development environment."""
  return cls.ENV.startswith('dev')
 @classmethod
 def is_production(cls)->bool:
        """Check if the current environment is production."""
        return cls.ENV == 'prod'

=== hospital-data-chatbot/app/utils/calculation_handler.py (python) ===
import re
import polars as pl
class CalculationHandler:
    """Handles calculations requested by the LLM."""

    def __init__(self, patient_data, diagnosis_data):
        self.datasets = {
            "patient_details": patient_data,
            "diagnosis_details": diagnosis_data
        }

    def process_response(self, llm_response):
        """
  Process the LLM response and execute any calculation requests.
  Args:
   llm_response: The response from the LLM
  Returns:
   Processed response with calculations executed
        """
        # Pattern to find calculation requests
        pattern = r'\[CALCULATE: ([^\]]+)\]'

        # Find all calculation requests
        calculation_requests = re.findall(pattern, llm_response)

        processed_response = llm_response

        # Execute each calculation
        for calc_request in calculation_requests:
            calc_result = self._execute_calculation(calc_request)

            # Replace the calculation request with the result
            processed_response = processed_response.replace(
                f"[CALCULATE: {calc_request}]",
                str(calc_result)
            )

        return processed_response

    def _execute_calculation(self, calc_request):
        """Execute a specific calculation request."""
  try:
   parts=calc_request.split('.')
   if len(parts)!=2:
    return "Error: Invalid calculation format. Use dataset.operation(column)"
   dataset_name=parts[0].strip()
   operation_part=parts[1].strip()
   if dataset_name not in self.datasets:
    return f"Error: Unknown dataset '{dataset_name}'"
   df=self.datasets[dataset_name]
   if "(" in operation_part and ")" in operation_part:
    operation=operation_part.split("(")[0].strip()
    column=operation_part.split("(")[1].split(")")[0].strip()
    if operation=="count" and column=="":
     return df.height
    if column and column not in df.columns:
     return f"Error: Column '{column}' not found in {dataset_name}"
    if operation=="mean" and column:
     result=df.select(pl.col(column).mean()).item()
     return f"{result:.2f}" if isinstance(result, float) else result
    elif operation=="min" and column:
     return df.select(pl.col(column).min()).item()
    elif operation=="max" and column:
     return df.select(pl.col(column).max()).item()
    elif operation=="sum" and column:
     result=df.select(pl.col(column).sum()).item()
     return f"{result:.2f}" if isinstance(result, float) else result
    elif operation=="median" and column:
     result=df.select(pl.col(column).median()).item()
     return f"{result:.2f}" if isinstance(result, float) else result
    elif operation=="std" and column:
     result=df.select(pl.col(column).std()).item()
     return f"{result:.2f}" if isinstance(result, float) else result
    elif operation=="count_unique" and column:
     return df.select(pl.col(column).n_unique()).item()
    elif operation=="count_by_patient" and dataset_name=="diagnosis_details":
     if "registry_id" not in df.columns:
      return "Error: registry_id column not found for patient grouping"
     result=df.group_by("registry_id").agg(pl.count().alias("diagnosis_count"))
     summary=(
      f"Min diagnoses per patient: {result.select(pl.min('diagnosis_count')).item()}, "
      f"Max: {result.select(pl.max('diagnosis_count')).item()}, "
      f"Avg: {result.select(pl.mean('diagnosis_count')).item():.2f}"
     )
     return summary
   return "Error: Unsupported operation"
  except Exception as e:
   return f"Error executing calculation: {str(e)}"

=== hospital-data-chatbot/app/utils/logging.py (python) ===
import logging
import sys
import os
from datetime import datetime
from app.config.settings import AppConfig
def setup_logging(log_level=None, log_to_file=True):
    """
    Set up application logging.

    Args:
        log_level: Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
        log_to_file: Whether to log to a file in addition to console

    Returns:
        Logger instance for the application
    """
 if log_level is None:
  log_level_str=AppConfig.LOG_LEVEL
  log_level=getattr(logging, log_level_str, logging.INFO)
 logger=logging.getLogger('hospital_chatbot')
 logger.setLevel(log_level)
 if logger.handlers:
  logger.handlers=[]
 formatter=logging.Formatter(
  '%(asctime)s-%(name)s-%(levelname)s-%(filename)s:%(lineno)d-%(message)s'
 )
 console_handler=logging.StreamHandler(sys.stdout)
 console_handler.setLevel(log_level)
 console_handler.setFormatter(formatter)
 logger.addHandler(console_handler)
 if log_to_file:
  logs_dir=os.path.join(os.getcwd(), 'logs')
  os.makedirs(logs_dir, exist_ok=True)
  timestamp=datetime.now().strftime('%Y%m%d_%H%M%S')
  env_name=AppConfig.ENV
  log_file=os.path.join(logs_dir, f'hospital_chatbot_{env_name}_{timestamp}.log')
  file_handler=logging.FileHandler(log_file)
  file_handler.setLevel(log_level)
  file_handler.setFormatter(formatter)
  logger.addHandler(file_handler)
 logger.info(f"Logging initialized for {AppConfig.get_environment_name()} environment at level {log_level_str}")
 return logger
def get_logger(name=None):
    """
    Get a logger instance.

    Args:
        name: Logger name (optional)

    Returns:
        Logger instance
    """
 if name:
  return logging.getLogger(f'hospital_chatbot.{name}')
 else:
  return logging.getLogger('hospital_chatbot')
default_logger=setup_logging()

=== hospital-data-chatbot/app/utils/db.py (python) ===
import psycopg2
import polars as pl
from psycopg2.extras import execute_values
from app.config.settings import AppConfig
from app.utils.logging import get_logger
logger=get_logger(__name__)
def _get_pg_type(polars_type):
    """Map Polars data types to PostgreSQL data types."""
    type_map = {
        pl.Int8: "TEXT",
        pl.Int16: "TEXT",
        pl.Int32: "TEXT",
        pl.Int64: "TEXT",
        pl.UInt8: "TEXT",
        pl.UInt16: "TEXT",
        pl.UInt32: "TEXT",
        pl.UInt64: "TEXT",
        pl.Float32: "TEXT",
        pl.Float64: "TEXT",
        pl.Boolean: "TEXT",
        pl.Utf8: "TEXT",
        pl.Date: "DATE",
        pl.Datetime: "TIMESTAMP",
        pl.Time: "TIME",
    }

    # Default to TEXT if type not found
    return type_map.get(polars_type, "TEXT")

def get_db_connection():
    """Get a connection to the PostgreSQL database."""
 try:
  logger.debug(f"Creating database connection to {AppConfig.DB_HOST}:{AppConfig.DB_PORT}/{AppConfig.DB_NAME}")
  conn=psycopg2.connect(
   host=AppConfig.DB_HOST,
   database=AppConfig.DB_NAME,
   user=AppConfig.DB_USER,
   password=AppConfig.DB_PASSWORD,
   port=AppConfig.DB_PORT,
   options="-c search_path=public"
  )
  logger.debug("Database connection established successfully")
  cursor=conn.cursor()
  cursor.execute("SET search_path TO public;")
  conn.commit()
  cursor.close()
  return conn
 except Exception as e:
  logger.error(f"Failed to connect to database: {str(e)}", exc_info=True)
  raise
def create_tables(conn, patient_df, diagnosis_df, drop_if_exists=False):
    """
    Create tables for patient data and diagnosis data with appropriate relationships.

    Args:
        conn: Database connection
        patient_df: Patient data DataFrame
        diagnosis_df: Diagnosis data DataFrame
        drop_if_exists: If True, drop existing tables before creating them (use only in development)
    """
 cursor=conn.cursor()
 try:
  if drop_if_exists:
   logger.warning("DROP_IF_EXISTS flag is True. Dropping existing tables!")
   logger.debug("Dropping diagnosis_details table if it exists")
   cursor.execute("DROP TABLE IF EXISTS public.diagnosis_details CASCADE;")
   logger.debug("Dropping patient_details table if it exists")
   cursor.execute("DROP TABLE IF EXISTS public.patient_details CASCADE;")
   conn.commit()
   logger.info("Existing tables dropped successfully")
  logger.debug("Creating patient_details table")
  create_dynamic_table(conn, "patient_details", patient_df)
  logger.debug("patient_details table created or already exists")
  logger.debug("Creating diagnosis_details table")
  create_dynamic_table(conn, "diagnosis_details", diagnosis_df)
  logger.debug("diagnosis_details table created or already exists")
  logger.debug("Setting registry_id as primary key on patient_details table")
  try:
            cursor.execute("""
            SELECT count(*) FROM pg_constraint
            WHERE conrelid = 'public.patient_details'::regclass
            AND contype = 'p';
            """)
   has_primary_key=cursor.fetchone()[0]>0
   if has_primary_key:
    logger.debug("Primary key already exists on patient_details table")
                cursor.execute("""
                SELECT a.attname
                FROM pg_index i
                JOIN pg_attribute a ON a.attrelid = i.indrelid AND a.attnum = ANY(i.indkey)
                WHERE i.indrelid = 'public.patient_details'::regclass
                AND i.indisprimary;
                """)
    pk_columns=[row[0] for row in cursor.fetchall()]
    logger.debug(f"Current primary key columns: {pk_columns}")
    if "registry_id" not in pk_columns:
     logger.debug("Dropping existing primary key to replace with registry_id")
                    cursor.execute("""
                    SELECT conname FROM pg_constraint
                    WHERE conrelid = 'public.patient_details'::regclass AND contype = 'p'
                    """)
     constraint_name=cursor.fetchone()[0]
                    cursor.execute(f"""
                    ALTER TABLE public.patient_details DROP CONSTRAINT IF EXISTS {constraint_name};
                    """)
                    cursor.execute("""
                    ALTER TABLE public.patient_details
                    ADD CONSTRAINT pk_patient_registry_id PRIMARY KEY (registry_id);
                    """)
     logger.debug("Primary key set to registry_id")
   else:
                cursor.execute("""
                ALTER TABLE public.patient_details
                ADD CONSTRAINT pk_patient_registry_id PRIMARY KEY (registry_id);
                """)
    logger.debug("Primary key constraint added on registry_id")
  except Exception as e:
   logger.warning(f"Could not set registry_id as primary key: {str(e)}")
   logger.warning("Foreign key relationship may be impacted. Check for duplicate registry_ids in patient data.")
  logger.debug("Checking if foreign key constraint exists")
        cursor.execute("""
        SELECT COUNT(*) FROM information_schema.table_constraints
        WHERE constraint_name = 'fk_diagnosis_patient'
        AND table_name = 'diagnosis_details'
        AND table_schema = 'public';
        """)
  fk_constraint_exists=cursor.fetchone()[0]>0
  logger.debug(f"Foreign key constraint exists: {fk_constraint_exists}")
  if not fk_constraint_exists:
   logger.debug("Adding foreign key constraint")
   try:
                cursor.execute("""
                ALTER TABLE public.diagnosis_details
                ADD CONSTRAINT fk_diagnosis_patient
                FOREIGN KEY (registry_id)
                REFERENCES public.patient_details (registry_id);
                """)
    logger.debug("Foreign key constraint added successfully")
   except Exception as e:
    logger.warning(f"Could not add foreign key constraint: {str(e)}")
    logger.warning("This may be due to lack of primary key or data integrity issues.")
  conn.commit()
  logger.debug("Schema changes committed")
 except Exception as e:
  conn.rollback()
  logger.error(f"Error updating schema: {str(e)}", exc_info=True)
  raise
 finally:
  cursor.close()
def create_dynamic_table(conn, table_name, df):
    """
    Dynamically create a table based on DataFrame schema.

    Args:
        conn: Database connection
        table_name: Name of the table to create
        df: DataFrame containing the schema information
    """
 cursor=conn.cursor()
 try:
  create_table_sql=f"CREATE TABLE IF NOT EXISTS public.{table_name} (\n"
  if table_name=="patient_details":
   create_table_sql+="    id SERIAL,\n"  # Not PRIMARY KEY
  else:
   create_table_sql+="    id SERIAL PRIMARY KEY,\n"
  for col_name, dtype in df.schema.items():
   col_name_snake=col_name.lower()
   pg_type=_get_pg_type(dtype)
   create_table_sql+=f"    {col_name_snake} {pg_type},\n"
  create_table_sql+="    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n"
  create_table_sql+=");"
  logger.debug(f"Executing SQL: {create_table_sql}")
  cursor.execute(create_table_sql)
  conn.commit()
  logger.debug(f"Table {table_name} created or already exists")
        cursor.execute("""
        SELECT EXISTS (
            SELECT FROM information_schema.tables
            WHERE table_schema = 'public'
            AND table_name = %s
        );
        """, (table_name,))
  exists=cursor.fetchone()[0]
  logger.debug(f"Table verification-{table_name} exists: {exists}")
 except Exception as e:
  logger.error(f"Error creating table {table_name}: {str(e)}", exc_info=True)
  raise
 finally:
  cursor.close()
def insert_data(conn, table_name, df):
    """
    Insert data into a table with validation.

    Args:
        conn: Database connection
        table_name: Name of the table to insert data into
        df: DataFrame containing the data to insert

    Returns:
        Dictionary containing insertion results including success/failure counts
    """
 cursor=conn.cursor()
 logger.info(f"Starting data validation and insertion for {table_name}")
 stats={
  "total_records": df.height,
  "valid_records": 0,
  "rejected_records": 0,
  "error_reasons": {}
 }
 try:
  columns=df.columns
  logger.debug(f"Columns for {table_name}: {columns}")
        cursor.execute(f"""
        SELECT column_name, data_type
        FROM information_schema.columns
        WHERE table_name = '{table_name}' AND table_schema = 'public'
        """)
  db_columns={row[0]: row[1] for row in cursor.fetchall()}
  logger.debug(f"Database schema for {table_name}: {db_columns}")
  valid_records=[]
  rejected_records=[]
  all_records=[tuple(row) for row in df.to_numpy()]
  for record_idx, record in enumerate(all_records):
   try:
    if len(record)!=len(columns):
     raise ValueError(f"Record has {len(record)} values but should have {len(columns)}")
    if table_name=="patient_details":
     registry_id_idx=columns.index("registry_id") if "registry_id" in columns else-1
     if registry_id_idx>=0:
      if not record[registry_id_idx] or str(record[registry_id_idx]).strip()=="":
       raise ValueError("Empty registry_id not allowed (primary key constraint)")
    elif table_name=="diagnosis_details":
     registry_id_idx=columns.index("registry_id") if "registry_id" in columns else-1
     if registry_id_idx>=0:
      if not record[registry_id_idx] or str(record[registry_id_idx]).strip()=="":
       raise ValueError("Empty registry_id not allowed (foreign key constraint)")
    for col_idx, col_name in enumerate(columns):
     if col_name in db_columns:
      value=record[col_idx]
      if value is None:
       continue
      if db_columns[col_name].startswith(('int', 'float', 'numeric', 'double', 'decimal')):
       try:
        if not isinstance(value, (int, float)) and value!='':
         float(value)  # Try to convert to validate
       except (ValueError, TypeError):
        raise ValueError(f"Invalid numeric value for {col_name}: {value}")
    valid_records.append(record)
   except Exception as e:
    rejected_records.append((record_idx, record, str(e)))
    error_type=str(e)
    if error_type in stats["error_reasons"]:
     stats["error_reasons"][error_type]+=1
    else:
     stats["error_reasons"][error_type]=1
  stats["valid_records"]=len(valid_records)
  stats["rejected_records"]=len(rejected_records)
  if rejected_records:
   logger.warning(f"Validation completed with {len(rejected_records)} rejected records out of {len(all_records)}")
   for i, (idx, record, reason) in enumerate(rejected_records[:5]):
    logger.warning(f"Rejected record #{idx}: {reason}")
    logger.debug(f"Record data: {record}")
   if len(rejected_records)>5:
    logger.warning(f"... and {len(rejected_records)-5} more rejected records")
  else:
   logger.info(f"All {len(valid_records)} records passed validation")
  if valid_records:
   sql=f"INSERT INTO public.{table_name} ({', '.join(columns)}) VALUES %s"
   logger.info(f"Inserting {len(valid_records)} valid records into public.{table_name}")
   try:
    execute_values(cursor, sql, valid_records)
    conn.commit()
    logger.info(f"Successfully inserted {len(valid_records)} records into {table_name}")
   except Exception as e:
    conn.rollback()
    logger.error(f"Database error during insertion: {str(e)}")
    stats["insertion_error"]=str(e)
    stats["valid_records"]=0  # Reset since insertion failed
    stats["rejected_records"]=df.height  # All records effectively rejected
  else:
   logger.warning(f"No valid records to insert into {table_name}")
  return stats
 except Exception as e:
  logger.error(f"Error in insert_data for {table_name}: {str(e)}", exc_info=True)
  stats["error"]=str(e)
  return stats
 finally:
  cursor.close()

=== hospital-data-chatbot/app/utils/math_utils.py (python) ===


=== hospital-data-chatbot/app/utils/__init__.py (python) ===


=== hospital-data-chatbot/app/utils/aws.py (python) ===
import boto3
import io
import os
from botocore.exceptions import ClientError
from app.config.settings import AppConfig
import logging
logger=logging.getLogger(__name__)
def get_s3_client():
    """
    Create and return an S3 client.

    Uses AWS credentials from environment variables or IAM role.
    """
 return boto3.client(
  's3',
  region_name=AppConfig.AWS_REGION
 )
def upload_to_s3(df, bucket_name, object_key):
    """
    Upload a dataframe to S3 as a CSV file.

    Args:
        df: A pandas or polars dataframe to upload
        bucket_name: S3 bucket name
        object_key: S3 object key (path/filename.csv)

    Returns:
        S3 URI of the uploaded file
    """
 try:
  s3_client=get_s3_client()
  csv_buffer=io.BytesIO()
  if hasattr(df, 'to_csv'):  # pandas DataFrame
   df.to_csv(csv_buffer, index=False)
  else:  # polars DataFrame
   if hasattr(df, 'to_pandas'):
    df.to_pandas().to_csv(csv_buffer, index=False)
   else:
    temp_path='/tmp/temp_csv.csv'
    df.write_csv(temp_path)
    with open(temp_path, 'rb') as f:
     csv_buffer.write(f.read())
    if os.path.exists(temp_path):
     os.remove(temp_path)
  csv_buffer.seek(0)
  s3_client.upload_fileobj(
   csv_buffer,
   bucket_name,
   object_key,
   ExtraArgs={
    'ContentType': 'text/csv'
   }
  )
  logger.info(f"File uploaded successfully to s3://{bucket_name}/{object_key}")
  return f"s3://{bucket_name}/{object_key}"
 except ClientError as e:
  logger.error(f"Error uploading to S3: {e}")
  raise
 except Exception as e:
  logger.error(f"Unexpected error during S3 upload: {e}")
  raise
def download_from_s3(bucket_name, object_key, local_path=None):
    """
    Download a file from S3.

    Args:
        bucket_name: S3 bucket name
        object_key: S3 object key
        local_path: Local path to save the file (optional)

    Returns:
        Local path of the downloaded file or file content as bytes
    """
 try:
  s3_client=get_s3_client()
  if local_path:
   os.makedirs(os.path.dirname(local_path), exist_ok=True)
   s3_client.download_file(bucket_name, object_key, local_path)
   logger.info(f"File downloaded from S3 to {local_path}")
   return local_path
  else:
   buffer=io.BytesIO()
   s3_client.download_fileobj(bucket_name, object_key, buffer)
   buffer.seek(0)
   logger.info(f"File downloaded from S3 to memory")
   return buffer.read()
 except ClientError as e:
  logger.error(f"Error downloading from S3: {e}")
  raise
 except Exception as e:
  logger.error(f"Unexpected error during S3 download: {e}")
  raise

=== hospital-data-chatbot/app/models/__init__.py (python) ===


=== hospital-data-chatbot/app/models/data_models.py (python) ===
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
from datetime import datetime
class Patient(BaseModel):
    """Model representing a patient record."""
    patient_id: str
    age: Optional[int] = None
    gender: Optional[str] = None
    admission_date: Optional[datetime] = None
    discharge_date: Optional[datetime] = None
    diagnosis: Optional[str] = None
    stay_duration: Optional[int] = None

    class Config:
        schema_extra = {
            "example": {
                "patient_id": "P12345",
                "age": 45,
                "gender": "F",
                "admission_date": "2023-01-15T10:30:00",
                "discharge_date": "2023-01-20T14:00:00",
                "diagnosis": "Pneumonia",
                "stay_duration": 5
            }
        }

class DataStats(BaseModel):
    """Model representing dataset statistics."""
 record_count: int
 column_count: int
 columns: List[str]

=== hospital-data-chatbot/app/api/sql_chat_routes.py (python) ===
from fastapi import APIRouter, Depends, HTTPException, Request
from pydantic import BaseModel
from app.core.sql_query_engine import SQLQueryEngine
from app.config.settings import AppConfig
from app.utils.logging import get_logger
from typing import Dict, Any, Optional, List
router=APIRouter()
logger=get_logger(__name__)
class SQLChatQuery(BaseModel):
    """Model for a SQL chat query request."""
    query: str
    include_sql: bool = False
    include_reasoning: bool = False

class SQLChatResponse(BaseModel):
    """Model for a SQL chat response."""
 response: str
 success: bool
 sql: Optional[str]=None
 reasoning: Optional[str]=None
 row_count: Optional[int]=None
 execution_time_ms: Optional[float]=None
 error: Optional[str]=None
 class Config:
  json_schema_extra={
   "example": {
    "response": "There are 25 patients who are over 65 years old...\n\nSQL Query Used:\n```sql\nSELECT COUNT(*) FROM patient_details WHERE CAST(NULLIF(age, '') AS NUMERIC)>65;\n```",
    "success": True,
    "sql": "SELECT COUNT(*) FROM patient_details WHERE CAST(NULLIF(age, '') AS NUMERIC)>65;",
    "row_count": 1,
    "execution_time_ms": 256.34
   }
  }
@router.post("/sql-chat", response_model=SQLChatResponse)
async def sql_chat(query_data: SQLChatQuery, request: Request)->Dict[str, Any]:
    """
    Process a natural language query by converting it to SQL,
    executing it on the database, and returning formatted results.

    Args:
        query_data: The query request containing the natural language question

    Returns:
        Formatted response with results and optional SQL details
    """
 if not query_data.query:
  raise HTTPException(status_code=400, detail="No query provided")
 try:
  logger.info(f"Processing SQL chat query: {query_data.query}")
  if not hasattr(request.app.state, "sql_query_engine"):
   logger.info("Initializing SQL Query Engine")
   request.app.state.sql_query_engine=SQLQueryEngine()
  sql_query_engine=request.app.state.sql_query_engine
  import time
  start_time=time.time()
  result=sql_query_engine.process_query(query_data.query)
  execution_time=(time.time()-start_time)*1000  # Convert to milliseconds
  response={
   "response": result.get("response", ""),
   "success": result.get("success", False),
   "execution_time_ms": execution_time
  }
  if query_data.include_sql or AppConfig.DEBUG:
   response["sql"]=result.get("sql", None)
  if query_data.include_reasoning or AppConfig.DEBUG:
   response["reasoning"]=result.get("reasoning", None)
  if "error" in result:
   response["error"]=result["error"]
  if "row_count" in result:
   response["row_count"]=result["row_count"]
  logger.info(f"SQL chat query processed successfully in {execution_time:.2f}ms")
  return response
 except Exception as e:
  logger.error(f"Error processing SQL chat query: {str(e)}", exc_info=True)
  raise HTTPException(
   status_code=500,
   detail=f"Failed to process query: {str(e)}"
  )
@router.get("/db-schema")
async def get_db_schema(request: Request)->Dict[str, Any]:
    """
    Get database schema information.
    Only available in development environments.

    Returns:
        Database schema details
    """
 if not AppConfig.is_development():
  raise HTTPException(
   status_code=403,
   detail="This endpoint is only available in development environments"
  )
 try:
  if not hasattr(request.app.state, "sql_query_engine"):
   logger.info("Initializing SQL Query Engine")
   request.app.state.sql_query_engine=SQLQueryEngine()
  sql_query_engine=request.app.state.sql_query_engine
  if sql_query_engine.db_schema:
   return {
    "status": "success",
    "schema": sql_query_engine.db_schema
   }
  else:
   return {
    "status": "error",
    "message": "Database schema not loaded"
   }
 except Exception as e:
  logger.error(f"Error retrieving database schema: {str(e)}", exc_info=True)
  raise HTTPException(
   status_code=500,
   detail=f"Failed to retrieve database schema: {str(e)}"
  )

=== hospital-data-chatbot/app/api/__init__.py (python) ===


=== hospital-data-chatbot/app/api/middleware.py (python) ===


=== hospital-data-chatbot/app/api/dev_routes.py (python) ===
from fastapi import APIRouter, Depends, Request, HTTPException
from app.config.settings import AppConfig
from app.utils.db import get_db_connection, create_tables
from app.utils.logging import get_logger
router=APIRouter()
logger=get_logger(__name__)
def dev_mode_only():
    """Dependency to ensure endpoint only works in development mode."""
    if not AppConfig.is_development():
        raise HTTPException(
            status_code=403,
            detail="This endpoint is only available in development environments"
        )

@router.post("/reset-tables", dependencies=[Depends(dev_mode_only)])
async def reset_tables(request: Request):
    """
 Development-only endpoint to reset database tables.
 Drops and recreates all tables, then reloads data.
    """
    logger.warning("Reset tables endpoint called - will drop and recreate all tables")

    try:
        # Get data from the data processor
        patient_data = request.app.state.data_processor.patient_data
        diagnosis_data = request.app.state.data_processor.diagnosis_data

        if patient_data is None or diagnosis_data is None:
            raise HTTPException(
                status_code=400,
                detail="No data available. Load data before resetting tables."
            )

        # Get database connection
        conn = get_db_connection()

        try:
            # Drop and recreate tables
            create_tables(conn, patient_data, diagnosis_data, drop_if_exists=True)

            # Re-insert data
            from app.utils.db import insert_data

            # Insert patient data first (for referential integrity)
            logger.info(f"Reinserting {patient_data.height} patient records")
            patient_count = insert_data(conn, "patient_details", patient_data)

            # Insert diagnosis data linked to patients
            logger.info(f"Reinserting {diagnosis_data.height} diagnosis records")
            diagnosis_count = insert_data(conn, "diagnosis_details", diagnosis_data)

            conn.commit()

            return {
                "status": "success",
                "message": "Tables reset and data reloaded successfully",
                "details": {
                    "tables_dropped": ["patient_details", "diagnosis_details"],
                    "patient_records_inserted": patient_count,
                    "diagnosis_records_inserted": diagnosis_count
                }
            }
        finally:
            conn.close()

    except Exception as e:
        logger.error(f"Failed to reset tables: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Failed to reset tables: {str(e)}"
        )

@router.get("/config", dependencies=[Depends(dev_mode_only)])
async def get_dev_config():
    """
 Development-only endpoint to view current configuration.
    """
    config_data = {
        "environment": AppConfig.ENV,
        "debug_mode": AppConfig.DEBUG,
        "database": {
            "host": AppConfig.DB_HOST,
            "port": AppConfig.DB_PORT,
            "name": AppConfig.DB_NAME,
            "user": AppConfig.DB_USER,
            # Password masked for security
            "password": "********" if AppConfig.DB_PASSWORD else None
        },
        "s3": {
            "enabled": AppConfig.USE_S3,
            "bucket": AppConfig.S3_BUCKET if hasattr(AppConfig, 'S3_BUCKET') else None
        },
        "bedrock": {
            "model_id": AppConfig.BEDROCK_MODEL_ID
        }
    }

    return config_data

=== hospital-data-chatbot/app/api/routes.py (python) ===
from fastapi import APIRouter, Depends, HTTPException, Request
from pydantic import BaseModel
from app.config.settings import AppConfig
from app.api.sql_chat_routes import router as sql_chat_router
router=APIRouter()
class ChatQuery(BaseModel):
 query: str
@router.get("/health")
async def health_check():
    """API health check endpoint."""
    return {
        "status": "healthy",
        "message": "API is operational",
        "environment": AppConfig.get_environment_name(),
        "debug_mode": AppConfig.DEBUG
    }

@router.get("/data/stats")
async def data_stats(request: Request):
    """Get statistics about the loaded data."""
 stats=request.app.state.data_processor.get_data_stats()
 if AppConfig.is_development():
  stats["environment"]=AppConfig.get_environment_name()
  stats["app_config"]={
   "db_host": AppConfig.DB_HOST,
   "s3_enabled": AppConfig.USE_S3,
   "s3_bucket": AppConfig.S3_BUCKET if AppConfig.USE_S3 else None,
  }
 return stats
@router.post("/import-to-db")
async def import_to_db(request: Request):
    """Import processed data to the database. Used for nightly batch processing."""
    try:
        from app.core.db_importer import DbImporter

        # Get processed data
        patient_data = request.app.state.data_processor.patient_data
        diagnosis_data = request.app.state.data_processor.diagnosis_data

        # Import to database
        importer = DbImporter()
        try:
            result = importer.import_data(patient_data, diagnosis_data)
            return {
                "status": "success",
                "message": "Data imported successfully",
                "details": result
            }
        finally:
            importer.close()

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to import data to database: {str(e)}"
        )

@router.post("/chat")
async def chat(query: ChatQuery, request: Request):
    """Process chat requests using LLM."""
 if not query.query:
  raise HTTPException(status_code=400, detail="No query provided")
 from app.core.query_engine import QueryEngine
 from app.utils.calculation_handler import CalculationHandler
 query_engine=QueryEngine(
  request.app.state.data_processor.patient_data,
  request.app.state.data_processor.diagnosis_data
 )
 llm_response=query_engine.process_query(query.query)
 calc_handler=CalculationHandler(
  request.app.state.data_processor.patient_data,
  request.app.state.data_processor.diagnosis_data
 )
 final_response=calc_handler.process_response(llm_response)
 return {"response": final_response}
@router.get("/health")
async def health_check():
    """Enhanced API health check endpoint for AWS load balancer health checks."""
    try:
        # Check database connection if needed
        db_status = "unknown"
        if hasattr(request.app.state, "data_processor"):
            try:
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("SELECT 1")
                cursor.close()
                conn.close()
                db_status = "connected"
            except:
                db_status = "error"

        return {
            "status": "healthy",
            "message": "API is operational",
            "environment": AppConfig.get_environment_name(),
            "timestamp": datetime.now().isoformat(),
            "db_status": db_status
        }
    except:
        # Simplified response for errors
        return {"status": "healthy"}  # Still return 200 for load balancer

# Include the SQL chat routes
router.include_router(sql_chat_router, prefix="/db")

=== hospital-data-chatbot/hospital_data_chatbot.egg-info/PKG-INFO (text) ===
Metadata-Version: 2.4
Name: hospital-data-chatbot
Version: 0.1.0
Summary: AI-powered hospital data chatbot using Ollama and Text-to-SQL
License: MIT
Requires-Python: >=3.9
Description-Content-Type: text/markdown
Requires-Dist: polars>=0.18.0
Requires-Dist: numpy>=1.24.0
Requires-Dist: fastapi>=0.100.0
Requires-Dist: uvicorn>=0.22.0
Requires-Dist: openpyxl>=3.1.2
Requires-Dist: fastexcel>=0.7.0
Requires-Dist: psycopg2-binary>=2.9.5
Requires-Dist: python-dotenv>=1.0.0
Requires-Dist: pydantic>=2.0.0
Requires-Dist: rich>=13.0.0
Requires-Dist: requests>=2.31.0
Requires-Dist: httpx>=0.24.1
Requires-Dist: boto3>=1.28.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: black>=23.0.0; extra == "dev"
Requires-Dist: isort>=5.0.0; extra == "dev"
Requires-Dist: flake8>=6.0.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0.0; extra == "dev"
Requires-Dist: mypy>=1.0.0; extra == "dev"
Provides-Extra: prod
Requires-Dist: gunicorn>=21.0.0; extra == "prod"
Provides-Extra: database
Requires-Dist: alembic>=1.10.0; extra == "database"
Requires-Dist: sqlalchemy>=2.0.0; extra == "database"
Provides-Extra: ollama
Requires-Dist: requests>=2.31.0; extra == "ollama"
Requires-Dist: retry>=0.9.2; extra == "ollama"
Dynamic: requires-python
> An AI-powered chatbot for analyzing hospital patient data using AWS Bedrock, Text-to-SQL, and Machine Learning
![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)
![Python: 3.9+](https://img.shields.io/badge/Python-3.9+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-0.100.0+-ff69b4)
![Polars](https://img.shields.io/badge/Polars-0.18.0+-orange)
This application provides an intelligent chatbot interface for hospital staff to query patient and diagnosis data through natural language. It uses AWS Bedrock Large Language Models to interpret queries, converts them to SQL, and leverages machine learning models to provide advanced insights and predictions.
-  **Natural Language to SQL**: Convert plain language questions into precise SQL queries
-  **Machine Learning Insights**: Predictive analytics for patient risk and outcomes
-  **Statistical Analysis**: Accurate calculations on patient metrics and trends
-  **Advanced Data Processing**: Data sanitization and feature engineering pipelines
-  **Automated Data Pipeline**: Scheduled processing for up-to-date insights
-  **PostgreSQL Integration**: Direct querying of hospital database with proper relationships
-  **AWS Integration**: Leverages AWS Bedrock, SageMaker, Lambda, and other services
```mermaid
flowchart TD
subgraph DataSources["Data Sources"]
RDS[("AWS RDS<br>PostgreSQL")]
S3Raw[("AWS S3<br>Raw Data")]
end
subgraph DataProcessing["Data Processing"]
Lambda["AWS Lambda<br>Feature Engineering"]
S3Features[("AWS S3<br>Feature Store")]
end
subgraph MLPipeline["ML Pipeline"]
SageTrain["Amazon SageMaker<br>Training Jobs"]
SageModel["Amazon SageMaker<br>Model Registry"]
SageEndpoint["Amazon SageMaker<br>Endpoints"]
end
subgraph APILayer["API Layer"]
API["Amazon API Gateway"]
EC2["EC2 Instance<br>FastAPI Application"]
ELB["Elastic Load Balancer"]
end
subgraph Monitoring["Monitoring & Management"]
CloudWatch["Amazon CloudWatch"]
CloudTrail["AWS CloudTrail"]
SNS["Amazon SNS<br>Alerts"]
end
%% Connections
RDS --> Lambda
S3Raw --> Lambda
Lambda --> S3Features
S3Features --> SageTrain
SageTrain --> SageModel
SageModel --> SageEndpoint
Lambda --> EC2
SageEndpoint --> EC2
EC2 --> ELB
ELB --> API
SageEndpoint --> CloudWatch
EC2 --> CloudWatch
CloudWatch --> SNS
API --> CloudTrail
%% Style definitions
classDef aws fill:#FF9900,stroke:#232F3E,color:#232F3E,stroke-width:2px
classDef db fill:#3B48CC,stroke:#232F3E,color:white,stroke-width:2px
classDef storage fill:#3B48CC,stroke:#232F3E,color:white,stroke-width:2px
classDef api fill:#CC2264,stroke:#232F3E,color:white,stroke-width:2px
classDef compute fill:#EC7211,stroke:#232F3E,color:white,stroke-width:2px
classDef monitoring fill:#CC2264,stroke:#232F3E,color:white,stroke-width:2px
%% Apply styles
class RDS,S3Raw,S3Features db
class Lambda compute
class SageTrain,SageModel,SageEndpoint aws
class API,ELB api
class EC2 compute
class CloudWatch,CloudTrail,SNS monitoring
```
Our application uses AWS Bedrock to intelligently translate natural language questions into SQL queries:
1. **Query Understanding**: Analyzes intent and context of natural language questions
2. **Schema-Aware Translation**: Generates SQL based on hospital database schema
3. **SQL Validation**: Ensures queries are safe and optimized before execution
4. **Result Formatting**: Presents results in an easy-to-understand natural language format
Example query:
```
"How many patients over 65 were diagnosed with pneumonia last month?"
```
The system provides several ML-powered insights:
1. **Patient Risk Stratification**: Classifies patients by risk level using demographic and clinical factors
2. **Readmission Prediction**: Identifies patients at risk of 30-day readmission
3. **Diagnosis Clustering**: Groups similar diagnoses to uncover patterns
4. **Length of Stay Prediction**: Forecasts expected hospital stay duration
All models are trained using Amazon SageMaker and served through SageMaker endpoints.
```bash
./setup_uv.sh
```
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
python -m app.main
```
1. Install a local PostgreSQL instance or use Docker:
```bash
docker run --name postgres-test -e POSTGRES_PASSWORD=postgres -e POSTGRES_DB=hospital_data_test -p 5432:5432 -d postgres:14
```
2. Create a `.env` file in project root:
```
DEBUG=True
PORT=8080
DATA_DIR=data
DB_HOST=localhost
DB_PORT=5432
DB_NAME=hospital_data_test
DB_USER=postgres
DB_PASSWORD=postgres
USE_S3=False
```
3. Prepare test data:
```bash
mkdir -p data/raw
cp path/to/your/hospital_data.xlsx data/raw/
```
4. Launch the API:
```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8080
```
5. Access the API documentation:
- http://localhost:8080/docs
```
POST /api/db/sql-chat
```
Request body:
```json
{
"query": "How many male patients with diabetes were admitted last year?",
"include_sql": true,
"include_reasoning": false
}
```
```
GET /api/ml/patient-risk?patient_id=P12345
GET /api/ml/readmission-risk/P12345
GET /api/ml/diagnosis-clusters
```
```
GET /api/health
GET /api/data/stats
POST /api/chat
POST /api/import-to-db
```
```bash
docker build -t hospital-chatbot:latest .
aws ecr get-login-password | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com
docker tag hospital-chatbot:latest $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/hospital-chatbot:latest
docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/hospital-chatbot:latest
aws cloudformation deploy \
```
-  **SQL Injection Prevention**: All SQL queries are validated and sanitized
-  **Input Validation**: Comprehensive data validation at all entry points
-  **IAM Role-Based Access**: Fine-grained AWS permissions
-  **Data Encryption**: Hospital data encrypted at rest and in transit
-  **API Key Authentication**: Secure API access with key validation
```
hospital-data-chatbot/

 app/                         # Application code
    api/                     # API endpoints
       routes.py            # Main API routes
       sql_chat_routes.py   # Text-to-SQL endpoints
       ml_routes.py         # Machine learning endpoints
    config/                  # Configuration
    core/                    # Core logic
       data_processor.py    # Data processing and sanitization
       sql_query_engine.py  # Text-to-SQL engine
       llm_connector.py     # AWS Bedrock LLM interface
    ml/                      # ML components
       feature_engineering.py  # Feature extraction
       feature_store.py     # Feature storage and caching
       sagemaker_integration.py # Model training and deployment
       hospital_ml_models.py   # Domain-specific ML models
    models/                  # Data models
    utils/                   # Utilities
        db.py                # Database utilities
        calculation_handler.py # Statistical calculation handling

 data/                        # Data files
    raw/                     # Original data
    processed/               # Processed data

 deploy/                      # Deployment files
    cloudformation.yaml      # AWS CloudFormation template

 tests/                       # Unit tests

 scripts/                     # Utility scripts
    nightly_import.py        # Nightly data import

 Dockerfile                   # Container definition
 pyproject.toml               # Project dependencies
 setup.py                     # Package setup
 README.md                    # This file
```
-  **Model A/B Testing**: Compare different model versions for optimal performance
-  **Model Drift Detection**: Automatically detect when models need retraining
-  **Web Dashboard**: Interactive dashboard for visualizing ML insights
-  **Real-time Monitoring**: Stream processing for immediate alerts
-  **Enhanced Visualizations**: Visual representation of prediction results
-  **Natural Language Explanations**: Human-readable explanations of ML predictions
This project is licensed under the MIT License - see the LICENSE file for details.
- Naman Sharma

=== hospital-data-chatbot/hospital_data_chatbot.egg-info/SOURCES.txt (text) ===
README.md
pyproject.toml
setup.py
app/__init__.py
app/main.py
app/api/__init__.py
app/api/dev_routes.py
app/api/middleware.py
app/api/routes.py
app/api/sql_chat_routes.py
app/config/__init__.py
app/config/settings.py
app/core/__init__.py
app/core/data_processor.py
app/core/db_importer.py
app/core/llm_connector.py
app/core/ollama_connector.py
app/core/query_engine.py
app/core/sql_query_engine.py
app/models/__init__.py
app/models/data_models.py
app/utils/__init__.py
app/utils/aws.py
app/utils/calculation_handler.py
app/utils/db.py
app/utils/logging.py
app/utils/math_utils.py
hospital_data_chatbot.egg-info/PKG-INFO
hospital_data_chatbot.egg-info/SOURCES.txt
hospital_data_chatbot.egg-info/dependency_links.txt
hospital_data_chatbot.egg-info/requires.txt
hospital_data_chatbot.egg-info/top_level.txt
tests/__init__.py
tests/conftest.py
tests/test_data_processor.py
tests/test_llm_connector.py
tests/test_ollama_setup.py
tests/test_query_engine.py

=== hospital-data-chatbot/hospital_data_chatbot.egg-info/requires.txt (text) ===
polars>=0.18.0
numpy>=1.24.0
fastapi>=0.100.0
uvicorn>=0.22.0
openpyxl>=3.1.2
fastexcel>=0.7.0
psycopg2-binary>=2.9.5
python-dotenv>=1.0.0
pydantic>=2.0.0
rich>=13.0.0
requests>=2.31.0
httpx>=0.24.1
boto3>=1.28.0
[database]
alembic>=1.10.0
sqlalchemy>=2.0.0
[dev]
pytest>=7.0.0
black>=23.0.0
isort>=5.0.0
flake8>=6.0.0
pytest-cov>=4.0.0
mypy>=1.0.0
[ollama]
requests>=2.31.0
retry>=0.9.2
[prod]
gunicorn>=21.0.0

=== hospital-data-chatbot/hospital_data_chatbot.egg-info/top_level.txt (text) ===
app
tests

=== hospital-data-chatbot/hospital_data_chatbot.egg-info/dependency_links.txt (text) ===


=== hospital-data-chatbot/tests/conftest.py (python) ===


=== hospital-data-chatbot/tests/test_ollama_setup.py (python) ===
import requests
import json
import argparse
def test_ollama(model, host="http://localhost:11434"):
    """Test if Ollama is working with the specified model."""
    print(f"Testing Ollama with model {model} at {host}...")

    try:
        # Check if Ollama is running
        response = requests.get(f"{host}/api/health")
        if response.status_code != 200:
            print(f"Error: Ollama service not responding properly: {response.status_code}")
            return False

        print(" Ollama service is running")

        # Check if the model is loaded
        payload = {
            "model": model,
            "prompt": "Generate a simple SQL query to count all patients.",
            "stream": False
        }

        response = requests.post(
            f"{host}/api/generate",
            json=payload,
            headers={"Content-Type": "application/json"}
        )

        if response.status_code != 200:
            print(f"Error: Failed to generate response with model {model}: {response.status_code}")
            return False

        result = response.json()
        print(" Model is working correctly")
        print("\nSample output:")
        print(result.get("response", "No response"))
        return True

    except Exception as e:
        print(f"Error connecting to Ollama: {str(e)}")
        return False

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Test Ollama setup")
    parser.add_argument("--model", default="qwen2.5-coder:14b", help="Model to test")
    parser.add_argument("--host", default="http://localhost:11434", help="Ollama host")

    args = parser.parse_args()
    success = test_ollama(args.model, args.host)
    exit(0 if success else 1)

=== hospital-data-chatbot/tests/curl.txt (text) ===
curl --request POST \
"query": "How many patients are above 65 years of age?",
"include_sql": true
}'

=== hospital-data-chatbot/tests/test_llm_connector.py (python) ===


=== hospital-data-chatbot/tests/__init__.py (python) ===


=== hospital-data-chatbot/tests/test_query_engine.py (python) ===


=== hospital-data-chatbot/tests/test_data_processor.py (python) ===


=== hospital-data-chatbot/scripts/nightly_deploy.py (python) ===
import requests
import os
import logging
from datetime import datetime
logging.basicConfig(
 level=logging.INFO,
 format='%(asctime)s-%(name)s-%(levelname)s-%(message)s',
 handlers=[
  logging.FileHandler(f"logs/nightly_import_{datetime.now().strftime('%Y%m%d')}.log"),
  logging.StreamHandler()
 ]
)
logger=logging.getLogger(__name__)
def run_nightly_import():
    """Run the nightly import process."""
    logger.info("Starting nightly import process")

    api_url = os.environ.get("API_URL", "http://localhost:8080")
    api_key = os.environ.get("API_KEY", "default_dev_key")

    try:
        # First, trigger data loading/processing
        headers = {"Authorization": f"Bearer {api_key}"}

        # Trigger the import
        import_url = f"{api_url}/api/import-to-db"
        response = requests.post(import_url, headers=headers)

        if response.status_code == 200:
            result = response.json()
            logger.info(f"Import completed successfully: {result}")
            return True
        else:
            logger.error(f"Import failed with status {response.status_code}: {response.text}")
            return False

    except Exception as e:
        logger.error(f"Error during nightly import: {str(e)}")
        return False

if __name__ == "__main__":
    run_nightly_import()

=== hospital-data-chatbot/.github/workflow/hospital-data-chatbot.yml (yaml) ===
name: Hospital Data Chatbot CI/CD Pipeline
on:
push:
branches: [main, develop]
pull_request:
branches: [main, develop]
workflow_dispatch:
inputs:
environment:
description: 'Environment to deploy to'
required: true
default: 'dev-cloud'
type: choice
options:
- dev-cloud
- stage
- prod
env:
AWS_REGION: ap-south-1
PROJECT_NAME: hospital-data-chatbot
TERRAFORM_VERSION: '1.5.7'
jobs:
test:
name: Run Tests
runs-on: ubuntu-latest
services:
postgres:
image: postgres:14
env:
POSTGRES_PASSWORD: postgres
POSTGRES_DB: hospital_data_test
ports:
- 5432:5432
options: >-
steps:
- uses: actions/checkout@v3
- name: Set up Python 3.9
uses: actions/setup-python@v4
with:
python-version: '3.9'
- name: Install uv
run: pip install uv
- name: Setup virtual environment
run: |
uv venv
source .venv/bin/activate
uv pip install -e ".[dev]"
- name: Run linting
run: |
source .venv/bin/activate
flake8 app tests
black --check app tests
isort --check-only app tests
- name: Run tests with coverage
run: |
source .venv/bin/activate
pytest tests/ --cov=app --cov-report=xml
- name: Upload coverage report
uses: codecov/codecov-action@v3
with:
file: ./coverage.xml
fail_ci_if_error: false
terraform:
name: Terraform Infrastructure
needs: test
if: github.event_name != 'pull_request'
runs-on: ubuntu-latest
outputs:
ecr_repository_url: ${{ steps.terraform-output.outputs.ecr_repository_url }}
db_endpoint: ${{ steps.terraform-output.outputs.db_endpoint }}
steps:
- uses: actions/checkout@v3
- name: Configure AWS credentials
uses: aws-actions/configure-aws-credentials@v2
with:
aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
aws-region: ${{ env.AWS_REGION }}
- name: Setup Terraform
uses: hashicorp/setup-terraform@v2
with:
terraform_version: ${{ env.TERRAFORM_VERSION }}
- name: Determine environment
id: environment
run: |
if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
echo "ENV=${{ github.event.inputs.environment }}" >> $GITHUB_ENV
elif [ "${{ github.ref }}" = "refs/heads/main" ]; then
echo "ENV=prod" >> $GITHUB_ENV
else
echo "ENV=dev-cloud" >> $GITHUB_ENV
fi
- name: Terraform Init
working-directory: deploy/terraform
run: |
if [ -f "environments/backend-config/${{ env.ENV }}.hcl" ]; then
terraform init -backend-config=environments/backend-config/${{ env.ENV }}.hcl -reconfigure
else
terraform init -reconfigure
fi
- name: Terraform Validate
working-directory: deploy/terraform
run: terraform validate
- name: Create Parameter Store for DB Password
run: |
aws ssm put-parameter \
- name: Terraform Plan
working-directory: deploy/terraform
run: |
terraform plan \
-var-file=environments/${{ env.ENV }}.tfvars \
-var="db_password=${{ secrets.DB_PASSWORD }}" \
-out=tfplan
- name: Terraform Apply
working-directory: deploy/terraform
run: terraform apply -auto-approve tfplan
- name: Terraform Output
id: terraform-output
working-directory: deploy/terraform
run: |
echo "ecr_repository_url=$(terraform output -raw ecr_repository_url)" >> $GITHUB_OUTPUT
echo "db_endpoint=$(terraform output -raw db_endpoint)" >> $GITHUB_OUTPUT
echo "vpc_id=$(terraform output -raw vpc_id)" >> $GITHUB_OUTPUT
echo "s3_bucket_name=$(terraform output -raw s3_bucket_name)" >> $GITHUB_OUTPUT
build-and-push:
name: Build and Push Docker Image
needs: terraform
runs-on: ubuntu-latest
steps:
- uses: actions/checkout@v3
- name: Configure AWS credentials
uses: aws-actions/configure-aws-credentials@v2
with:
aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
aws-region: ${{ env.AWS_REGION }}
- name: Determine environment
id: environment
run: |
if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
echo "ENV=${{ github.event.inputs.environment }}" >> $GITHUB_ENV
elif [ "${{ github.ref }}" = "refs/heads/main" ]; then
echo "ENV=prod" >> $GITHUB_ENV
else
echo "ENV=dev-cloud" >> $GITHUB_ENV
fi
- name: Login to Amazon ECR
id: login-ecr
uses: aws-actions/amazon-ecr-login@v1
- name: Build and tag Docker image
id: build-image
env:
ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
IMAGE_TAG: ${{ github.sha }}
run: |
mkdir -p data/raw
touch data/raw/hospital_data.xlsx
docker build -t $ECR_REGISTRY/${{ env.PROJECT_NAME }}-${{ env.ENV }}:$IMAGE_TAG \
.
docker tag $ECR_REGISTRY/${{ env.PROJECT_NAME }}-${{ env.ENV }}:$IMAGE_TAG $ECR_REGISTRY/${{ env.PROJECT_NAME }}-${{ env.ENV }}:latest
echo "image=$ECR_REGISTRY/${{ env.PROJECT_NAME }}-${{ env.ENV }}:$IMAGE_TAG" >> $GITHUB_OUTPUT
echo "image_latest=$ECR_REGISTRY/${{ env.PROJECT_NAME }}-${{ env.ENV }}:latest" >> $GITHUB_OUTPUT
- name: Push Docker image
env:
ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
run: |
docker push ${{ steps.build-image.outputs.image }}
docker push ${{ steps.build-image.outputs.image_latest }}
deploy-app:
name: Deploy Application
needs: [terraform, build-and-push]
runs-on: ubuntu-latest
steps:
- uses: actions/checkout@v3
- name: Configure AWS credentials
uses: aws-actions/configure-aws-credentials@v2
with:
aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
aws-region: ${{ env.AWS_REGION }}
- name: Determine environment
run: |
if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
echo "ENV=${{ github.event.inputs.environment }}" >> $GITHUB_ENV
elif [ "${{ github.ref }}" = "refs/heads/main" ]; then
echo "ENV=prod" >> $GITHUB_ENV
else
echo "ENV=dev-cloud" >> $GITHUB_ENV
fi
- name: Setup Terraform
uses: hashicorp/setup-terraform@v2
with:
terraform_version: ${{ env.TERRAFORM_VERSION }}
- name: Terraform Init
working-directory: deploy/terraform
run: |
if [ -f "environments/backend-config/${{ env.ENV }}.hcl" ]; then
terraform init -backend-config=environments/backend-config/${{ env.ENV }}.hcl -reconfigure
else
terraform init -reconfigure
fi
- name: Apply Deployment Configuration
working-directory: deploy/terraform
run: |
terraform apply \
-var-file=environments/${{ env.ENV }}.tfvars \
-var="db_password=${{ secrets.DB_PASSWORD }}" \
-var="image_tag=${{ github.sha }}" \
-auto-approve \
-target=module.app_deployment
- name: Verify Deployment
working-directory: deploy/terraform
run: |
APP_URL=$(terraform output -raw app_url)
if [ -n "$APP_URL" ]; then
timeout=300
start_time=$(date +%s)
echo "Waiting for application to be available..."
while true; do
current_time=$(date +%s)
elapsed=$((current_time - start_time))
if [ $elapsed -gt $timeout ]; then
echo "Timed out waiting for application"
exit 1
fi
http_status=$(curl -s -o /dev/null -w "%{http_code}" $APP_URL/api/health || echo "000")
if [ "$http_status" = "200" ]; then
echo "Application is available!"
break
fi
echo "Application not available yet, waiting..."
sleep 10
done
else
echo "Application URL not available in outputs"
fi

=== hospital-data-chatbot/.github/workflow/ReadMe.md (markdown) ===
This document describes the CI/CD pipeline for the Hospital Data Chatbot project, which uses GitHub Actions and Terraform for infrastructure provisioning and application deployment.
The CI/CD pipeline consists of the following stages:
1. **Test**: Run unit tests, linting, and code quality checks
2. **Terraform**: Provision or update infrastructure with Terraform
3. **Build and Push**: Build and push Docker image to ECR
4. **Deploy Application**: Deploy the application using ECS Fargate
The pipeline supports three environments:
- **Development (dev-cloud)**: Used for development and testing
- **Staging (stage)**: Used for pre-production testing
- **Production (prod)**: Used for production deployment
The pipeline is triggered by:
- **Push to main branch**: Automatically deploys to production
- **Push to develop branch**: Automatically deploys to development
- **Manual trigger**: Can deploy to any environment via GitHub Actions UI
- Sets up Python environment with the `uv` package manager
- Installs dependencies and development tools
- Runs linting with flake8, black, and isort
- Runs unit tests with pytest and uploads coverage report
- Sets up Terraform with the specified version
- Determines the target environment
- Initializes Terraform with the correct backend configuration
- Validates the Terraform configuration
- Plans the infrastructure changes
- Applies the infrastructure changes
- Outputs important resource identifiers for use in later stages
- Configures AWS credentials
- Logs in to Amazon ECR
- Builds the Docker image with environment-specific variables
- Tags the image with the commit SHA and environment name
- Pushes the Docker image to ECR
- Re-initializes Terraform to ensure up-to-date state
- Applies the application deployment configuration
- Updates the ECS service with the new Docker image
- Verifies the deployment was successful by checking the application health endpoint
The pipeline follows these best practices:
- **Infrastructure as Code**: All infrastructure is defined as code using Terraform
- **Environment Isolation**: Each environment has its own isolated infrastructure
- **Automated Testing**: All code changes are tested before deployment
- **Immutable Infrastructure**: Infrastructure is updated through Terraform, not manual changes
- **Continuous Integration**: Code changes are automatically tested and deployed
- **Continuous Deployment**: Successful changes to the main branch are automatically deployed to production
- **Rollback Capability**: Failed deployments can be rolled back by redeploying the previous version
- **Logging and Monitoring**: Application logs and metrics are captured in CloudWatch
- **Security**: Secrets are stored in AWS Secrets Manager and accessed securely
To set up the CI/CD pipeline:
1. Add the following secrets to your GitHub repository:
- `AWS_ACCESS_KEY_ID`: AWS access key with appropriate permissions
- `AWS_SECRET_ACCESS_KEY`: AWS secret access key
- `DB_PASSWORD`: Database password for each environment
2. Bootstrap the initial infrastructure:
```bash
./deploy/terraform/bootstrap.sh

